%% This BibTeX bibliography file was created using BibDesk.
%% http://bibdesk.sourceforge.net/


%% Created for Ozan Öktem at 2015-03-06 07:43:36 +0100 


%% Saved with string encoding Unicode (UTF-8) 



@article{EtGrOb14,
	Abstract = {This paper introduces an FFT-based implementation of a fast finite ridgelet transform which we call FFRT. Inspired by recent work where it was shown that ridgelet discretizations of linear transport equations can be easily preconditioned by diagonal preconditioning, we use the FFRT for the numerical solution of such equations. Combining this FFRT-based method with a sparse collocation scheme, we construct a novel solver for the radiative transport equation which results in uniformly well-conditioned linear systems.},
	Author = {Etter, S. and Grohs, P. and Obermeier, A.},
	Date-Added = {2015-03-06 06:42:11 +0000},
	Date-Modified = {2015-03-06 06:43:35 +0000},
	Journal = {Multiscale Modeling \& Simulation},
	Number = {1},
	Pages = {1---42},
	Title = {{FFRT}: {A} Fast Finite Ridgelet Transform for Radiative Transport},
	Volume = {13},
	Year = {2014}}

@incollection{St09,
	Abstract = {In [Math. Comp, 70 (2001), 27--75] and [Found. Comput. Math., 2(3) (2002), 203--245], Cohen, Dahmen and DeVore introduced adaptive wavelet meth- ods for solving operator equations. These papers meant a break-through in the field, because their adaptive methods were not only proven to converge, but also with a rate better than that of their non-adaptive counterparts in cases where the latter meth- ods converge with a reduced rate due a lacking regularity of the solution. Until then, adaptive methods were usually assumed to converge via a saturation assumption. An exception was given by the work of Do ̈rfler in [SIAM J. Numer. Anal., 33 (1996), 1106--1124], where an adaptive finite element method was proven to converge, with no rate though.
This work contains a complete analysis of the methods from the aforementioned two papers of Cohen, Dahmen and DeVore. Furthermore, we give an overview over the subsequent developments in the field of adaptive wavelet methods. This includes a precise analysis of the near-sparsity of an operator in wavelet coordinates needed to obtain optimal computational complexity; the avoidance of coarsening; quan- titative improvements of the algorithms; their generalization to frames; and their application with tensor product wavelet bases which give dimension independent rates.},
	Author = {Stevenson, R.},
	Booktitle = {Multiscale, Nonlinear and Adaptive Approximation: Dedicated to Wolfgang Dahmen on the Occasion of his 60th Birthday},
	Date-Added = {2015-03-06 06:38:44 +0000},
	Date-Modified = {2015-03-06 06:40:44 +0000},
	Editor = {DeVore, R. and Kunoth, A.},
	Pages = {543--596},
	Publisher = {Springer-Verlag},
	Title = {Adaptive wavelet methods for solving operator equations: An overview},
	Year = {2009}}

@techreport{GrKeKuSc14,
	Abstract = {Within the area of applied harmonic analysis, various multiscale systems such as wavelets, ridgelets, curvelets, and shearlets have been introduced and successfully applied. The key property of each of those systems are their (optimal) approximation properties in terms of the decay of the L2-error of the best N-term approximation for a certain class of functions. In this paper, we introduce the general framework of alpha-molecules, which encompasses most multiscale systems from applied harmonic analysis, in particular, wavelets, ridgelets, curvelets, and shearlets as well as extensions of such with alpha being a parameter measuring the degree of anisotropy, as a means to allow a unified treatment of approximation results within this area. Based on an alpha-scaled index distance, we first prove that two systems of alpha-molecules are almost orthogonal. This leads to a general methodology to transfer approximation results within this framework, provided that certain consistency and time-frequency localization conditions of the involved systems of alpha-molecules are satisfied. We finally utilize these results to enable the derivation of optimal sparse approximation results for a specific class of cartoon- like functions by sufficient conditions on the `control' parameters of a system of alpha-molecules.},
	Author = {Grohs, P. and Keiper, S. and Kutyniok, G. and Sch{\"a}fer, M.},
	Date-Added = {2015-03-06 06:24:24 +0000},
	Date-Modified = {2015-03-06 06:28:12 +0000},
	Institution = {Department of Mathematics, ETH Z{\"u}rich},
	Number = {2014-16},
	Title = {$\alpha$-Molecules},
	Type = {SAM Research Reports},
	Year = {2014}}

@article{GrKu13,
	Abstract = {Anisotropic decompositions using representation systems based on parabolic scaling such as curvelets or shearlets have recently attracted significant attention due to the fact that they were shown to provide optimally sparse approximations of functions exhibiting singularities on lower dimensional embedded manifolds. The literature now contains various direct proofs of this fact and of related sparse approximation results. However, it seems quite cumbersome to prove such a canon of results for each system separately, while many of the systems exhibit certain similarities.

In this paper, with the introduction of the notion of parabolic molecules, we aim to provide a comprehensive framework which includes customarily employed representation systems based on parabolic scaling such as curvelets and shearlets. It is shown that pairs of parabolic molecules have the fundamental property to be almost orthogonal in a particular sense. This result is then applied to analyze parabolic molecules with respect to their ability to sparsely approximate data governed by anisotropic features. For this, the concept of sparsity equivalence is introduced which is shown to allow the identification of a large class of parabolic molecules providing the same sparse approximation results as curvelets and shearlets. Finally, as another application, smoothness spaces associated with parabolic molecules are introduced providing a general theoretical approach which even leads to novel results for, for instance, compactly supported shearlets.
},
	Author = {Grohs, P. and Kutyniok, G.},
	Date-Added = {2015-03-06 06:22:53 +0000},
	Date-Modified = {2015-03-06 06:23:39 +0000},
	Journal = {Foundations of Computational Mathematics},
	Number = {2},
	Pages = {299--337},
	Title = {Parabolic Molecules},
	Volume = {14},
	Year = {2013}}

@article{DoVe05,
	Abstract = {The limitations of commonly used separable extensions of one-dimensional transforms, such as the Fourier and wavelet transforms, in capturing the geometry of image edges are well known. In this paper, we pursue a "true" two-dimensional transform that can capture the intrinsic geometrical structure that is key in visual information. The main challenge in exploring geometry in images comes from the discrete nature of the data. Thus, unlike other approaches, such as curvelets, that first develop a transform in the continuous domain and then discretize for sampled data, our approach starts with a discrete-domain construction and then studies its convergence to an expansion in the continuous domain. Specifically, we construct a discrete-domain multiresolution and multidirection expansion using nonseparable filter banks, in much the same way that wavelets were derived from filter banks. This construction results in a flexible multiresolution, local, and directional image expansion using contour segments, and, thus, it is named the contourlet transform. The discrete contourlet transform has a fast iterated filter bank algorithm that requires an order N operations for N-pixel images. Furthermore, we establish a precise link between the developed filter bank and the associated continuous-domain contourlet expansion via a directional multiresolution analysis framework. We show that with parabolic scaling and sufficient directional vanishing moments, contourlets achieve the optimal approximation rate for piecewise smooth functions with discontinuities along twice continuously differentiable curves. Finally, we show some numerical experiments demonstrating the potential of contourlets in several image processing applications.

},
	Author = {Do, M. N. and Vetterli, M.},
	Date-Added = {2015-03-06 06:20:55 +0000},
	Date-Modified = {2015-03-06 06:22:06 +0000},
	Journal = {IEEE Transactions on Image Processing},
	Number = {12},
	Pages = {2091--2106},
	Title = {The contourlet transform: an efficient directional multiresolution image representation},
	Volume = {14},
	Year = {2005}}

@article{CaDeDoYi06,
	Abstract = {This paper describes two digital implementations of a new mathematical transform, namely, the second generation curvelet transform in two and three dimensions. The first digital transformation is based on unequally spaced fast Fourier transforms, while the second is based on the wrapping of specially selected Fourier samples. The two implementations essentially differ by the choice of spatial grid used to translate curvelets at each scale and angle. Both digital transformations return a table of digital curvelet coefficients indexed by a scale parameter, an orientation parameter, and a spatial location parameter. And both implementations are fast in the sense that they run in O(n^2 \log n) flops for n by n Cartesian arrays; in addition, they are also invertible, with rapid inversion algorithms of about the same complexity. Our digital transformations improve upon earlier implementations---based upon the first generation of curvelets---in the sense that they are conceptually simpler, faster, and far less redundant. The software CurveLab, which implements both transforms presented in this paper, is available at http://www.curvelet.org.},
	Author = {Cand{\`e}s, E. J. and Demanet, L. and Donoho, D. L. and Ying, L.},
	Date-Added = {2015-03-06 06:16:01 +0000},
	Date-Modified = {2015-03-06 06:17:10 +0000},
	Journal = {Multiscale Modeling \& Simulation},
	Number = {3},
	Pages = {861--899},
	Title = {Fast Discrete Curvelet Transforms},
	Volume = {5},
	Year = {2006}}

@article{CaDo05a,
	Abstract = {We discuss a Continuous Curvelet Transform (CCT), a transform f --> Gamma(f)(a,b,θ) of functions f(x1,x2) on R^2 into a transform domain with continuous scale a>0, location b in R^2, and orientation θ. Here Gamma(f)(a,b,θ)=〈f,gamma_abθ〉projects f onto analyzing elements called curvelets gamma_abθ which are smooth and of rapid decay away from an a by sqrt(a) rectangle with minor axis pointing in direction θ. We call them curvelets because this anisotropic behavior allows them to `track' the behavior of singularities along curves. They are continuum scale/space/orientation analogs of the discrete frame of curvelets discussed in [E.J. Cand{\`e}s, F. Guo, New multiscale transforms, minimum total variation synthesis: applications to edge-preserving image reconstruction, Signal Process. 82 (2002) 1519--1543; E.J. Cand{\`e}s, L. Demanet, Curvelets and Fourier integral operators, C. R. Acad. Sci. Paris, S{\'e}r. I (2003) 395--398; E.J. Cand{\`e}s, D.L. Donoho, Curvelets: a surprisingly effective nonadaptive representation of objects with edges, in: A. Cohen, C. Rabut, L.L. Schumaker (Eds.), Curve and Surface Fitting: Saint-Malo 1999, Vanderbilt Univ. Press, Nashville, TN, 2000]. We use the CCT to analyze several objects having singularities at points, along lines, and along smooth curves. },
	Author = {Cand{\`e}s, E. J. and Donoho, D. L.},
	Date-Added = {2015-03-06 06:12:19 +0000},
	Date-Modified = {2015-03-06 06:13:13 +0000},
	Journal = {Applied and Computational Harmonic Analysis},
	Number = {2},
	Pages = {162---197},
	Title = {Continuous curvelet transform: {I}. {R}esolution of the wavefront set},
	Volume = {19},
	Year = {2005}}

@article{CaDo05b,
	Abstract = {We develop a unifying perspective on several decompositions exhibiting directional parabolic scaling. In each decomposition, the individual atoms are highly anisotropic at fine scales, with effective support obeying the parabolic scaling principle width≈length^2. Our comparisons allow to extend theorems known for one decomposition to others. We start from a continuous curvelet transform f --> Gamma(f)(a,b,θ) of functions f(x1,x2) on R^2, with parameter space indexed by scale a>0, location b in R^2, and orientation θ. The transform projects f onto a curvelet gamma_abθ, yielding coefficient Gamma(f)(a,b,θ)= f * gamma_abθ; the corresponding curvelet gamma_abθ  is defined by parabolic dilation in polar frequency domain coordinates. We establish a reproducing formula and Parseval relation for the transform, showing that these curvelets provide a continuous tight frame. The CCT is closely related to a continuous transform pioneered by Hart Smith in his study of Fourier Integral Operators. Smith's transform is based on true affine parabolic scaling of a single mother wavelet, while the CCT can only be viewed as true affine parabolic scaling in Euclidean coordinates by taking a slightly different mother wavelet at each scale. Smith's transform, unlike the CCT, does not provide a continuous tight frame. We show that, with the right underlying wavelet in Smith's transform, the analyzing elements of the two transforms become increasingly similar at increasingly fine scales. We derive a discrete tight frame essentially by sampling the CCT at dyadic intervals in scale, at equispaced intervals in direction, and equispaced sampling on a rotated anisotropic grid in space. This frame is a complexification of the `Curvelets 2002' frame in [E.J. Cand{\`e}s, F. Guo, Signal Process. 82 (2002) 1519--1543; E.J. Cand{\`e}s, L. Demanet, C. R. Acad. Sci. Paris, Ser. I 336 (2003) 395--398; E.J. Cand{\`e}s, D.L. Donoho, Comm. Pure Appl. Math. LVII (2004) 219--266]. We compare this discrete frame with a composite system which at coarse scales is the same as this frame but at fine scales is based on sampling Smith's transform rather than the CCT. We are able to show a very close approximation of the two systems at fine scales, in a strong operator norm sense. Smith's continuous transform was intended for use in forming molecular decompositions of Fourier Integral Operators (FIOs). Our results showing close approximation of the curvelet frame by a composite frame using true affine parabolic scaling at fine scales allow us to cross-apply Smith's results, proving that the discrete curvelet transform gives sparse representations of FIOs of order 0. This yields an alternate proof of a recent result of Cand{\`e}s and Demanet about the sparsity of FIO representations in discrete curvelet frames.},
	Author = {Cand{\`e}s, E. J. and Donoho, D. L.},
	Date-Added = {2015-03-06 06:05:43 +0000},
	Date-Modified = {2015-03-06 06:15:19 +0000},
	Journal = {Applied and Computational Harmonic Analysis},
	Number = {2},
	Pages = {198---222},
	Title = {Continuous curvelet transform: {II}. {D}iscretization and frames},
	Volume = {19},
	Year = {2005}}

@incollection{FlHeAvCaCo03,
	Abstract = {The Ridgelet Packets library provides a large family of orthonormal bases for functions f(x,y) in L2(dxdy) which includes orthonormal ridgelets as well as bases deriving from tilings reminiscent from the theory of wavelets and the study of oscillatory Fourier integrals. An intuitively appealing feature: many of these bases have elements whose envelope is strongly aligned along specified `ridges' while displaying oscillatory components across the main `ridge'.

There are two approaches to constructing ridgelet packets; the most direct is a frequency-domain viewpoint. We take a recursive dyadic partition of the polar Fourier domain into a collection of rectangular tiles of various widths and lengths. Focusing attention on each tile in turn, we take a tensor basis, using windowed sinusoids in θ times windowed sinusoids in r. There is also a Radon-domain approach to constructing ridgelet packets, which involves applying the Radon isometry and then, in the Radon plane, using wavelets in θ times wavelet packets in t, with the scales of the wavelets in the two directions carefully related.

We discuss digital implementations of the two continuum approaches, yielding many new frames for representation of digital images (i,j). These rely on two tools: the pseudopolar Fast Fourier Transform, and a pseudo Radon isometry called the normalized Slant Stack; these are described in Averbuch et al. (2001). In the Fourier approach, we mimic the continuum Fourier approach by partitioning the pseudopolar Fourier domain, building an orthonormal basis in the image space subordinate to each tile of the partition. On each rectangle of the partition, we use windowed sinusoids in θ times windowed sinusoids in r. In the Radon approach, we operate on the pseudo-Radon plane, and mimic the construction of orthonormal ridgelets, but with different scaling relationships between angular wavelets and ridge wavelets. Using wavelet packets in the ridge direction would also be possible.

Because of the wide range of possible ridgelet packet frames, the question arises: what is the best frame for a given dataset? Because of the Cartesian format of our 2-D pseudopolar domain, it is possible to apply best-basis algorithms for best anisotropic cosine packets bases; this will rapidly search among all such frames for the best possible frame according to a sparsity criterion --- compare N. Bennett's 1997 Yale Thesis. This automatically finds the best ridgelet packet frame for a given dataset.},
	Author = {Flesia, A. G. and Hel-Or, H. and Averbuch, A. and Cand{\`e}s, E. J. and Coifman, R. R. and Donoho, D. L.},
	Booktitle = {Beyond Wavelets},
	Date-Added = {2015-03-06 06:01:00 +0000},
	Date-Modified = {2015-03-06 06:03:23 +0000},
	Pages = {31--60},
	Publisher = {Academic Press/Elsevier},
	Series = {Studies in Computational Mathematics},
	Title = {Digital implementation of ridgelet packets},
	Volume = {10},
	Year = {2003}}

@phdthesis{Ca98,
	Abstract = {Single hidden-layer feedforward neural networks have b een prop osed as an approach to bypass the curse of dimensionality and are now becoming widely applied to approximation or prediction in applied sciences. In that approach one approximates a multivariate target function by a sum of ridge functions this is similar to projection pursuit in the literatureof statistics. This approach poses new and chalenging questions both at a practical and theoretical levels ranging from the construction of neural networks to their efficiency and capability. The topic of this thesis is to show that ridgelets, a new set of functions, provide an elegant to ol to answer some of these fundamental questions.

In the first part of the thesis, we introduce a special admissibility condition for neural activation functions. Using admissible neurons, we develop two linear transforms, namely the continous and discrete ridglet transforms. Both transforms represent quite general functions f as a superposition of ridge functions in a stable and concrete way. A frame of"nearly orthogonal" ridgelets underlies the discrete transform.

In the second part, we show how to use the ridgelet transform to derive new approximation bounds. That is, we intro duce a new family of smoothness classes and show how they model "real-life" signals by exhibiting some specific sorts of high-dimensional spatial inhomogeneities. Roughly speaking, finite linear combinations of ridgelets are optimal for approximating functions from these new classes. In addition, we use the ridgelet transform to study the limitations of neural networks. As a surprising and remarkable example, we discuss the case of approximating radial functions.

Finally, it is explained in the conclusion why these new ridgelet expansions overer decisive improvements over traditional neural networks.},
	Author = {Cand{\`e}s, E. J.},
	Date-Added = {2015-03-06 05:54:11 +0000},
	Date-Modified = {2015-03-06 05:58:52 +0000},
	School = {Stanford University},
	Title = {Ridgelets: Theory and Applications},
	Type = {Ph.D. thesis},
	Year = {1998}}

@book{Co03,
	Abstract = {Since their introduction in the 1980's, wavelets have become a powerful tool in mathematical analysis, with applications such as image compression, statistical estimation and numerical simulation of partial differential equations. One of their main attractive features is the ability to accurately represent fairly general functions with a small number of adaptively chosen wavelet coefficients, as well as to characterize the smoothness of such functions from the numerical behaviour of these coefficients. The theoretical pillar that underlies such properties involves approximation theory and function spaces, and plays a pivotal role in the analysis of wavelet-based numerical methods. 

This book offers a self-contained treatment of wavelets, which includes this theoretical pillar and it applications to the numerical treatment of partial differential equations. Its key features are:

1. Self-contained introduction to wavelet bases and related numerical algorithms, from the simplest examples to the most numerically useful general constructions.

2. Full treatment of the theoretical foundations that are crucial for the analysis
of wavelets and other related multiscale methods : function spaces, linear and nonlinear approximation, interpolation theory.

3. Applications of these concepts to the numerical treatment of partial differential equations : multilevel preconditioning, sparse approximations of differential and integral operators, adaptive discretization strategies.},
	Author = {Cohen, A.},
	Date-Added = {2015-03-06 05:52:49 +0000},
	Date-Modified = {2015-03-06 05:53:39 +0000},
	Publisher = {North-Holland},
	Series = {Studies in Mathematics and its Applications},
	Title = {Numerical Analysis of Wavelet Methods},
	Volume = {32},
	Year = {2003}}

@book{Da92,
	Abstract = {This monograph contains 10 lectures presented by Dr. Daubechies as the principal speaker at the 1990 CBMS-NSF Conference on Wavelets and Applications. Wavelets are a mathematical development that many experts think may revolutionize the world of information storage and retrieval. They are a fairly simple mathematical tool now being applied to the compression of data, such as fingerprints, weather satellite photographs, and medical x-rays - that were previously thought to be impossible to condense without losing crucial details. The opening chapter provides an overview of the main problems presented in the book. Following chapters discuss the theoretical and practical aspects of wavelet theory, including wavelet transforms, orthonormal bases of wavelets, and characterization of functional spaces by means of wavelets. The last chapter presents several topics under active research, as multidimensional wavelets, wavelet packet bases, and a construction of wavelets tailored to decompose functions defined in a finite interval.},
	Author = {Daubechies, I.},
	Date-Added = {2015-03-06 05:51:05 +0000},
	Date-Modified = {2015-03-06 05:51:56 +0000},
	Publisher = {SIAM},
	Series = {CBMS-NSF Regional Conference Series in Applied Mathematics},
	Title = {Ten Lectures on Wavelets},
	Volume = {61},
	Year = {1992}}

@incollection{PeCaVa14,
	Abstract = {We introduce iLang, a language and software framework for probabilistic inference. The iLang framework enables the definition of directed and undirected probabilistic graphical models and the automated synthesis of high performance inference algorithms for imaging applications. The iLang framework is composed of a set of language primitives and of an inference engine based on a message-passing system that integrates cutting-edge computational tools, including proximal algorithms and high performance Hamiltonian Markov Chain Monte Carlo techniques. A set of domain-specific highly optimized GPU-accelerated primitives specializes iLang to the spatial data-structures that arise in imaging applications. We illustrate the framework through a challenging application: spatio-temporal tomographic reconstruction with compressive sensing.
},
	Author = {Pedemonte, S. and Catana, C. and Van Leemput, K.},
	Booktitle = {Bayesian and {grAphical} Models for Biomedical Imaging},
	Date-Added = {2015-02-19 23:19:40 +0000},
	Date-Modified = {2015-02-19 23:22:10 +0000},
	Editor = {Cardoso, M. J. and Simpson, I. and Arbel, T. and Precup, D. and Ribbens, A.},
	Pages = {61--72},
	Publisher = {Springer-Verlag},
	Series = {Lecture Notes in Computer Science},
	Title = {An Inference Language for Imaging},
	Volume = {8677},
	Year = {2014}}

@book{Ci11,
	Abstract = {This book is an attempt at a comprehensive treatment of those medical imaging techniques commonly referred to as Computed Tomography (CT) and sometimes known as Computerised Tomography, which rely on X-rays for their action. As this is a place to explain my reasons for writing the book, I would like to begin by assuring the reader of my passion for the medical technology discussed here. My main motivation in publishing this work was a desire to share with the widest possible readership my fascination with the topic. I would expect the target audience for this account to be primarily academics, students and technicians involved with biomedical engineering, as well as doctors and medical technicians concerned with medical imaging. The structure and content of the book place particular emphasis on issues related to the reconstruction of images from projections, a key problem in tomography. This reflects my area of interest in the field. Other problems will be treated as technical and physical background to the reconstruction algorithms, in so far as is necessary for an understanding of how they work (and perhaps a little more). The reconstruction algorithms covered relate to all the basic designs of tomographic equipment, from the first Hounsfield scanner to the most recent spiral CT devices for reconstructing images in three dimensions. I hope that the summaries of various practical implementations of the algorithms will help people to test the individual reconstruction methods for themselves. The final chapter contains an account of a virtual test environment so that those without access to physical measurement data from a real scanner can carry out these tests. Perhaps it is a good point here to wish you the best of luck.},
	Author = {Cierniak, R.},
	Date-Added = {2015-02-19 23:11:46 +0000},
	Date-Modified = {2015-02-19 23:13:10 +0000},
	Publisher = {Springer-Verlag},
	Title = {X-Ray Computed Tomography in Biomedical Engineering},
	Year = {2011}}

@incollection{PoSc12,
	Abstract = {In this paper, for certain classes of functions, we present analytical evaluations of double integral expressions representing approximations of the total variation seminorm. These calculations are carried out by using the Maple computer algebra software package in a sophisticated way.

The derived expressions can be used for approximations of the total variation seminorm, and, in particular, can be used for efficient numerical total variation regularization energy minimization.},
	Author = {Pontow, C. and Scherzer, O.},
	Booktitle = {Numerical and Symbolic Scientific Computing},
	Date-Added = {2015-02-15 08:40:11 +0000},
	Date-Modified = {2015-02-15 08:41:50 +0000},
	Editor = {Langer, U. and Paule, P.},
	Pages = {193--218},
	Publisher = {Springer-Verlag},
	Series = {Texts and Monographs in Symbolic Computation},
	Title = {Analytical Evaluations of Double Integral Expressions Related to Total Variation},
	Year = {2012}}

@article{HaZh14,
	Abstract = {Tensor product real-valued wavelets have been employed in many applications such as image processing with impressive performance. Edge singularities are ubiquitous and play a fundamental role in image processing and many other two-dimensional problems. Tensor product real-valued wavelets are known to be only suboptimal since they can only capture edges well along the coordinate axis directions (that is, the horizontal and vertical directions in dimension two). Among several approaches in the literature to enhance the performance of tensor product real-valued wavelets, the dual tree complex wavelet transform ($DT-\mathbb{C}WT$), proposed by Kingsbury [Phil. Trans. R. Soc. Lond. A, 357 (1999), pp. 2543--2560] and further developed by Selesnick, Baraniuk, and Kingsbury [IEEE Signal Process. Mag., 22 (2005), pp. 123--151], is one of the most popular and successful enhancements of the classical tensor product real-valued wavelets by employing a correlated pair of orthogonal wavelet filter banks. The two-dimensional $DT-\mathbb{C}WT$ is obtained essentially via tensor product and offers improved directionality with six directions. In this paper we shall further enhance the performance of the $DT-\mathbb{C}WT$ for the problem of image denoising. Using a framelet-based approach and the notion of discrete affine systems, we shall propose a family of tensor product complex tight framelets $TP-\mathbb{C}TF_n$ for all integers $n\ge 3$ with increasing directionality, where $n$ refers to the number of filters in the underlying one-dimensional complex tight framelet filter bank. For dimension two, such a tensor product complex tight framelet $TP-\mathbb{C}TF_n$ offers $\frac{1}{2}(n-1)(n-3)+4$ directions when $n$ is odd and $\frac{1}{2}(n-4)(n+2)+6$ directions when $n$ is even. In particular, we shall show that $TP-\mathbb{C}TF_4$, which is different from the $DT-\mathbb{C}WT$ in both nature and design, provides an alternative to the $DT-\mathbb{C}WT$. Indeed, we shall see that $TP-\mathbb{C}TF_4$ behaves quite similar to the $DT-\mathbb{C}WT$ by offering six directions in dimension two, employing the tensor product structure, and enjoying slightly less redundancy than the $DT-\mathbb{C}WT$. Then we shall apply $TP-\mathbb{C}TF_n$ to the problem of image denoising. We shall see that the performance of $TP-\mathbb{C}TF_4$ for image denoising is comparable to that of the $DT-\mathbb{C}WT$. Better results on image denoising can be obtained by using other $TP-\mathbb{C}TF_n$, for example, n=6, which has 14 directions in dimension two. Moreover, $TP-\mathbb{C}TF_n$ allows us to further improve the $DT-\mathbb{C}WT$ by using $TP-\mathbb{C}TF_n$ as the first stage filter bank in the $DT-\mathbb{C}WT$. We shall also provide discussion and comparison of $TP-\mathbb{C}TF_n$ with several generalizations of the $DT-\mathbb{C}WT$, shearlets, and directional nonseparable tight framelets.},
	Author = {Han, B. and Zhao, Z.},
	Date-Added = {2015-02-15 08:35:43 +0000},
	Date-Modified = {2015-02-15 08:37:13 +0000},
	Journal = {SIAM Journal on Imaging Sciences},
	Number = {2},
	Pages = {997--1034},
	Title = {Tensor Product Complex Tight Framelets with Increasing Directionality},
	Volume = {7},
	Year = {2014}}

@article{BlTa13,
	Abstract = {Forappropriatematrixensembles,greedyalgorithmshaveproventobean efficient means of solving the combinatorial optimization problem associated with compressed sensing. This paper describes an implementation for graphics processing units (GPU) of hard thresholding, iterative hard thresholding, normalized iterative hard thresholding, hard thresholding pursuit, and a two-stage thresholding algorithm based on compressive sampling matching pursuit and subspace pursuit. The GPU acceleration of the former bottleneck, namely the matrix--vector multiplications, transfers a significant portion of the computational burden to the identification of the support set. The software solves high-dimensional problems in fractions of a second which permits large-scale testing at dimensions currently unavailable in the literature. The GPU implementations exhibit up to 70× acceleration over standard Matlab central processing unit implementations using automatic multi-threading.},
	Author = {Blanchard, J. and Tanner, J.},
	Date-Added = {2015-02-15 08:33:21 +0000},
	Date-Modified = {2015-02-15 08:33:57 +0000},
	Journal = {Mathematical Programming Computation},
	Pages = {267--304},
	Title = {{GPU} accelerated greedy algorithms for compressed sensing},
	Volume = {5},
	Year = {2013}}

@article{AdHaRoTe14,
	Abstract = {The purpose of this chapter is to report on recent approaches to reconstruction problems based on analog, (or in other words, infinite-dimensional) image and signal models. We describe three main contributions to this problem: first, linear reconstructions from sampled measurements via so-called generalized sampling (GS); second, the extension of generalized sampling to inverse and ill-posed problems; and third, the combination of generalized sampling with sparse recovery techniques. This final contribution leads to a theory and set of methods for infinite-dimensional compressed sensing, or as we shall also refer to it, compressed sensing over the continuum.},
	Author = {Adcock, B. and Hansen, A. and Roman, B. and Teschke, G.},
	Date-Added = {2015-02-15 08:20:07 +0000},
	Date-Modified = {2015-02-15 08:22:09 +0000},
	Journal = {Advances in Imaging and Electron Physics},
	Pages = {187--279},
	Title = {Generalized Sampling: Stable Reconstructions, Inverse Problems and Compressed Sensing over the Continuum},
	Volume = {182},
	Year = {2014}}

@article{LaStWoFa14,
	Abstract = {In this chapter, we review a variety of 3-D sparse representations developed in recent years and adapted to different kinds of 3-D signals. In particular, we describe 3-D wavelets, ridgelets, beamlets, and curvelets. We also present very recent 3-D sparse representations on the 3-D ball that has been adapted to 3-D signals naturally observed in spherical coordinates. Illustrative examples are provided for the different transforms.

},
	Author = {Lanusse, F. and Starck, J.-L. and Woiselle, A. and Fadili, J. M.},
	Date-Added = {2015-02-15 08:16:17 +0000},
	Date-Modified = {2015-02-15 08:18:04 +0000},
	Journal = {Advances in Imaging and Electron Physics},
	Pages = {99--204},
	Title = {{3-D} Sparse Representations},
	Volume = {183},
	Year = {2014}}

@article{LoScWe14,
	Abstract = {The linearized Bregman method is a method to calculate sparse solutions to systems of linear equations. We formulate this problem as a split feasibility problem, propose an algorithmic framework based on Bregman projections, and prove a general convergence result for this framework. Convergence of the linearized Bregman method will be obtained as a special case. Our approach also allows for several generalizations such as other objective functions, incremental iterations, incorporation of non-Gaussian noise models, and box constraints.},
	Author = {Lorenz, D. A. and Sch{\"o}pfer, F. and Wenger, S.},
	Date-Added = {2015-02-15 08:13:02 +0000},
	Date-Modified = {2015-02-15 08:14:13 +0000},
	Journal = {SIAM Journal on Imaging Sciences},
	Number = {2},
	Pages = {1237--1262},
	Title = {The Linearized {B}regman Method via Split Feasibility Problems: Analysis and Generalizations},
	Volume = {7},
	Year = {2014}}

@article{GoODSeBa14,
	Abstract = {Alternating direction methods are a common tool for general mathematical programming and optimization. These methods have become particularly important in the field of variational image processing, which frequently requires the minimization of nondifferentiable objectives. This paper considers accelerated (i.e., fast) variants of two common alternating direction methods: the alternating direction method of multipliers (ADMM) and the alternating minimization algorithm (AMA). The proposed acceleration is of the form first proposed by Nesterov for gradient descent methods. In the case that the objective function is strongly convex, global convergence bounds are provided for both classical and accelerated variants of the methods. Numerical examples are presented to demonstrate the superior performance of the fast methods for a wide variety of problems.},
	Author = {Goldstein, T. and O'Donoghue, B. and Setzer, S. and Baraniuk, R.},
	Date-Added = {2015-02-15 08:09:07 +0000},
	Date-Modified = {2015-02-15 08:10:06 +0000},
	Journal = {SIAM Journal on Imaging Sciences},
	Number = {3},
	Pages = {1588--1623},
	Title = {Fast Alternating Direction Optimization Methods},
	Volume = {7},
	Year = {2014}}

@book{NaWu01,
	Abstract = {Since the advent of computerized tomography in radiology, many imaging techniques have been introduced in medicine, science, and technology. This book describes the state of the art of the mathematical theory and numerical analysis of imaging. The authors survey and provide a unified view of imaging techniques, provide the necessary mathematical background and common framework, and give a detailed analysis of the numerical algorithms. This book not only reflects the theoretical progress and the growth of the field in the last 10 years but also serves as an excellent reference. It will provide readers with a superior understanding of the mathematical principles behind imaging and will enable them to write state-of-the-art software as a result.

Mathematical Methods in Image Reconstruction provides a very detailed description of two-dimensional algorithms. For three-dimensional algorithms, the authors derive exact and approximate inversion formulas for specific imaging devices and describe their algorithmic implementation (which by and large parallels the two-dimensional algorithms). Integral geometry is surveyed as far as is necessary for imaging purposes; imaging techniques based on or related to integral geometry are briefly described in the section on tomography.

Some of the applications covered in the book include computerized tomography, magnetic resonance imaging, emission tomography, electron microscopy, ultrasound transmission tomography, industrial tomography, seismic tomography, impedance tomography, and NIR imaging. The authors provide the necessary mathematical background and common mathematical framework needed to understand the book. Knowledge of tomography literature from the 1980s will be useful to the reader.},
	Author = {Natterer, F. and W{\"u}bbeling, F.},
	Date-Added = {2015-02-15 07:51:23 +0000},
	Date-Modified = {2015-02-15 07:51:55 +0000},
	Publisher = {SIAM},
	Series = {{SIAM} monographs on mathematical modeling and computation},
	Title = {Mathematical methods in image reconstruction},
	Volume = {5},
	Year = {2001}}

@article{PuGoYa05,
	Abstract = {Digital image reconstruction is a robust means by which the underlying images hidden in blurry and noisy data can be revealed. The main challenge is sensitivity to measurement noise in the input data, which can be magnified strongly, resulting in large artifacts in the reconstructed image. The cure is to restrict the permitted images. This review summarizes image reconstruction methods in current use. Progressively more sophisticated image restrictions have been developed, including (a) filtering the input data, (b) regularization by global penalty functions, and (c) spatially adaptive methods that impose a variable degree of restriction across the image. The most reliable reconstruction is the most conservative one, which seeks the simplest underlying image consistent with the input data. Simplicity is context-dependent, but for most imaging applications, the simplest reconstructed image is the smoothest one. Imposing the maximum, spatially adaptive smoothing permitted by the data results in the best image reconstruction.
},
	Author = {Puetter, R. C. and Gosnell, T. R. and Yahil, A.},
	Date-Added = {2015-02-15 07:48:43 +0000},
	Date-Modified = {2015-02-15 07:49:56 +0000},
	Journal = {Annual Review of Astronomy and Astrophysics},
	Pages = {139--194},
	Title = {Digital Image Reconstruction: Deblurring and Denoising},
	Volume = {43},
	Year = {2005}}

@incollection{BoLu11,
	Abstract = {This chapter surveys key concepts in convex duality theory and their application to the analysis and numerical solution of problem archetypes in imaging.},
	Author = {Borwein, J. M. and Luke, R. D.},
	Booktitle = {Handbook of Mathematical Methods in Imaging},
	Chapter = {7},
	Date-Added = {2015-02-15 07:35:45 +0000},
	Date-Modified = {2015-02-15 07:38:07 +0000},
	Editor = {Scherzer, O.},
	Pages = {229--270},
	Publisher = {Springer-Verlag},
	Title = {Duality and Convex Programming},
	Year = {2011}}

@article{SjFoAnKn15,
	Abstract = {Radiotherapy planning and attenuation correction of PET images require simulation of radiation transport. The necessary physical properties are typically derived from computed tomography (CT) images, but in some cases, including stereotactic neurosurgery and combined PET/MR imaging, only magnetic resonance (MR) images are available. With these applications in mind, we describe how a realistic, patient-specific, pseudo-CT of the head can be derived from anatomical MR images. We refer to the method as atlas-based regression, because of its similarity to atlas-based segmentation.

Given a target MR and an atlas database comprising MR and CT pairs, atlas-based regression works by registering each atlas MR to the target MR, applying the resulting displacement fields to the corresponding atlas CTs and, finally, fusing the deformed atlas CTs into a single pseudo-CT.

We use a deformable registration algorithm known as the Morphon and augment it with a certainty mask that allows a tailoring of the influence certain regions are allowed to have on the registration. Moreover, we propose a novel method of fusion, wherein the collection of deformed CTs is iteratively registered to their joint mean and find that the resulting mean CT becomes more similar to the target CT. However, the voxelwise median provided even better results; at least as good as earlier work that required special MR imaging techniques. This makes atlas-based regression a good candidate for clinical use.},
	Author = {Sj{\"o}lund, J. and Forsberg, D. and Andersson, M. and Knutsson, H.},
	Date-Added = {2015-02-12 06:08:56 +0000},
	Date-Modified = {2015-02-12 06:10:21 +0000},
	Journal = {Physics in Medicine and Biology},
	Pages = {825--839},
	Title = {Generating patient specific pseudo-{CT} of the head from {MR} using atlas-based regression},
	Volume = {60},
	Year = {2015}}

@inproceedings{Lo04,
	Abstract = {The MATLAB toolbox YALMIP is introduced. It is described how YALMIP can be used to model and solve optimization problems typically occurring in systems and control theory. In this paper, free MATLAB toolbox YALMIP, developed initially to model SDPs and solve these by interfacing eternal solvers. The toolbox makes development of optimization problems in general, and control oriented SDP problems in particular, extremely simple. In fact, learning 3 YALMIP commands is enough for most users to model and solve the optimization problems
},
	Author = {L{\"o}fberg, J.},
	Booktitle = {IEEE International Symposium on Computer Aided Control Systems Design, 2004},
	Date-Added = {2015-02-09 13:06:23 +0100},
	Date-Modified = {2015-02-09 13:07:44 +0100},
	Pages = {284--289},
	Title = {{YALMIP}: {A} Toolbox for Modeling and Optimization in {MATLAB}},
	Year = {2004}}

@article{Lo12,
	Abstract = {The number of available algorithms for the so-called Basis Pursuit Denoising problem (or the related LASSO-problem) is large and keeps growing. Similarly, the number of experiments to evaluate and compare these algorithms on different instances is growing. In this correspondence, we present a method to produce instances with exact solutions that is based on a simple observation, related to the so-called source condition from sparse regularization.

},
	Author = {Lorenz, D. A.},
	Date-Added = {2015-02-08 15:08:27 +0000},
	Date-Modified = {2015-02-08 15:09:14 +0000},
	Journal = {IEEE Transactions in Signal Processing},
	Number = {5},
	Pages = {1210--1214},
	Title = {Constructing Test Instances for Basis Pursuit Denoising},
	Volume = {61},
	Year = {2012}}

@techreport{LoPfTi13,
	Abstract = {The problem of finding a minimum l^1-norm solution to an underdetermined linear system is an important problem in compressed sensing, where it is also known as basis pursuit. We propose a heuristic optimality check as a general tool for l^1-minimization, which often allows for early termination by ``guessing'' a primal-dual optimal pair based on an approximate support. Moreover, we provide an extensive numerical comparison of various state-of-the-art l^1-solvers that have been proposed during the last decade, on a large test set with a variety of explicitly given matrices and several right hand sides per matrix reflecting different levels of solution difficulty. The results, as well as improvements by the proposed heuristic optimality check, are analyzed in detail to provide an answer to the question which algorithm is the best.

},
	Author = {Lorenz, D. A. and Pfetsch, M. E. and Tillmann, A. M.},
	Date-Added = {2015-02-08 15:05:07 +0000},
	Date-Modified = {2015-02-08 15:07:27 +0000},
	Institution = {Optimization Online},
	Number = {Preprint},
	Title = {Solving Basis Pursuit: Heuristic Optimality Check and Solver Comparison},
	Year = {2013}}

@article{BeCaGr11,
	Abstract = {This paper develops a general framework for solving a variety of convex cone problems that frequently arise in signal processing, machine learning, statistics, and other fields. The approach works as follows: first, determine a conic formulation of the problem; second, determine its dual; third, apply smoothing; and fourth, solve using an optimal first-order method. A merit of this approach is its flexibility: for example, all compressed sensing problems can be solved via this approach. These include models with objective functionals such as the total-variation norm, 
| W x |_1 where W is arbitrary, or a combination thereof. In addition, the paper also introduces a number of technical contributions such as a novel continuation scheme, a novel approach for controlling the step size, and some new results showing that the smooth and unsmoothed problems are sometimes formally equivalent. Combined with our framework, these lead to novel, stable and computationally efficient algorithms. For instance, our general implementation is competitive with state-of-the-art methods for solving intensively studied problems such as the LASSO. Further, numerical experiments show that one can solve the Dantzig selector problem, for which no efficient large-scale solvers exist, in a few hundred iterations. Finally, the paper is accompanied with a software release. This software is not a single, monolithic solver; rather, it is a suite of programs and routines designed to serve as building blocks for constructing complete algorithms.},
	Author = {Becker, S. and Cand{\`e}s, E. J. and Grant, M.},
	Date-Added = {2015-02-08 14:54:41 +0000},
	Date-Modified = {2015-02-08 14:56:32 +0000},
	Journal = {Mathematical Programming Computation},
	Number = {3},
	Pages = {165--218},
	Title = {Templates for convex cone problems with applications to sparse signal recovery},
	Volume = {3},
	Year = {2011}}

@article{AfBiFi11,
	Abstract = {We propose a new fast algorithm for solving one of the standard approaches to ill-posed linear inverse problems (IPLIP), where a (possibly nonsmooth) regularizer is minimized under the constraint that the solution explains the observations sufficiently well. Although the regularizer and constraint are usually convex, several particular features of these problems (huge dimensionality, nonsmoothness) preclude the use of off-the-shelf optimization tools and have stimulated a considerable amount of research. In this paper, we propose a new efficient algorithm to handle one class of constrained problems (often known as basis pursuit denoising) tailored to image recovery applications. The proposed algorithm, which belongs to the family of augmented Lagrangian methods, can be used to deal with a variety of imaging IPLIP, including deconvolution and reconstruction from compressive observations (such as MRI), using either total-variation or wavelet-based (or, more generally, frame-based) regularization. The proposed algorithm is an instance of the so-called alternating direction method of multipliers, for which convergence sufficient conditions are known; we show that these conditions are satisfied by the proposed algorithm. Experiments on a set of image restoration and reconstruction benchmark problems show that the proposed algorithm is a strong contender for the state-of-the-art.
},
	Author = {Afonso, M. and Bioucas-Dias, J. and Figueiredo, M.},
	Date-Added = {2015-02-08 14:23:50 +0000},
	Date-Modified = {2015-02-08 14:24:32 +0000},
	Journal = {IEEE Transactions on Image Processing},
	Number = {3},
	Pages = {681--695},
	Title = {An augmented {L}agrangian approach to the constrained optimization formulation of imaging inverse problems},
	Volume = {20},
	Year = {2011}}

@article{AfBiFi10,
	Abstract = {We propose a new fast algorithm for solving one of the standard formulations of image restoration and reconstruction which consists of an unconstrained optimization problem where the objective includes an l2 data-fidelity term and a nonsmooth regularizer. This formulation allows both wavelet-based (with orthogonal or frame-based representations) regularization or total-variation regularization. Our approach is based on a variable splitting to obtain an equivalent constrained optimization formulation, which is then addressed with an augmented Lagrangian method. The proposed algorithm is an instance of the so-called alternating direction method of multipliers, for which convergence has been proved. Experiments on a set of image restoration and reconstruction benchmark problems show that the proposed algorithm is faster than the current state of the art methods.

},
	Author = {Afonso, M. and Bioucas-Dias, J. and Figueiredo, M.},
	Date-Added = {2015-02-08 14:21:31 +0000},
	Date-Modified = {2015-02-08 14:22:44 +0000},
	Journal = {IEEE Transactions on Image Processing},
	Number = {9},
	Pages = {2345--2356},
	Title = {Fast image recovery using variable splitting and constrained optimization},
	Volume = {10},
	Year = {2010}}

@article{BeBoCa11,
	Abstract = {Accurate signal recovery or image reconstruction from indirect and possibly undersampled data is a topic of considerable interest; for example, the literature in the recent field of compressed sensing is already quite immense. This paper applies a smoothing technique and an accelerated first-order algorithm, both from Nesterov [Math. Program. Ser. A, 103 (2005), pp. 127--152], and demonstrates that this approach is ideally suited for solving large-scale compressed sensing reconstruction problems as (1) it is computationally efficient, (2) it is accurate and returns solutions with several correct digits, (3) it is flexible and amenable to many kinds of reconstruction problems, and (4) it is robust in the sense that its excellent performance across a wide range of problems does not depend on the fine tuning of several parameters. Comprehensive numerical experiments on realistic signals exhibiting a large dynamic range show that this algorithm compares favorably with recently proposed state-of-the-art methods. We also apply the algorithm to solve other problems for which there are fewer alternatives, such as total-variation minimization and convex programs seeking to minimize the $\ell_1$ norm of $Wx$ under constraints, in which W is not diagonal. The code is available online as a free package in the MATLAB language.
},
	Author = {Becker, S. and Bobin, J. and Cand{\`e}s, E. J.},
	Date-Added = {2015-02-08 11:06:40 +0000},
	Date-Modified = {2015-02-08 11:08:29 +0000},
	Journal = {SIAM Journal on Imaging Sciences},
	Number = {1},
	Pages = {1--39},
	Title = {{NESTA}: {A} Fast and Accurate First-Order Method for Sparse Recovery},
	Volume = {4},
	Year = {2011}}

@article{BeFr11,
	Abstract = {The use of convex optimization for the recovery of sparse signals from incomplete or compressed data is now common practice. Motivated by the success of basis pursuit in recovering sparse vectors, new formulations have been proposed that take advantage of different types of sparsity. In this paper we propose an efficient algorithm for solving a general class of sparsifying formulations. For several common types of sparsity we provide applications, along with details on how to apply the algorithm, and experimental results.
},
	Author = {van den Berg, E. and Friedlander, M. P.},
	Date-Added = {2015-02-08 11:00:48 +0000},
	Date-Modified = {2015-02-08 11:01:46 +0000},
	Journal = {SIAM Journal on Optimization},
	Number = {4},
	Pages = {1201---1229},
	Title = {Sparse optimization with least-squares constraints},
	Volume = {21},
	Year = {2011}}

@article{MeOlHoWi07,
	Abstract = {Object-oriented programming is a relatively new tool in the development of optimization software. The code extensibility and the rapid algorithm prototyping capability enabled by this programming paradigm promise to enhance the reliability, utility, and ease of use of optimization software. While the use of object-oriented programming is growing, there are still few examples of general purpose codes written in this manner, and a common approach is far from obvious. This paper describes OPT++, a C++ class library for nonlinear optimization. The design is predicated on the concept of distinguishing between an algorithm-independent class hierarchy for nonlinear optimization problems and a class hierarchy for nonlinear optimization methods that is based on common algorithmic traits. The interface is designed for ease of use while being general enough so that new optimization algorithms can be added easily to the existing framework. A number of nonlinear optimization algorithms have been implemented in OPT++ and are accessible through this interface. Furthermore, example applications demonstrate the simplicity of the interface as well as the advantages of a common interface in comparing multiple algorithms.},
	Author = {Meza, J. C. and Oliva, R. A. and Hough, P. D. and Williams, P. J.},
	Date-Added = {2015-02-08 10:23:21 +0000},
	Date-Modified = {2015-02-08 10:24:32 +0000},
	Journal = {ACM Transactions on Mathematical Software},
	Number = {2},
	Title = {{OPT++}: {A}n object-oriented toolkit for nonlinear optimization},
	Volume = {33},
	Year = {2007}}

@article{PeJaMa12,
	Abstract = {We present pyOpt, an object-oriented framework for formulating and solving nonlinear constrained optimization problems in an efficient, reusable and portable manner. The framework uses object-oriented concepts, such as class inheritance and operator overloading, to maintain a distinct separation between the problem formulation and the optimization approach used to solve the problem. This creates a common interface in a flexible environment where both practitioners and developers alike can solve their optimization problems or develop and benchmark their own optimization algorithms. The framework is developed in the Python programming language, which allows for easy integration of optimization software programmed in Fortran, C, C+ +, and other languages. A variety of optimization algorithms are integrated in pyOpt and are accessible through the common interface. We solve a number of problems of increasing complexity to demonstrate how a given problem is formulated using this framework, and how the framework can be used to benchmark the various optimization algorithms.
},
	Author = {Perez, R. E. and Jansen, P. W. and Martins, J. R. R. A.},
	Date-Added = {2015-02-08 08:55:12 +0000},
	Date-Modified = {2015-02-08 08:56:25 +0000},
	Journal = {Structural and Multidisciplinary Optimization},
	Number = {1},
	Pages = {101--118},
	Title = {{pyOpt}: a {P}ython-based object-oriented framework for nonlinear constrained optimization},
	Volume = {45},
	Year = {2012}}

@incollection{ByNoWa06,
	Abstract = {This paper describes Knitro 5.0, a C-package for nonlinear optimization that combines complementary approaches to nonlinear optimization to achieve robust performance over a wide range of application requirements. The package is designed for solving large-scale, smooth nonlinear programming problems, and it is also effective for the following special cases: unconstrained optimization, nonlinear systems of equations, least squares, and linear and quadratic programming. Various algorithmic options are available, including two interior methods and an active-set method. The package provides crossover techniques between algorithmic options as well as automatic selection of options and settings.
},
	Author = {Byrd, R. H. and Nocedal, J. and Waltz, R. A.},
	Booktitle = {Large-Scale Nonlinear Optimization},
	Date-Added = {2015-02-08 08:30:54 +0000},
	Date-Modified = {2015-02-08 08:32:21 +0000},
	Editor = {di Pillo, G. and Roma, M.},
	Pages = {35--59},
	Publisher = {Springer-Verlag},
	Title = {{KNITRO}: An Integrated Package for Nonlinear Optimization},
	Year = {2006}}

@article{GoOrTo04,
	Abstract = {We describe the design of version 1.0 of GALAHAD, a library of Fortran 90 packages for large-scale nonlinear optimization. The library particularly addresses quadratic programming problems, containing both interior point and active set algorithms, as well as tools for preprocessing problems prior to solution. It also contains an updated version of the venerable nonlinear programming package, LANCELOT.},
	Author = {Gould, N. I. M. and Orban, D. and Toint, P. L.},
	Date-Added = {2015-02-08 08:26:36 +0000},
	Date-Modified = {2015-02-08 08:28:19 +0000},
	Journal = {ACM Transactions on Mathematical Software},
	Number = {4},
	Pages = {353--372},
	Title = {{GALAHAD}, a library of thread-safe {F}ortran 90 packages for large-scale nonlinear optimization},
	Volume = {29},
	Year = {2004}}

@article{GiMuSa05,
	Abstract = {Sequential quadratic programming (SQP) methods have proved highly effective for solving constrained optimization problems with smooth nonlinear functions in the objective and constraints. Here we consider problems with general inequality constraints (linear and nonlinear). We assume that first derivatives are available and that the constraint gradients are sparse. Second derivatives are assumed to be unavailable or too expensive to calculate.

We discuss an SQP algorithm that uses a smooth augmented Lagrangian merit function and makes explicit provision for infeasibility in the original problem and the QP subproblems. The Hessian of the Lagrangian is approximated using a limited-memory quasi-Newton method.

SNOPT is a particular implementation that uses a reduced-Hessian semidefinite QP solver (SQOPT) for the QP subproblems. It is designed for problems with many thousands of constraints and variables but is best suited for problems with a moderate number of degrees of freedom (say, up to 2000). Numerical results are given for most of the CUTEr and COPS test collections (about 1020 examples of all sizes up to 40000 constraints and variables, and up to 20000 degrees of freedom).
},
	Author = {Gill, P. E. and Murray, W. and Saunders, M. A.},
	Date-Added = {2015-02-08 08:23:08 +0000},
	Date-Modified = {2015-02-08 08:24:33 +0000},
	Journal = {SIAM Rview},
	Number = {1},
	Pages = {99--131},
	Title = {{SNOPT}: An {SQP} Algorithm for Large-Scale Constrained Optimization},
	Volume = {47},
	Year = {2005}}

@conference{MaJoEr91,
	Author = {MacLeod, R. S. and Johnson, C. R. and Ershler, P. R.},
	Booktitle = {IEEE Engineering in Medicine and Biology Society 13th Annual International Conference},
	Date-Added = {2015-02-08 07:35:08 +0000},
	Date-Modified = {2015-02-08 07:36:11 +0000},
	Pages = {688--689},
	Title = {Construction of an Inhomogeneous Model of the Human Torso for Use in Computational Electrocardiography},
	Year = {1991}}

@book{FoRa13,
	Abstract = {At the intersection of mathematics, engineering, and computer science sits the thriving field of compressive sensing. Based on the premise that data acquisition and compression can be performed simultaneously, compressive sensing finds applications in imaging, signal processing, and many other domains. In the areas of applied mathematics, electrical engineering, and theoretical computer science, an explosion of research activity has already followed the theoretical results that highlighted the efficiency of the basic principles. The elegant ideas behind these principles are also of independent interest to pure mathematicians.

A Mathematical Introduction to Compressive Sensing gives a detailed account of the core theory upon which the field is build. Key features include:

- The first textbook completely devoted to the topic of compressive sensing

- Comprehensive treatment of the subject, including background material from probability theory, detailed proofs of the main theorems, and an outline of possible applications

- Numerous exercises designed to help students understand the material

- An extensive bibliography with over 500 references that guide researchers through the literature

With only moderate prerequisites, A Mathematical Introduction to Compressive Sensing is an excellent textbook for graduate courses in mathematics, engineering, and computer science. It also serves as a reliable resource for practitioners and researchers in these disciplines who want to acquire a careful understanding of the subject.

},
	Author = {Foucart, S. and Rauhut, H.},
	Date-Added = {2015-02-08 07:19:56 +0000},
	Date-Modified = {2015-02-08 07:22:56 +0000},
	Publisher = {Springer-Verlag},
	Series = {Applied and Numerical Harmonic Analysis},
	Title = {A Mathematical Introduction to Compressive Sensing},
	Year = {2013}}

@conference{HoEdGo03,
	Abstract = {The Tomlab Optimization Environment is the most powerful optimization package available in MATLAB. TOMLAB of today has grown to include more than 70 different algorithms for linear, discrete, global and nonlinear optimization and also includes a large number of C and Fortran solvers 1 implemented as MEX interfaces. A growing number of companies like ILOG S.A., Dash Optimization Ltd, and Stanford Business Software Inc. are joining the TOMLAB partner network to provide better software for the practitioners. The overall objective is to provide the best modeling and optimization tools for the MATLAB user and thereby enable a wide range of opportunities for large-scale robust optimization for practically every area of optimization. The TOMLAB environment is call-compatible with MathWorks' Optimization Toolbox and supports problems formulated in the AMPL modeling language. This gives the MATLAB user unique functionality for model building and a conversion tool to the TOMLAB format. The call-compatibility is an important feature as a larger number of solvers can be evaluated for a specific problem area. Tomlab Optimization emphasizes the great importantance of using high-quality numerical software and a uniform approach in the solution process of optimization problems. The procedure for solving large-scale optimization problems in TOMLAB can be viewed as a five step process.

1. Define your problem in MATLAB by using the TOMLAB tools applicable to your area of optimization. This will create a standardized problem structure to work with.
2. By using the TOMLAB driver routine, tomRun, evaluate every solver that can potentially solve your problem. The routine will automatically make sure that the solvers have the correct format.
3. Exclude the solvers that do not perform or cannot solve the problem.
4. Tune the solvers, add problem patterns, and make sure that the tolerance levels are equivalent.
5. Continue to evaluate the solvers, and select the one that fit your needs.

This paper aims to describe how TOMLAB has enabled more options and necessary tools for the user in their solution process. The TOMLAB gateway and driver routines for automatic format mapping to different solver types, as well as the integration with other MATLAB toolboxes will be described in detail. The paper also exemplifies how customers have used this added functionality to embed parts of the TOMLAB solver portfolio in their products and systems. The following list shows some of the current application areas.
* TOMLAB has been selected as the optimization platform in DOTS, a 3 year European Commission project for the pulp and paper industry led by KCL, Finland.
* Halliburton Energy Services uses TOMLAB for application development of NMR technology in their MCC-compiled NMRStudio package.
* Philips and Lumileds uses MCC compiled Tomlab /Xpress for on-site plant production planning systems.
* TOMLAB has been embedded in portfolio optimization software.
* Many financial institutions now solve their problem faster with Financial Toolbox using TOMLAB.
* Claremont University in cooperation with JPL/NASA uses TOMLAB to optimize the dataflow from a Mars lander through the orbits and back to earth. To be embedded in mission planning system.},
	Author = {Holmstr{\"o}m, K. and Edvall, M. M. and G{\"o}ran, A. O.},
	Booktitle = {Proceedings, Nordic {MATLAB} Conference 2003, October 21, 2003},
	Date-Added = {2015-02-07 10:51:28 +0000},
	Date-Modified = {2015-02-07 10:54:14 +0000},
	Title = {{TOMLAB} -- for Large-Scale Robust Optimization},
	Year = {2003}}

@book{Sch11,
	Abstract = {We present an evaluation of state-of-the-art computer hardware architectures for implementing the FDK method, which solves the three-dimensional image reconstruction task in cone-beam computed tomography (CT). The computational complexity of the FDK method prohibits its use for many clinical applications unless appropriate hardware acceleration is employed. Today's most powerful hardware architectures for high-performance computing applications are based on standard multi-core processors, off-the-shelf graphics boards, the Cell Broadband Engine Architecture (CBEA), or customized accelerator platforms (e.g., FPGA-based computer components).

For each hardware platform under consideration, we describe a thoroughly optimized implementation of the most time-consuming parts of the FDK method; the filtering step as well as the subsequent back-projection step. We further explain the required code transformations to parallelize the algorithm for the respective target architecture. We compare both the implementation complexity and the resulting performance of all architectures under consideration using the same two medical datasets which have been acquired using a standard C-arm device. Our optimized backprojection implementations achieve at least a speed-up of 6.5 (CBEA), 22.0 (GPU), and 35.8 (FPGA) compared to a standard workstation equipped with a quad-core processor. It is demonstrated that three hardware platforms (namely CBEA, GPUs, and FPGA-based architectures) enable real-time CT reconstruction and therefore support highly efficient clinical workflow.

We further describe and evaluate an optimized CBEA-based implementation of the M-line method, which is a theoretically exact and stable reconstruction algorithm. The M-line method solves the problem of cone-artifacts, which may cover small object details, thus providing excellent image quality. Its implementation, however, has an increased computational complexity as the M-line method requires additional computations for the filtering of the projection images, e.g. derivative computation and filtering along oblique lines in the projections. The execution time of filtering increases by a factor of 3.5 compared to the FDK method. Nevertheless, we are able to demonstrate on-the-fly rconstruction capability on a dual Cell Blade.

Finally, we present an efficient implementation of the computationally most demanding steps in iterative reconstruction algorithms on off-the-shelf graphics boards. Because the back-projection step can be implemented similar to the FDK method we especially consider the forward-projection step. Our implementation is based on a ray casting algorithm in order to make efficient use of the texture hardware in current graphics accelerators. Using a reasonable parameter configuration the forward-projection step requires roughly twice as much processing time as the back-projection step.},
	Author = {Scherl, H.},
	Date-Added = {2015-02-06 20:16:57 +0000},
	Date-Modified = {2015-02-06 20:18:48 +0000},
	Publisher = {Vieweg+Teubner Verlag},
	Title = {Evaluation of State-of-the-Art Hardware Architectures for Fast Cone-Beam {CT} Reconstruction},
	Year = {2011}}

@book{GzVe11,
	Abstract = {These lecture notes were originally prepared as backup material for a course on Inverse Problems and Maximum Entropy taught at the Venezuelan School of Mathematics. The event takes place annually in the city of Merida, a university town in the Venezuelan Andean mountains. The attendance, mainly graduate students are exposed material that goes a bit beyond the standard courses.

The course had three aims. On one hand, to present some basic results about linear inverse problems and how to solve them. On the other, to develop the method of maximum entropy in the mean, and to apply it to study linear inverse problems. This would show the weaknesses and strengths of both approaches. The other aim was to acquaint the participants with the use of the software that is provided along with the book. This consists of interactive screens on which the data of typical problems can be uploaded, and a solution is provided.

The present notes eliminate many mistakes and misprints that plague the original version, and hopefully not many new ones crept in the new material that we added for this version. The material was reorganized slightly, new applications were added, but no real effort was undertaken to update the enormous literature on applications of the maximum entropy method.},
	Author = {Gzyl, H. and Velasquez, Y.},
	Date-Added = {2015-02-06 19:58:17 +0000},
	Date-Modified = {2015-02-06 19:59:13 +0000},
	Publisher = {World Scienific},
	Series = {Series on Advances in Mathematics for Applied Sciences},
	Title = {Linear Inverse Problems. The Maximum Entropy Connection},
	Volume = {83},
	Year = {2011}}

@book{DaJiSc08,
	Abstract = {In the past 40 years the field of tomography has evolved from simple X-ray projection imaging, autoradiography, fluorescent imaging and magnetic resonance imaging to production of the three dimensional images of the accumulation of chemical tracers injected into animals, plants and human subjects. The first major breakthrough technology since R ̈ontgen's discovery of X-ray projection imaging was X-Ray computer-assisted tomography known in the 1970s as CAT scanning or just CT. The method allowed one to compute the internal distribution of tissue characteristics and injected contrast material from the amount of X-Rays (photons) transmitted through an object from multiple angles. Though analog methods were available to accomplish the representation of what is inside an object from projections by merely back projecting the beams onto photographic material, the digital computer-enabled X-Ray computer assisted tomography to revolutionize medical radiography. X-rays aimed in multiple directions through an object (patient) allowed measurements that could be used to calculate the most likely distribution of tissue parameters that would result in the data observed by detectors positioned at multiple angles around the object.

The mathematical problem is as follows. Given the number of X-Ray photons entering the object in a particular direction and given the number of photons that were able to exit the object, estimate the amount of material that attenuated the photons and do this estimation for many paths through the object. The problem is generally known as the inverse problem wherein an unknown is calculated from known observations that represent or are a transformation of the unknown. Mathematically this turns out to be a simple manipulation because the ratio between the number of photons from the X-Ray generator that enter the body at a particular angle to the number of photons exiting the body along a line from the X-ray generator to that detector can be converted to the sum of the individual pieces of tissue between the X-ray input and the detected output for a particular line through the object. The conversion was merely the logarithm of this ratio.

By accumulating these ratios from many projections of X-rays, one can answer the question: What is the most likely distribution of tissue attenuation that can give the observed results? The X-Ray CAT scan is the 3D distribution of attenuation which is generally equivalent to the distribution of electron density.

So if this problem of tomography was solved 40 years ago with the main progress being made in perfection of computer methods to achieve and display the information as well as methods to improve the collection of information, why this book? Indeed, even progress in removal of artifacts from motion such as respiration and heart motion and changes in photon energy spectra within the object have been dealt with, so why not use the X-Ray computed tomography methods for imaging injected radionuclides, radiopharmaceuticals and fluorescent molecules? The most concise answer is that emission tomography has four unknowns for a six-parameter problem but transmission tomography has only one unknown for a five-parameter problem.

In X-ray computed tomography the mathematical problem is to compute the amount of attenuation given the known input X-ray photons and the known number of output photons detected from the object at every angle or projection. But, in emission tomography the problem is to compute the distribution of photon sources inside the body without knowing the intensity of the sources, nor their position, nor the amount that is attenuated and the contamination of the detected photons from scattering elsewhere in the object. What is known is the amount of photons that get out of the object along a particular trajectory (see Table 0.1). Thus we must estimate the source strengths, their position, and the attenuation of the emitted photons as well as how many of the photons detected were scattered from multiple sources. To a mathematician this is an intractable problem (cf. Phys. Med Biol 19:387-389, 1974); nevertheless it is solved by methods discussed in this book along with methods to compensate for motion, partial volume, registration and other factors that influence detector performance.

It is refreshing to have a text on emission molecular imaging relevant to animals and human beings with an emphasis on those factors that detract from resolution and quantification. This book implicitly distinguishes between molecular imaging of emitters and molecular imaging provided by magnetic resonance techniques such as magnetic resonance spectroscopy, magnetic resonance imaging of hyperpolarized and other contrast agents, and other magnetic resonance methods wherein the response to the injected pattern of the radiofrequency field is measured. The mathematics of image reconstruction of an intrinsic emitter that is not stimulated by a known external probing signal are more complicated for light and gamma ray photon emitter reconstruction problems. The exception to this statement is functional molecular tomography (FMT) wherein the behavior of the stimulating photons and that of the excited photons from injected flurophors need to be incorporated in the reconstruction strategy (Chapter 12). Yet, the benefits of emission tomography for molecular imaging of radionuclide emissions relative to other modalities lie in the exquisite sensitivity of radionuclide detection and the broad scales in time, space and object size served by SPECT and PET techniques as well as their role in hypbid imaging systems that employ a combination of emission with CT, MRI and FMT.

In sum, this book shows how researchers have overcome limitations in emission tomography noted 40 years ago and have brought the methods to the goal of high spatial resolution and quantification. Most importantly, these advances have enabled clinically useful applications not available to other diagnostic methods.
},
	Date-Added = {2015-02-06 15:52:47 +0000},
	Date-Modified = {2015-02-06 15:53:56 +0000},
	Editor = {Dawood, M. and Jiang, X. and Sch{\"a}fers, K. P.},
	Publisher = {CRC Press},
	Series = {Series in Medical Physics and Biomedical Engineering},
	Title = {Correction Techniques in Emission Tomography},
	Year = {2008}}

@book{Pi14,
	Abstract = {Welcome to the image-is-everything era! Smartphones, tablets, 3D glasses, and even wrist monitors - what's next? Undoubtedly, the list will become obsolete by the time I finish this sentence, but do not blame it on me. Our lives have become image-driven; we are pixel addicts.

I had my moment a couple of years ago, aboard a little overcrowded boat that was motoring a bunch of tourists like me to the base of Niagara Falls. As we were approaching this most spectacular point, and as water roared and dense plumes of mist billowed higher and higher, I looked around and saw a quite interesting scene: everyone started reaching for their pockets and bags, pulling out the most diverse arsenal of digital filming technology. By the time we reached the pinnacle of our adventure - the beautiful, massive, mesmerizing waterfall - no one was looking at it! Instead, everybody was filming the thing, taking pictures in front of it, and taking pictures in front of the others taking pictures. ''What's wrong with you people,'' I thought to myself. ''You get to experience one of the most fascinating spectacles live, in its full and infinite dimensionality and sound, and you do not care; all you can think about is making cheap snapshots?!'' Would you trade the most stunning reality for a pinch of megapixels? And how many megapixels is your reality worth? 

Then I had another experience, this time while looking for the famous ''Frau Roentgen X-ray'' online. As you may know, Roentgen's wife was the first inspiration for his imagery - her immortal hand with a wedding ring is now known to many. So I was studying the quality of the early X-rays, and after an hour of web-browsing I found the largest ''Frau Roentgen's image'' possible. But imagine my disappointment when I discovered that my digital replica was full of lossy compression artifacts. That is, despite the highest resolution and size, the image was absolutely useless, at least for my purposes. Lossy compression had wiped out all of the most intricate features.

And herein lies our first lesson: even the best images can be far from reality, and even the largest images can be far from the best. This matters a lot when we start talking about the diagnostic image value. And it only makes sense to dedicate the first medical informatics book in this series to the diagnostic quality of digital images - before we get lost in our mega-, giga-, and terapixels.

},
	Author = {Pianykh, O. S.},
	Date-Added = {2015-02-06 15:46:24 +0000},
	Date-Modified = {2015-02-06 15:48:25 +0000},
	Publisher = {Springer Verlag},
	Series = {Understanding Medical Informatics},
	Title = {Digital Image Quality in Medicine},
	Year = {2014}}

@article{KlAnArAsAv09,
	Abstract = {All fields of neuroscience that employ brain imaging need to communicate their results with reference to anatomical regions. In particular, comparative morphometry and group analysis of functional and physiological data require coregistration of brains to establish correspondences across brain structures. It is well established that linear registration of one brain to another is inadequate for aligning brain structures, so numerous algorithms have emerged to nonlinearly register brains to one another. This study is the largest evaluation of nonlinear deformation algorithms applied to brain image registration ever conducted. Fourteen algorithms from labo- ratories around the world are evaluated using 8 different error measures. More than 45,000 registrations between 80 manually labeled brains were performed by algorithms including: AIR, ANIMAL, ART, Diffeomorphic Demons, FNIRT, IRTK, JRD-fluid, ROMEO, SICLE, SyN, and four different SPM5 algorithms (``SPM2-type'' and regular Normalization, Unified Segmentation, and the DARTEL Toolbox). All of these registrations were preceded by linear registration between the same image pairs using FLIRT. One of the most significant findings of this study is that the relative performances of the registration methods under comparison appear to be little affected by the choice of subject population, labeling protocol, and type of overlap measure. This is important because it suggests that the findings are generalizable to new subject populations that are labeled or evaluated using different labeling protocols. Furthermore, we ranked the 14 methods according to three completely independent analyses (permutation tests, one-way ANOVA tests, and indifference-zone ranking) and derived three almost identical top rankings of the methods. ART, SyN, IRTK, and SPM's DARTEL Toolbox gave the best results according to overlap and distance measures, with ART and SyN delivering the most consistently high accuracy across subjects and label sets. Updates will be published on the http://www.mindboggle.info/papers/ website.},
	Author = {Klein, A. and Andersson, J. and Ardekani, B. A. and Ashburner, J. and Avants, B. and Chiang, M.-C. and Christensen, G. E. and Collins, L. and Gee, J. and Hellier, P. and Song, J. H. and Jenkinson, M. and Lepage, C. and Rueckert, D. and Thompson, P. Vercauteren, T. and Woods, R. P. and Mann, J. J. and Ramin, V. P.},
	Date-Added = {2015-02-02 23:17:52 +0000},
	Date-Modified = {2015-02-02 23:21:38 +0000},
	Journal = {NeuroImage},
	Pages = {786---802},
	Title = {Evaluation of 14 nonlinear deformation algorithms applied to human brain {MRI} registration},
	Volume = {46},
	Year = {2009}}

@book{EhLo13,
	Abstract = {Respiratory motion causes an important uncertainty in radiotherapy planning of the thorax and upper abdomen. The main objective of radiation therapy is to eradicate or shrink tumor cells without damaging the surrounding tissue by delivering a high radiation dose to the tumor region and a dose as low as possible to healthy organ tissues. Meeting this demand remains a challenge especially in case of lung tumors due to breathing-induced tumor and organ motion where motion amplitudes can measure up to several centimeters. Therefore, modeling of respiratory motion has become increasingly important in radiation therapy. With 4D imaging techniques spatiotemporal image sequences can be acquired to investigate dynamic processes in the patient's body. Furthermore, image registration enables the estimation of the breathing-induced motion and the description of the temporal change in position and shape of the structures of interest by establishing the correspondence between images acquired at different phases of the breathing cycle. In radiation therapy these motion estimations are used to define accurate treatment margins, e.g. to calculate dose distributions and to develop prediction models for gated or robotic radiotherapy. In this book, the increasing role of image registration and motion estimation algorithms for the interpretation of complex 4D medical image sequences is illustrated. Different 4D CT image acquisition techniques and conceptually different motion estimation algorithms are presented. The clinical relevance is demonstrated by means of example applications which are related to the radiation therapy of thoracic and abdominal tumors. The state of the art and perspectives are shown by an insight into the current field of research. The book is addressed to biomedical engineers, medical physicists, researchers and physicians working in the fields of medical image analysis, radiology and radiation therapy.},
	Date-Added = {2015-02-02 23:13:49 +0000},
	Date-Modified = {2015-02-02 23:15:24 +0000},
	Editor = {Ehrhardt, J. and Lorenz, C.},
	Publisher = {Springer-Verlag},
	Series = {Biological and Medical Physics, Biomedical Engineering},
	Title = {{4D} Modeling and Estimation of Respiratory Motion for Radiation Therapy},
	Year = {2013}}

@article{LuKrVoYuXu09,
	Abstract = {Objectives: To implement the total variation (TV) and Huber prior (HP) regularization methods to the ordered subsets expectation maximization (OSEM) maximum a posteriori (MAP) SPECT reconstruction in the mesh domain.

Methods: We developed TV and HP regularization methods in 2D mesh domain. Pixel-based discrete regularization methods cannot be used in mesh-based reconstruction because of non-uniform sampling in mesh domain. Therefore, we applied Gibbs prior knowledge approach to regularization of OSEM reconstruction in mesh domain. Starting from continuous form of TV regularization and Huber function, we derived discrete iteration formulae for mesh-based OSEM. Noise-free and noisy parallel-beam SPECT synthetic projection sets were created for Zubal thorax and simple geometrical phantoms. Attenuation, scatter, and system special resolution were not modeled. Quadratic, HP, and TV regularization methods in mesh domain were compared. L-curve method was applied for selecting the optimal regularization parameters.

Results: All measured image quality parameters (with exception of bias) are superior for TV regularization, followed by with HP regularization and by quadratic regularization. The optimal values of the TV and HP regularization parameters depend on mesh structure and increase with noise.

Conclusions: The mesh-based OSEM algorithm with TV regularization yields superior image quality, better spatial resolution, lower noise, better uniformity, and improved contrast-to-noise ratio, as compared to mesh-based OSEM algorithm with HP, quadratic, and no regularization.},
	Author = {Lu, Y. and Krol, A. and Vogelsang, L. and Yu, B. and Xu, Y. and Feiglin, D.},
	Date-Added = {2015-02-02 23:04:52 +0000},
	Date-Modified = {2015-02-02 23:06:17 +0000},
	Journal = {Journal of Nuclear Medicine},
	Number = {Supplement 2},
	Pages = {403},
	Title = {Total variation and {H}uber prior regularization for {OSEM SPECT} reconstruction in mesh domain},
	Volume = {50},
	Year = {2009}}

@conference{KaPeBoPaAr10,
	Abstract = {In emission tomography (ET), fast developing Bayesian reconstruction methods can incorporate anatomical information derived from co-registered scanning modalities, such as magnetic resonance (MR) and computed tomography (CT). We propose a Bayesian image reconstruction method for single photon emission computed tomography (SPECT), using a joint entropy (JE) similarity measure to embed MR anatomical data. An optimized non-parametric Parzen window approach is used for fast and efficient estimation of the probability density function (PDF) of the JE metric. It is known that the quality of the Parzen estimates strongly depends on the kernel bandwidth of the smoothing function. When the density is over or under-smoothed, because of too large or small bandwidth value, this leads to an incorrect entropy estimate and, eventually, to a biased solution. To alleviate the problem of searching manually for the most suitable weight for the smoothing function and the number of bins for the histogram, we use an adaptive method to find these parameters automatically from the data on each iteration of the Bayesian algorithm. We assess the NRMSE-variance behaviour of the MAP-EM reconstruction method in relation to the quality of the PDF building. For the different bandwidth values of the Gaussian kernel for the density function, an emission image is reconstructed using MR data as a prior. Preliminary numerical experiments are performed using simulated co-registered 2D and 3D SPECT/MR data. Comparison of proposed technique with neighbourhood dependent anatomically-based prior is presented. Lesions are simulated to be apparent on the gray matter of the 3D SPECT data, but invisible on MRI. Preliminary results demonstrate that applying optimal density estimation for JE metric is feasible and more efficient compared to non-adaptive techniques

},
	Author = {Kazantsev, D. and Pedemonte, S. and Bousse, A. and Panagiotou, C. and Arridge, S. R. and Hutton, B. F. and Ourselin, S.},
	Booktitle = {2010 IEEE Nuclear Science Symposium Conference Record (NSS/MIC)},
	Date-Added = {2015-02-02 23:02:55 +0000},
	Date-Modified = {2015-02-02 23:04:20 +0000},
	Pages = {3301--3307},
	Title = {{ET} {B}ayesian reconstruction using automatic bandwidth selection for joint entropy optimization},
	Year = {2010}}

@conference{ZhZhHuZeBi13,
	Abstract = {Statistical iterative reconstruction (SIR) approaches have shown great potential in x-ray computed tomographic (CT) reconstruction in the case of low-dose protocol. For yielding high quality image, an edge-preserving regularization should be incorporated into the objective function of SIR approaches. A typical example is the Huber regularization with an edge-preserving non-quadratic potential function which increases less rapidly than the quadratic potential function for sufficiently large arguments. However, a major drawback of the Huber regularization is the determining the threshold, which precludes its extensive applications. In this paper, we investigate both global- and local- edge-detecting operators for threshold choices of Huber regularization and apply them to SIR CT image reconstruction with low-dose scan protocol. Experiments were performed on XCAT phantom by using a CT simulator to obtain the low-dose projection data.

},
	Author = {Zhang, H. and Zhang, S. and Hu, D. and Zeng, D. and Bian, Z. and Lu, L. and Ma, J. and Huang, J.},
	Booktitle = {2013 35th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)},
	Date-Added = {2015-02-02 22:52:41 +0000},
	Date-Modified = {2015-02-02 22:54:44 +0000},
	Pages = {2352--2355},
	Title = {Threshold choices of {H}uber regularization using global- and local-edge-detecting operators for X-ray computed tomographic reconstruction},
	Year = {2013}}

@article{VaLaSi09,
	Abstract = {Wavelet-based Besov space prior models for X-ray tomography are studied using the empirical Bayes approach. The hyperparameters for the prior models are estimated from statistical properties of the wavelet coefficients of measured X-ray projection images (which are related to the smoothness of the attenuation coefficient). Various statistical models for the wavelet coefficients are studied. Experiments using measured in vitro data suggest that the hyperparameters can be estimated with a simple method, leading to automated choice of prior parameters and improved tomographic reconstruction.},
	Author = {S. Vanska and M. Lassas and S. Siltanen},
	Date-Added = {2015-02-02 22:51:43 +0000},
	Date-Modified = {2015-02-02 22:51:43 +0000},
	Journal = {International Journal of Tomography and Statistics},
	Number = {3--32},
	Title = {Statistical {X}-ray tomography using empirical {B}esov priors.},
	Volume = {30},
	Year = {2009}}

@article{Bo12,
	Abstract = {We discuss informally two approaches to solving convex and nonconvex feasibility problems -- via entropy optimization and via algebraic iterative methods. We shall highlight the advantages and disadvantages of each and give various related applications and limiting examples. While some of the results are very classical, they are not as well-known to practitioners as they should be. A key role is played by the Fenchel conjugate.

},
	Author = {Borwein, J. M.},
	Date-Added = {2015-02-02 22:43:34 +0000},
	Date-Modified = {2015-02-02 22:44:40 +0000},
	Journal = {Optimization},
	Number = {1},
	Pages = {1--33},
	Title = {Maximum entropy and feasibility methods for convex and non-convex inverse problems},
	Volume = {61},
	Year = {2012}}

@inproceedings{BeBeBiMuNo14,
	Abstract = {Statistical, iterative reconstruction has become a major topic for computed tomography (CT) in the last years as computational efforts have become feasible. In comparison to conventional filtered-backprojection methods the iterative methods highly rely on a consistent model and consistent data. Motion and inconsistencies can cause strong artifacts in the resulting images. In statistical reconstructions side information about the distribution of noise in the measured data is available. A simple modification of this noise modeling is proposed, which enables the down-weighting of systematic deviations in the measured data, making the statistical reconstruction more robust against inconsistencies.
},
	Author = {Bergner, F. and Bernhard Brendel, B. and Bippus, R. and Muenzel, D. and Noel, P. B. and Koehler, T.},
	Booktitle = {The third international conference on image formation in X-ray computed tomography},
	Date-Added = {2015-02-02 22:39:09 +0000},
	Date-Modified = {2015-02-02 22:42:54 +0000},
	Pages = {95--98},
	Title = {Modified Noise Modeling for Robust Statistical Reconstructions},
	Year = {2014}}

@article{Ba00,
	Abstract = {Generalization of the maximum entropy method (MEM) for the reconstruction of sign-altering functions from two-dimensional tomographic measurement data is developed. Three-dimensional algorithms for parallel beam geometry are considered. Results of numerical simulations for composite model are presented.

},
	Author = {Baladin, A. L.},
	Date-Added = {2015-02-02 22:33:48 +0000},
	Date-Modified = {2015-02-02 23:07:04 +0000},
	Journal = {Computers \& Mathematics with Applications},
	Number = {9--10},
	Pages = {15--24},
	Title = {Tomographic analysis of sign-altering functions by maximum entropy method},
	Volume = {39},
	Year = {2000}}

@incollection{Mo96,
	Abstract = {Tomography is the process of estimating an unknown two-dimensional density distribution from a set of its one-dimensional projections (views). The ability of Minerbo's [1] maximum entropy tomography (MENT) algorithm to work with a very small number of views makes it especially useful in industrial radiography, accelerator beam diagnostics, and other potential applications. The MENT algorithm is reviewed using a simple continuum derivation of the factored product form for the two dimensional maximum entropy reconstruction function. The projection coordinates for the various views are related by arbitrary linear transformations. The unknown one dimensional Lagrange factor functions that constitute the maximum entropy interpolant are sampled identically to the digital projection data arrays, thus avoiding the geometric issues of pixel projection and underdeter- mined null-spaces. An iterative procedure is described for solving the large system of coupled non-linear integral equations that result when the product form is substituted into the projection integral constraints. Some of the advantages and costs of using this pure maximum entropy formalism for tomographic reconstruction are illustrated and discussed.
},
	Author = {Mottershead, C. T.},
	Booktitle = {Maximum Entropy and Bayesian Methods. Santa Fe, New Mexico, U.S.A., 1995 Proceedings of the Fifteenth International Workshop on Maximum Entropy and Bayesian Methods},
	Date-Added = {2015-02-02 22:29:33 +0000},
	Date-Modified = {2015-02-02 22:31:02 +0000},
	Editor = {Hanson, K. M. and Silver, R. N.},
	Pages = {425--430},
	Publisher = {Springer-Verlag},
	Series = {Fundamental Theories of Physics},
	Title = {Maximum Entropy Tomography},
	Volume = {79},
	Year = {1996}}

@article{ZhMaWaLiLu14,
	Abstract = {Low-dose computed tomography (CT) imaging without sacrifice of clinical tasks is desirable due to the growing concerns about excessive radiation exposure to the patients. One common strategy to achieve low-dose CT imaging is to lower the milliampere-second (mAs) setting in data scanning protocol. However, the reconstructed CT images by the conventional filtered back-projection (FBP) method from the low-mAs acquisitions may be severely degraded due to the excessive noise. Statistical image reconstruction (SIR) methods have shown potentials to significantly improve the reconstructed image quality from the low-mAs acquisitions, wherein the regularization plays a critical role and an established family of regularizations is based on the Markov random field (MRF) model. Inspired by the success of nonlocal means (NLM) in image processing applications, in this work, we propose to explore the NLM-based regularization for SIR to reconstruct low-dose CT images from low-mAs acquisitions. Experimental results with both digital and physical phantoms consistently demonstrated that SIR with the NLM-based regularization can achieve more gains than SIR with the well-known Gaussian MRF regularization or the generalized Gaussian MRF regularization and the conventional FBP method, in terms of image noise reduction and resolution preservation.
},
	Author = {Zhang, H. and Ma, J. and Wang, J. and Liu, Y. and Lu, H. and Liang, Z.},
	Date-Added = {2015-02-02 22:27:19 +0000},
	Date-Modified = {2015-02-02 22:28:28 +0000},
	Journal = {Computerized Medical Imaging and Graphics},
	Number = {6},
	Pages = {423--435},
	Title = {Statistical image reconstruction for low-dose {CT} using nonlocal means-based regularization},
	Volume = {38},
	Year = {2014}}

@article{WaQi12,
	Abstract = {Iterative image reconstruction for positron emission tomography (PET) can improve image quality by using spatial regularization that penalizes image intensity difference between neighboring pixels. The most commonly used quadratic penalty often over-smoothes edges and fine features in reconstructed images. Non-quadratic penalties can preserve edges but often introduce piece-wise constant blocky artifacts and the results are also sensitive to the hyper-parameter that controls the shape of the penalty function. This paper presents a patch-based regularization for iterative image reconstruction that uses neighborhood patches instead of individual pixels in computing the non-quadratic penalty. The new regularization is more robust than the conventional pixel-based regularization in differentiating sharp edges from random fluctuations due to noise. An optimization transfer algorithm is developed for the penalized maximum likelihood estimation. Each iteration of the algorithm can be implemented in three simple steps: an EM-like image update, an image smoothing and a pixel-by-pixel image fusion. Computer simulations show that the proposed patch-based regularization can achieve higher contrast recovery for small objects without increasing background variation compared with the quadratic regularization. The reconstruction is also more robust to the hyper-parameter than conventional pixel-based non-quadratic regularizations. The proposed regularization method has been applied to real 3D PET data.

},
	Author = {Wang, G. and Qi, J.},
	Date-Added = {2015-02-02 22:22:51 +0000},
	Date-Modified = {2015-02-02 22:23:51 +0000},
	Journal = {IEEE Transactions on Medical Imaging},
	Number = {12},
	Pages = {2194--2204},
	Title = {Penalized likelihood {PET} image reconstruction using patch-based edge-preserving regularizationPenalized Likelihood PET Image Reconstruction using Patch-based Edge-preserving Regularization},
	Volume = {31},
	Year = {2012}}

@article{ZhWeZh14,
	Abstract = {Sparse-projection image reconstruction is a useful approach to lower the radiation dose; however, the incompleteness of projection data will cause degeneration of imaging quality. As a typical compressive sensing method, total variation has obtained great attention on this problem. Suffering from the theoretical imperfection, total variation will produce blocky effect on smooth regions and blur edges. To overcome this problem, in this paper, we introduce the nonlocal total variation into sparse-projection image reconstruction and formulate the minimization problem with new nonlocal total variation norm. The qualitative and quantitative analyses of numerical as well as clinical results demonstrate the validity of the proposed method. Comparing to other existing methods, our method more efficiently suppresses artifacts caused by low-rank reconstruction and reserves structure information better.

},
	Author = {Zhang, Y. and Weihua, Z. and Zhou, J.},
	Date-Added = {2015-02-02 22:18:18 +0000},
	Date-Modified = {2015-02-02 22:20:43 +0000},
	Journal = {The Scientific World Journal},
	Pages = {458496 (7 pp.)},
	Title = {Accurate Sparse-Projection Image Reconstruction via Nonlocal {TV} Regularization},
	Volume = {2014},
	Year = {2014}}

@inproceedings{ZhMaLiHaLi14,
	Abstract = {Statistical iterative reconstruction (SIR) methods have shown remarkable gains over the conventional filtered backprojection (FBP) method in improving image quality for low-dose computed tomography (CT). They reconstruct the CT images by maximizing/minimizing a cost function in a statistical sense, where the cost function usually consists of two terms: the data-fidelity term modeling the statistics of measured data, and the regularization term reflecting a prior information. The regularization term in SIR plays a critical role for successful image reconstruction, and an established family of regularizations is based on the Markov random field (MRF) model. Inspired by the success of nonlocal means (NLM) algorithm in image processing applications, we proposed, in this work, a family of generic and edgepreserving NLM-based regularizations for SIR. We evaluated one of them where the potential function takes the quadratic-form. Experimental results with both digital and physical phantoms clearly demonstrated that SIR with the proposed regularization can achieve more significant gains than SIR with the widely-used Gaussian MRF regularization and the conventional FBP method, in terms of image noise reduction and resolution preservation. },
	Author = {Zhang, H. and Ma, J. and Liu, Y. and Han, H. and Li, L. and Wang, J. and Liang, Z.},
	Booktitle = {Medical Imaging 2014: Physics of Medical Imaging, San Diego, California, USA, February 15, 2014},
	Date-Added = {2015-02-02 22:12:51 +0000},
	Date-Modified = {2015-02-02 22:16:41 +0000},
	Editor = {Whiting, B. R. and Hoeschen, C.},
	Journal = {Zhang, H. and Ma, J. and Liu, Y. and Han, H. and Li, L. and Wang, J. and Liang, Z.},
	Series = {Proceedings of SPIE},
	Title = {Nonlocal means-based regularizations for statistical {CT} reconstruction},
	Volume = {9033},
	Year = {2014}}

@article{PaSoKiPaKi12,
	Abstract = {PURPOSE:
Compressed sensing theory has enabled an accurate, low-dose cone-beam computed tomography (CBCT) reconstruction using a minimal number of noisy projections. However, the reconstruction time remains a significant challenge for practical implementation in the clinic. In this work, we propose a novel gradient projection algorithm, based on the Gradient-Projection-Barzilai-Borwein formulation (GP-BB), that handles the total variation (TV)-norm regularization-based least squares problem for the CBCT reconstruction in a highly efficient manner, with speed acceptable for routine use in the clinic.
METHODS:
CBCT is reconstructed by minimizing an energy function consisting of a data fidelity term and a TV-norm regularization term. Both terms are simultaneously minimized by calculating the gradient projection of the energy function with the step size determined using an approximate Hessian calculation at each iteration, based on the Barzilai-Borwein formulation. To speed up the process, a multiresolution optimization is used. In addition, the entire algorithm was designed to run with a single graphics processing unit (GPU) card. To evaluate the performance, the Shepp-Logan numerical phantom, the CatPhan 600 physical phantom, and a clinically-treated head-and-neck patient were acquired from the TrueBeam{\texttrademark} system (Varian Medical Systems, Palo Alto, CA). For each scan, in total, 364 projections were acquired in a 200$\,^{\circ}$ rotation. The imager has 1024 × 768 pixels with 0.388 × 0.388-mm resolution. This was down-sampled to 512 × 384 pixels with 0.776 × 0.776-mm resolution for reconstruction. Evenly spaced angles were subsampled and used for varying the number of projections for the image reconstruction. To assess the performance of our GP-BB algorithm, we have implemented and compared with three compressed sensing-type algorithms, the two of which are popular and published (forward-backward splitting techniques), and the other one with a basic line-search technique. In addition, the conventional Feldkamp-Davis-Kress (FDK) reconstruction of the clinical patient data is compared as well.
RESULTS:
In comparison with the other compressed sensing-type algorithms, our algorithm showed convergence in ≤30 iterations whereas other published algorithms need at least 50 iterations in order to reconstruct the Shepp-Logan phantom image. With the CatPhan phantom, the GP-BB algorithm achieved a clinically-reasonable image with 40 projections in 12 iterations, in less than 12.6 s. This is at least an order of magnitude faster in reconstruction time compared with the most recent reports utilizing GPU technology given the same input projections. For the head-and-neck clinical scan, clinically-reasonable images were obtained from 120 projections in 34-78 s converging in 12-30 iterations. In this reconstruction range (i.e., 120 projections) the image quality is visually similar to or better than the conventional FDK reconstructed images using 364 projections. This represents a dose reduction of nearly 67% (120∕364 projections) while maintaining a reasonable speed in clinical implementation.
CONCLUSIONS:
In this paper, we proposed a novel, fast, low-dose CBCT reconstruction algorithm using the Barzilai-Borwein step-size calculation. A clinically viable head-and-neck image can be obtained within ∼34-78 s while simultaneously cutting the dose by approximately 67%. This makes our GP-BB algorithm potentially useful in an on-line image-guided radiation therapy (IGRT).},
	Author = {Park, J. C. and Song, B. and Kim, J. S. and Park, S. H. and Kim, H. K. and Liu, Z. and Suh, T. S. and Song, W. Y.},
	Date-Added = {2015-02-02 22:06:56 +0000},
	Date-Modified = {2015-02-02 22:08:43 +0000},
	Journal = {Medical Physics},
	Number = {3},
	Pages = {1207--1217},
	Title = {Fast compressed sensing-based {CBCT} reconstruction using {B}arzilai-{B}orwein formulation for application to on-line {IGRT}},
	Volume = {39},
	Year = {2012}}

@article{ChChHuWa11,
	Abstract = {PURPOSE:
Iterative reconstruction techniques hold great potential to mitigate the effects of data noise and/or incompleteness, and hence can facilitate the patient dose reduction. However, they are not suitable for routine clinical practice due to their long reconstruction times. In this work, the authors accelerated the computations by fully taking advantage of the highly parallel computational power on single and multiple graphics processing units (GPUs). In particular, the forward projection algorithm, which is not included in the close-form formulas, will be accelerated and optimized by using GPU here.

METHODS:
The main contribution is a novel forward projection algorithm that uses multithreads to handle the computations associated with a bunch of adjacent rays simultaneously. The proposed algorithm is free of divergence and bank conflict on GPU, and benefits from data locality and data reuse. It achieves the efficiency particularly by (i) employing a tiled algorithm with three-level parallelization, (ii) optimizing thread block size, (iii) maximizing data reuse on constant memory and shared memory, and (iv) exploiting built-in texture memory interpolation capability to increase efficiency. In addition, to accelerate the iterative algorithms and the Feldkamp-Davis-Kress (FDK) algorithm on GPU, the authors apply batched fast Fourier transform (FFT) to expedite filtering process in FDK and utilize projection bundling parallelism during backprojection to shorten the execution times in FDK and the expectation-maximization (EM).

RESULTS:
Numerical experiments conducted on an NVIDIA Tesla C1060 GPU demonstrated the superiority of the proposed algorithms in computational time saving. The forward projection, filtering, and backprojection times for generating a volume image of 512 x 512 x 512 with 360 projection data of 512 x 512 using one GPU are about 4.13, 0.65, and 2.47 s (including distance weighting), respectively. In particular, the proposed forward projection algorithm is ray-driven and its paralleli-zation strategy evolves from single-thread-for-single-ray (38.56 s), multithreads-for-single-ray (26.05 s), to multithreads-for-multirays (4.13 s). For the voxel-driven backprojection, the use of texture memory reduces the reconstruction time from 4.95 to 3.35 s. By applying the projection bundle technique, the computation time is further reduced to 2.47 s. When employing multiple GPUs, near-perfect speedups were observed as the number of GPUs increases. For example, by using four GPUs, the time for the forward projection, filtering, and backprojection are further reduced to 1.11, 0.18, and 0.66 s. The results obtained by GPU-based algorithms are virtually indistinguishable with those by CPU.

CONCLUSIONS:
The authors have proposed a highly optimized GPU-based forward projection algorithm, as well as the GPU-based FDK and expectation-maximization reconstruction algorithms. Our compute unified device architecture (CUDA) codes provide the exceedingly fast forward projection and backprojection that outperform those using the shading languages, cell broadband engine architecture and previous CUDA implementations. The reconstruction times in the FDK and the EM algorithms were considerably shortened, and thus can facilitate their routine usage in a variety of applications such as image quality improvement and dose reduction.},
	Author = {Chou, C. Y. and Chuo, Y. Y. and Hung, Y. and Wang, W.},
	Date-Added = {2015-02-02 21:59:47 +0000},
	Date-Modified = {2015-02-02 21:59:47 +0000},
	Journal = {Medical Physics},
	Number = {7},
	Pages = {4052--4065},
	Title = {A fast forward projection using multithreads for multirays on {GPU}s in medical image reconstruction},
	Volume = {38},
	Year = {2011}}

@article{RiOrScGr07,
	Abstract = {We introduce a clinical prototype for 3D soft tissue imaging to support surgical or interventional procedures based on a mobile C-arm. An overview of required methods and materials is followed by first clinical images of animals and human patients including dosimetry. The mobility and flexibility of 3D C-arms gives free access to the patient and therefore avoids relocation of the patient between imaging and surgical intervention. Image fusion with diagnostic data (MRI, CT, PET) is demonstrated and promising applications for brachytherapy, RFTT and others are discussed.},
	Author = {Ritter, D. and Orman, J. and Schmidgunst, C. and Graumann, R.},
	Date-Added = {2015-02-02 21:59:33 +0000},
	Date-Modified = {2015-02-02 21:59:33 +0000},
	Journal = {Computerized Medical Imaging and Graphics},
	Pages = {91---102},
	Title = {{3D} soft tissue imaging with a mobile {C}-arm},
	Volume = {31},
	Year = {2007}}

@inproceedings{WaScStOtSu12,
	Abstract = {C-arm cone-beam CT (CBCT) is an emerging tool for intraoperative imaging, but current embodiments exhibit modest soft-tissue imaging capability and are largely constrained to high-contrast imaging tasks. A major advance in image quality is facilitated by statistical iterative reconstruction techniques. This work adapts a general penalized likelihood (PL) reconstruction approach with variable penalties and regularization to C-arm CBCT and investigates performance in imaging of large (>10 mm), low-contrast (<100 HU) tasks pertinent to soft-tissue surgical guidance. Experiments involved a mobile C-arm for CBCT with phantoms and cadavers presenting soft-tissue structures imaged using 3D filtered backprojection (FBP), quadratic, and non-quadratic PL reconstruction. Polyethylene phantoms with various tissue-equivalent inserts were used to quantify contrast-to-noise / resolution tradeoffs in low-contrast (~40 HU) structures, and the optimal reconstruction parameters were translated to imaging an anthropomorphic head phantom with low-contrasts targets and a cadaveric torso. Statistical reconstruction - especially non-quadratic PL variants - boosted soft-tissue image quality through reduction of noise and artifacts (e.g., a ~2-4 fold increase in contrast-to-noise ratio (CNR) at equivalent spatial resolution). For tasks relating to large, low-contrast tissues, even greater gains were possible using non-quadratic penalties and strong regularization that sacrificed spatial resolution in a manner still consistent with the imaging task. The advances in image quality offered by statistical reconstruction present promise and new challenges for interventional imaging, with high-speed computing facilitating realistic application. Careful investigation of performance relative to specific imaging tasks permits knowledgeable application of such techniques in a manner that overcomes conventional tradeoffs in noise, resolution, and dose and could extend application of CBCT-capable C-arms to soft-tissue interventions in neurosurgery as well as thoracic and abdominal interventions.},
	Author = {Wang, A. S. and Schafer, S. and Stayman, W. J. and Otake, Y. and Sussman, M. S. and Khanna, J. A. and Gallia, G. L. and Siewerdsen, J. H.},
	Booktitle = {Medical Imaging 2012: Physics of Medical Imaging, 83133I (February 23, 2012)},
	Date-Added = {2015-02-02 21:59:20 +0000},
	Date-Modified = {2015-02-02 21:59:20 +0000},
	Title = {Soft-tissue imaging in low-dose, C-arm cone-beam CT using statistical image reconstruction},
	Volume = {8668},
	Year = {2012}}

@article{BiWaHaSiSh13,
	Abstract = {Paper
The field of view (FOV) of a cone-beam computed tomography (CBCT) unit in a single-photon emission computed tomography (SPECT)/CBCT system can be increased by offsetting the CBCT detector. Analytic-based algorithms have been developed for image reconstruction from data collected at a large number of densely sampled views in offset-detector CBCT. However, the radiation dose involved in a large number of projections can be of a health concern to the imaged subject. CBCT-imaging dose can be reduced by lowering the number of projections. As analytic-based algorithms are unlikely to reconstruct accurate images from sparse-view data, we investigate and characterize in the work optimization-based algorithms, including an adaptive steepest descent-weighted projection onto convex sets (ASD-WPOCS) algorithms, for image reconstruction from sparse-view data collected in offset-detector CBCT. Using simulated data and real data collected from a physical pelvis phantom and patient, we verify and characterize properties of the algorithms under study. Results of our study suggest that optimization-based algorithms such as ASD-WPOCS may be developed for yielding images of potential utility from a number of projections substantially smaller than those used currently in clinical SPECT/CBCT imaging, thus leading to a dose reduction in CBCT imaging.

General scientific summary 
Cone-beam CT (CBCT) is used in single-photon emission computed tomography (SPECT) for providing information of subject anatomy and for photon-attenuation correction in SPECT. This work concerns the development and characterization of optimization-based algorithms for image reconstruction from sparse-view CBCT data in clinical SPECT/CT. We carefully designed and developed an algorithm for solving the reconstruction problem, followed by quantitative studies for verifying and characterizing reconstruction properties. Results of the studies suggest that the designed reconstruction and developed algorithm may yield CBCT images of potential utility from a number of projections substantially smaller than those used currently in clinical SPECT/CT. Because radiation dose in CBCT can be a health concern to the subject, the proposed work may thus have implications for dose reduction in CBCT through lowering the number of projections. This work also clarifies issues concerning imaging models, data redundancy, and algorithm development involved in CBCT.
},
	Author = {Bian, J. and Wang, J. and Han, X. and Sidky, E. Y. and Shao, L. and Pan, X.},
	Date-Added = {2015-02-02 21:59:09 +0000},
	Date-Modified = {2015-02-02 21:59:09 +0000},
	Journal = {Physics in Medicine and Biology},
	Number = {2},
	Pages = {205--230},
	Title = {Optimization-based image reconstruction from sparse-view data in offset-detector {CBCT}},
	Volume = {58},
	Year = {2013}}

@article{ChWaZhSuBo10,
	Abstract = {PURPOSE: This article considers the problem of reconstructing cone-beam computed tomography (CBCT) images from a set of undersampled and potentially noisy projection measurements.

METHODS: The authors cast the reconstruction as a compressed sensing problem based on l1 norm minimization constrained by statistically weighted least-squares of CBCT projection data. For accurate modeling, the noise characteristics of the CBCT projection data are used to determine the relative importance of each projection measurement. To solve the compressed sensing problem, the authors employ a method minimizing total-variation norm, satisfying a prespecified level of measurement consistency using a first-order method developed by Nesterov.

RESULTS: The method converges fast to the optimal solution without excessive memory requirement, thanks to the method of iterative forward and back-projections. The performance of the proposed algorithm is demonstrated through a series of digital and experimental phantom studies. It is found a that high quality CBCT image can be reconstructed from undersampled and potentially noisy projection data by using the proposed method. Both sparse sampling and decreasing x-ray tube current (i.e., noisy projection data) lead to the reduction of radiation dose in CBCT imaging.

CONCLUSIONS: It is demonstrated that compressed sensing outperforms the traditional algorithm when dealing with sparse, and potentially noisy, CBCT projection views.},
	Author = {Choi, K. and Wang, J. and Zhu, L. and Suh, T. S. and Boyd, S. and Xing, L.},
	Date-Added = {2015-02-02 21:58:47 +0000},
	Date-Modified = {2015-02-02 21:58:47 +0000},
	Journal = {Medical Physics},
	Number = {9},
	Pages = {5113-5125},
	Title = {Compressed sensing based cone-beam computed tomography reconstruction with a first-order method},
	Volume = {37},
	Year = {2010}}

@article{BiSiHaSiPr10,
	Abstract = {Flat-panel-detector x-ray cone-beam computed tomography (CBCT) is used in a rapidly increasing host of imaging applications, including image-guided surgery and radiotherapy. The purpose of the work is to investigate and evaluate image reconstruction from data collected at projection views significantly fewer than what is used in current CBCT imaging. Specifically, we carried out imaging experiments using a bench-top CBCT system that was designed to mimic imaging conditions in image-guided surgery and radiotherapy; we applied an image reconstruction algorithm based on constrained total-variation (TV)-minimization to data acquired with sparsely sampled view-angles and conducted extensive evaluation of algorithm performance. Results of the evaluation studies demonstrate that, depending upon scanning conditions and imaging tasks, algorithms based on constrained TV-minimization can reconstruct images of potential utility from a small fraction of the data used in typical, current CBCT applications. A practical implication of the study is that the optimization of algorithm design and implementation can be exploited for considerably reducing imaging effort and radiation dose in CBCT.},
	Author = {Bian, J. and Siewerdsen, J. H. and Han, X. and Sidky, E. Y. and Prince, J. L. and Pelizzari, C. A. and Pan, X.},
	Date-Added = {2015-02-02 21:58:37 +0000},
	Date-Modified = {2015-02-02 21:58:37 +0000},
	Journal = {Physics in Medicine and Biology},
	Number = {22},
	Pages = {6575-6599},
	Title = {Evaluation of sparse-view reconstruction from flat-panel-detector cone-beam {CT}},
	Volume = {55},
	Year = {2010}}

@article{JiReLiKiWe10,
	Abstract = {PURPOSE: The authors propose a combined scatter reduction and correction method to improve image quality in cone-beam computed tomography (CBCT). Although using a beam-block approach similar to previous techniques to measure the scatter, this method differs in that the authors utilize partially blocked projection data obtained during scatter measurement for CBCT image reconstruction. This study aims to evaluate the feasibility of the proposed approach.

METHODS: A 1D grid, composed of lead septa, was placed between the radiation source and the imaging object for scatter measurement. Image data were collected from the grid interspace regions while the scatter distribution was measured in the blocked regions under the grid. Scatter correction was performed by subtracting the measured scatter from the imaging data. Image information in the penumbral regions of the grid was derived. Three imaging modes were developed to reconstruct full CBCT images from partial projection data. The single-rotation half-fan mode uses interpolation to fill the missing data. The dual-rotation half-fan mode uses two rotations, with the grid offset by half a grid cycle, to acquire two complementary sets of projections, which are then merged to form complete projections for reconstruction. The single-rotation full-fan mode was designed for imaging a small object or a region of interest. Full-fan projection images were acquired over a 360 degrees scan angle with the grid shifting a distance during the scan. An enlarged Catphan phantom was used to evaluate potential improvement in image quality with the proposed technique. An anthropomorphic pelvis phantom was used to validate the feasibility of reconstructing a complete set of CBCT images from the partially blocked projections using three imaging modes. Rigid-body image registration was performed between the CBCT images from the single-rotation half-fan mode and the simulation CT and the results were compared to that for the CBCT images from dual-rotation mode and conventional CBCT images.

RESULTS: The proposed technique reduced the streak artifact index from 58% to 1% in comparison with the conventional CBCT. It also improved CT number linearity from 0.880 to 0.998 and the contrast-to-noise ratio (CNR) from 4.29 to 6.42. Complete sets of CBCT images with overall improved image quality were achieved for all three image modes. The longitudinal resolution was slightly compromised for the single-rotation half-fan mode. High resolution was retained for the dual-rotation half-fan and single-rotation full-fan modes in the longitudinal direction. The registration error for the CBCT images from the single-rotation half-fan mode was 0.8 +/- 0.3 mm in the longitudinal direction and negligible in the other directions.

CONCLUSIONS: The proposed method provides combined scatter correction and direct scatter reduction. Scatter correction may eliminate scatter artifacts, while direct scatter reduction may improve the CNR to compensate the CNR degradation due to scatter correction. Complete sets of CBCT images are reconstructed in all three imaging modes. The single-rotation mode can be used for rigid-body patient alignment despite degradation in longitudinal resolution. The dual-rotation mode may be used to improve CBCT image quality for soft tissue delineation in adaptive radiation therapy.},
	Author = {Jin, J. Y. and Ren, L. and Liu, Q. and Kim, J. and Wen, N. and Guan, H. and Movsas, B. and Chetty, I. J.},
	Date-Added = {2015-02-02 21:58:20 +0000},
	Date-Modified = {2015-02-02 21:58:20 +0000},
	Journal = {Medical Physics},
	Number = {11},
	Pages = {5634--5644},
	Title = {Combining scatter reduction and correction to improve image quality in cone-beam computed tomography {(CBCT)}},
	Volume = {37},
	Year = {2010}}

@article{KiRaFe15,
	Abstract = {Statistical X-ray computed tomography (CT) reconstruction can improve image quality from reduced dose scans, but requires very long computation time. Ordered subsets (OS) methods have been widely used for research in X-ray CT statistical image reconstruction (and are used in clinical PET and SPECT reconstruction). In particular, OS methods based on separable quadratic surrogates (OS-SQS) are massively parallelizable and are well suited to modern computing architectures, but the number of iterations required for convergence should be reduced for better practical use. This paper introduces OS-SQS-momentum algorithms that combine Nesterov's momentum techniques with OS-SQS methods, greatly improving convergence speed in early iterations. If the number of subsets is too large, the OS-SQS-momentum methods can be unstable, so we propose diminishing step sizes that stabilize the method while preserving the very fast convergence behavior. Experiments with simulated and real 3D CT scan data illustrate the performance of the proposed algorithms.
},
	Author = {Kim, D. and Ramani, S. and Fessler, J. A.},
	Date-Added = {2015-02-02 17:23:34 +0100},
	Date-Modified = {2015-02-02 17:24:35 +0100},
	Journal = {IEEE Transactions on Medical Imaging},
	Number = {1},
	Pages = {167--178},
	Title = {Combining ordered subsets and momentum for accelerated X-ray {CT} image reconstruction},
	Volume = {34},
	Year = {2015}}

@article{HiLa13,
	Abstract = {The minimization of a functional composed of a nonsmooth and nonadditive regularization term and a combined $L^1$ and $L^2$ data-fidelity term is proposed. It is shown analytically and numerically that the new model has noticeable advantages over popular models in image processing tasks. For the numerical minimization of the new objective, subspace correction methods are introduced which guarantee the convergence and monotone decay of the associated energy along the iterates. Moreover, an estimate of the distance between the outcome of the subspace correction method and the global minimizer of the nonsmooth objective is derived. This estimate and numerical experiments for image denoising, inpainting, and deblurring indicate that in practice the proposed subspace correction methods indeed approach the global solution of the underlying minimization problem.
},
	Author = {Hinterm{\"u}ller, M. and Langer, A.},
	Date-Added = {2015-02-02 17:18:22 +0100},
	Date-Modified = {2015-02-02 17:19:13 +0100},
	Journal = {SIAM Journal on Imaging Sciences},
	Number = {4},
	Pages = {2134--2173},
	Title = {Subspace Correction Methods for a Class of Nonsmooth and Nonadditive Convex Variational Problems with Mixed $L^1/L^2$ Data-Fidelity in Image Processing},
	Volume = {6},
	Year = {2013}}

@article{HiWu13,
	Abstract = {A nonconvex variational model is introduced which contains the lq-"norm," 0<q<1, of the gradient of the underlying image in the regularization part together with a least squares--type data fidelity term which may depend on a possibly spatially dependent weighting parameter. Hence, the regularization term in this functional is a nonconvex compromise between the minimization of the support of the reconstruction and the classical convex total variation model. In the discrete setting, existence of a minimizer is proved, and a Newton-type solution algorithm is introduced and its global as well as local superlinear convergence toward a stationary point of a locally regularized version of the problem is established. The potential nonpositive definiteness of the Hessian of the objective during the iteration is handled by a trust-region--based regularization scheme. The performance of the new algorithm is studied by means of a series of numerical tests. For the associated infinite dimensional model an existence result based on the weakly lower semicontinuous envelope is established, and its relation to the original problem is discussed.},
	Author = {Hinterm{\"u}ller, M. and Wu, T.},
	Date-Added = {2015-02-02 17:13:46 +0100},
	Date-Modified = {2015-02-02 17:14:37 +0100},
	Journal = {SIAM Journal on Imaging Sciences},
	Number = {3},
	Pages = {1385--1415},
	Title = {Nonconvex TV${}^q$-Models in Image Restoration: Analysis and a Trust-Region Regularization--Based Superlinearly Convergent Solver},
	Volume = {6},
	Year = {2013}}

@incollection{ChChYi11,
	Abstract = {Since their introduction in a classic paper by Rudin, Osher, and Fatemi [5], total variation minimizing models have become one of the most popular and successful methodologies for image restoration. New developments continue to expand the capability of the basic method in various aspects. Many faster numerical algorithms and more sophisticated applications have been proposed. This chapter reviews some of these recent developments.},
	Author = {Chan, R. and Chan, T. and Yip, A.},
	Booktitle = {Handbook of Mathematical Methods in Imaging},
	Chapter = {24},
	Date-Added = {2015-02-02 17:11:16 +0100},
	Date-Modified = {2015-02-02 17:11:16 +0100},
	Editor = {Scherzer, O.},
	Pages = {1059--1094},
	Publisher = {Springer-Verlag},
	Title = {Numerical Methods and Applications in Total Variation Image Restoration},
	Year = {2011}}

@incollection{CaChNo11,
	Abstract = {The use of total variation as a regularization term in imaging problems was motivated by its ability to recover the image discontinuities. This is at the basis of its numerous applications to denoising, optical flow, stereo imaging and 3D surface reconstruction, segmentation, or interpolation to mention some of them. On one hand, we review here the main theoretical arguments that have been given to support this idea. On the other, we review the main numerical approaches to solve different models where total variation appears. We describe both the main iterative schemes and the global optimization methods based on the use of max-flow algorithms. Then, we review the use of anisotropic total variation models to solve different geometric problems and its use in finding a convex formulation of some non-convex total variation problems. Finally, we study the total variation formulation of image restoration.},
	Author = {Caselles, V. and Chambolle, A. and Novaga, M.},
	Booktitle = {Handbook of Mathematical Methods in Imaging},
	Chapter = {23},
	Date-Added = {2015-02-02 17:09:39 +0100},
	Date-Modified = {2015-02-02 17:12:11 +0100},
	Editor = {Scherzer, O.},
	Pages = {1015--1057},
	Publisher = {Springer-Verlag},
	Title = {Total Variation in Imaging},
	Year = {2011}}

@article{CaDoOsSh12,
	Abstract = {The variational techniques (e.g. the total variation based method) are well established and effective for image restoration, as well as many other applications, while the wavelet frame based approach is relatively new and came from a different school. This paper is designed to establish a connection between these two major approaches for image restoration. The main result of this paper shows that when spline wavelet frames of are used, a special model of a wavelet frame method, called the analysis based approach, can be viewed as a discrete approximation at a given resolution to variational methods. A convergence analysis as image resolution increases is given in terms of objective functionals and their approximate minimizers. This analysis goes beyond the establishment of the connections between these two approaches, since it leads to new understandings for both approaches. First, it provides geometric interpretations to the wavelet frame based approach as well as its solutions. On the other hand, for any given variational model, wavelet frame based approaches provide various and flexible discretizations which immediately lead to fast numerical algorithms for both wavelet frame based approaches and the corresponding variational model. Furthermore, the built-in multiresolution structure of wavelet frames can be utilized to adaptively choose proper differential operators in different regions of a given image according to the order of the singularity of the underlying solutions. This is important when multiple orders of differential operators are used in various models that generalize the total variation based method. These observations will enable us to design new methods according to the problems at hand, hence, lead to wider applications of both the variational and wavelet frame based approaches. Links of wavelet frame based approaches to some more general variational methods developed recently will also be discussed.},
	Author = {Cai, J-F. and Dong, B. and Osher, S. and Shen, Z.},
	Date-Added = {2015-02-02 17:05:34 +0100},
	Date-Modified = {2015-02-02 17:07:10 +0100},
	Journal = {Journal of the American Mathematical Society},
	Number = {4},
	Pages = {1033--1089},
	Title = {Image restoration. Total variation, wavelet frames, and beyond},
	Volume = {25},
	Year = {2012}}

@phdthesis{Mu11,
	Abstract = {The instability of ill-posed inverse problems is remedied by applying stabilisation techniques called regularization methods. We consider the inverse problem of denoising/deblurring of a signal (1D) or an image (2D) using total variation regularization. Thus, we deal with the constrained minimisation of the total variation of the image/signal, so that accurate estimations of discontinuous solutions are obtained. The Lagrange multiplier in the constrained minimisation problem corresponds to the regularization parameter in the well-known Tikhonov regularization. One has to be cautious in choosing this regularization parameter. Too large a parameter leads to over-smoothing, while too small a parameter leads to instability. There are at least three popular ways to choose an appropriate regularization parameter; a-priori choice rules - depending on the noise level only, a-posteriori rules - depending on the noise level and noisy data, and heuristic choice rules (noise level free). It is the aim of this thesis to investigate several noise level free rules regarding total variation regularization for image denoising/deblurring.},
	Author = {Mutimbu, L. D.},
	Date-Added = {2015-02-02 17:03:04 +0100},
	Date-Modified = {2015-02-02 17:04:39 +0100},
	School = {Technisch-Naturwissenschaftliche Fakult{\"a}t, Johannes Kepler University of Linz},
	Title = {Heuristic Parameter Choice Rules for Total Variation Regularization},
	Type = {MSc thesis},
	Year = {2011}}

@techreport{ChCaNoCrPo09,
	Abstract = {These are the lecture notes of a course taught in Linz in Sept., 2009, at the school "summer school on sparsity", organized by Massimo Fornasier and Ronny Romlau. They address various theoretical and practical topics related to Total Variation-based image reconstruction. They focu first on some theoretical results on functions which minimize the total variation, and in a second part, describe a few standard and less standard algorithms to minimize the total variation in a finite-differences setting, with a series of applications from simple denoising to stereo, or deconvolution issues, and even more exotic uses like the minimization of minimal partition problems.
},
	Author = {Chambolle, A. and Caselles, V. and Novaga, M. and Cremers, D. and Pock, T.},
	Date-Added = {2015-02-02 17:00:09 +0100},
	Date-Modified = {2015-02-02 17:02:36 +0100},
	Institution = {The open archive HAL},
	Number = {hal-00437581, version 1},
	Title = {An introduction to Total Variation for Image Analysis},
	Year = {2009}}

@article{DoHiNe02,
	Abstract = {Image restoration based on an $\ell^1$-data-fitting term and edge preserving total variation regularization is considered. The associated nonsmooth energy minimization problem is handled by utilizing Fenchel duality and dual regularization techniques. The latter guarantee uniqueness of the dual solution and an efficient way for reconstructing a primal solution, i.e., the restored image, from a dual solution. For solving the resulting primal-dual system, a semismooth Newton solver is proposed and its convergence is studied. The paper ends with a report on restoration results obtained by the new algorithm for salt-and-pepper or random-valued impulse noise including blurring. A comparison with other methods is provided as well.},
	Author = {Dong, Y. and Hinterm{\"u}ller, M. and Neri, M.},
	Date-Added = {2015-02-02 16:46:01 +0100},
	Date-Modified = {2015-02-02 16:47:33 +0100},
	Journal = {SIAM Journal on Imaging Sciences},
	Number = {4},
	Pages = {1168--1189},
	Title = {An Efficient Primal-Dual Method for $L^1${TV} Image Restoration},
	Volume = {2},
	Year = {2009}}

@book{Vo02,
	Abstract = {Inverse problems arise in a number of important practical applications, ranging from biomedical imaging to seismic prospecting. This book provides the reader with a basic understanding of both the underlying mathematics and the computational methods used to solve inverse problems. It also addresses specialized topics like image reconstruction, parameter identification, total variation methods, nonnegativity constraints, and regularization parameter selection methods.

Because inverse problems typically involve the estimation of certain quantities based on indirect measurements, the estimation process is often ill-posed. Regularization methods, which have been developed to deal with this ill-posedness, are carefully explained in the early chapters of Computational Methods for Inverse Problems. The book also integrates mathematical and statistical theory with applications and practical computational methods, including topics like maximum likelihood estimation and Bayesian estimation.

Several web-based resources are available to make this monograph interactive, including a collection of MATLAB m-files used to generate many of the examples and figures. These resources enable readers to conduct their own computational experiments in order to gain insight. They also provide templates for the implementation of regularization methods and numerical solution techniques for other inverse problems. Moreover, they include some realistic test problems to be used to further develop and test various numerical methods.
},
	Address = {Philadelphia},
	Author = {Vogel, C. R.},
	Date-Added = {2015-02-02 16:14:03 +0100},
	Date-Modified = {2015-02-02 16:15:00 +0100},
	Publisher = {SIAM},
	Series = {Frontiers in Applied Mathematics},
	Title = {Computational Methods for Inverse Problems},
	Year = {2002}}

@article{StBoBe11,
	Abstract = {In this paper, we investigate an approximate model for Poisson data reconstruction inspired by a discrepancy principle for the selection of the regularization parameter, recently proposed by Bardsley and Goldes. The model can be obtained by approximating the generalized Kullback--Leibler (KL) divergence in terms of a weighted least-squares function, with weights depending on the object to be reconstructed. We show that it is possible to develop a complete theory, based on this approximation, including results of existence and uniqueness of regularized solutions and simple gradient-based reconstruction algorithms for their computation. Moreover, in this context, the criterion of Bardsley and Goldes is a natural one and it is possible to prove that, in several important cases, it provides a unique value of the regularization parameter. We describe a few numerical tests for comparing the approximate approach with the exact one based on the generalized KL divergence. In the case of a moderate or large number of photons, they provide essentially the same results and therefore the approximate model can be considered as a possible alternative to the exact one.
},
	Author = {Staglian{\`o}, A. and Boccacci, P. and Bertero, M.},
	Date-Added = {2015-02-02 16:12:15 +0100},
	Date-Modified = {2015-02-02 16:12:55 +0100},
	Journal = {Inverse Problems},
	Number = {12},
	Pages = {125003},
	Title = {Analysis of an approximate model for {P}oisson data reconstruction and a related discrepancy principle},
	Volume = {27},
	Year = {2011}}

@article{BaJo09,
	Abstract = {In image processing applications, image intensity is often measured via the counting of incident photons emitted by the object of interest. In such cases, image data noise is accurately modeled by a Poisson distribution. This motivates the use of Poisson maximum likelihood estimation for image reconstruction. However, when the underlying model equation is ill-posed, regularization is needed. Regularized Poisson likelihood estimation has been studied extensively by the authors, though a problem of high importance remains: the choice of the regularization parameter. We will present three statistically motivated methods for choosing the regularization parameter, and numerical examples will be presented to illustrate their effectiveness.
},
	Author = {Bardsley, J. M. and John Goldes, J.},
	Date-Added = {2015-02-02 16:10:29 +0100},
	Date-Modified = {2015-02-02 16:11:10 +0100},
	Journal = {Inverse Problems},
	Number = {9},
	Pages = {095005},
	Title = {Regularization parameter selection methods for ill-posed {P}oisson maximum likelihood estimation},
	Volume = {25},
	Year = {2009}}

@book{WaYaYa11,
	Abstract = {"Optimization and Regularization for Computational Inverse Problems and Applications" focuses on advances in inversion theory and recent developments with practical applications, particularly emphasizing the combination of optimization and regularization for solving inverse problems. This book covers both the methods, including standard regularization theory, Fejer processes for linear and nonlinear problems, the balancing principle, extrapolated regularization, nonstandard regularization, nonlinear gradient method, the nonmonotone gradient method, subspace method and Lie group method; and the practical applications, such as the reconstruction problem for inverse scattering, molecular spectra data processing, quantitative remote sensing inversion, seismic inversion using the Lie group method, and the gravitational lensing problem. Scientists, researchers and engineers, as well as graduate students engaged in applied mathematics, engineering, geophysics, medical science, image processing, remote sensing and atmospheric science will benefit from this book. Dr. Yanfei Wang is a Professor at the Institute of Geology and Geophysics, Chinese Academy of Sciences, China. Dr. Sc. Anatoly G. Yagola is a Professor and Assistant Dean of the Physical Faculty, Lomonosov Moscow State University, Russia. Dr. Changchun Yang is a Professor and Vice Director of the Institute of Geology and Geophysics, Chinese Academy of Sciences, China.},
	Date-Added = {2015-02-02 13:53:55 +0100},
	Date-Modified = {2015-02-02 13:54:44 +0100},
	Editor = {Wang, Y. and Yagola, A. G. and Yang, C.},
	Publisher = {Springer-Verlag},
	Title = {Optimization and Regularization for Computational Inverse Problems and Applications},
	Year = {2011}}

@book{Ki11,
	Abstract = {This book introduces the reader to the area of inverse problems. The study of inverse problems is of vital interest to many areas of science and technology such as geophysical exploration, system identification, nondestructive testing and ultrasonic tomography.

The aim of this book is twofold: in the first part, the reader is exposed to the basic notions and difficulties encountered with ill-posed problems. Basic properties of regularization methods for linear ill-posed problems are studied by means of several simple analytical and numerical examples.

The second part of the book presents three special nonlinear inverse problems in detail - the inverse spectral problem, the inverse problem of electrical impedance tomography (EIT), and the inverse scattering problem.

The corresponding direct problems are studied with respect to existence, uniqueness and continuous dependence on parameters. Then some theoretical results as well as numerical procedures for the inverse problems are discussed.

In this new edition, the Factorization Method is included as one of the prominent members in this monograph. Since the Factorization Method is particularly simple for the problem of EIT and this field has attracted a lot of attention during the past decade a chapter on EIT has been added in this monograph.

The book is highly illustrated and contains many exercises. This together with the choice of material and its presentation in the book are new, thus making it particularly suitable for graduate students in mathematics and engineering.},
	Author = {Kirsch, A.},
	Date-Added = {2015-02-02 13:52:41 +0100},
	Date-Modified = {2015-02-02 13:53:25 +0100},
	Edition = {2nd},
	Publisher = {Springer-Verlag},
	Series = {Applied Mathematical Sciences},
	Title = {An Introduction to the Mathematical Theory of Inverse Problems},
	Volume = {120},
	Year = {2011}}

@book{EnHaNe96,
	Abstract = {In the last two decades, the field of inverse problems has certainly been one of the fastest growing areas in applied mathematics. This growth has largely been driven by the needs of applications both in other sciences and in industry. In Chapter 1, we will give a short overview over some classes of inverse problems of practical interest. Like everything in this book, this overview is far from being complete and quite subjective. As will be shown, inverse problems typically lead to mathematical models that are not well-posed in the sense of Hadamard, i.e., to ill-posed problems. This means especially that their solution is unstable under data perturbations. Numerical meth- ods that can cope with this problem are the so-called regularization methods. This book is devoted to the mathematical theory of regularization methods. For linear problems, this theory can be considered to be relatively complete and will be de- scribed in Chapters 2 - 8. For nonlinear problems, the theory is so far developed to a much lesser extent. We give an account of some of the currently available results, as far as they might be of lasting value, in Chapters 10 and 11. Although the main emphasis of the book is on a functional analytic treatment in the context of operator equations, we include, for linear problems, also some information on numerical aspects in Chapter 9.},
	Author = {Engl, H. W. and Hanke, M. and Neubauer, G.},
	Date-Added = {2015-02-02 13:51:06 +0100},
	Date-Modified = {2015-02-02 13:51:55 +0100},
	Publisher = {Springer-Verlag},
	Series = {Mathematics and Its Applications},
	Title = {Regularization of Inverse Problems},
	Volume = {375},
	Year = {1996}}

@book{Ha10,
	Abstract = {Inverse problems arise when we reconstruct a sharper image from a blurred one or reconstruct the underground mass density from measurements of the gravity above the ground. When we solve an inverse problem, we compute the source that gives rise to some observed data using a mathematical model for the relation between the source and the data.

This book gives an introduction to the practical treatment of inverse problems by means of numerical methods, with a focus on basic mathematical and computational aspects. To solve inverse problems, we demonstrate that insight about them goes hand in hand with algorithms.

Discrete Inverse Problems: Insight and Algorithms includes a number of tutorial exercises that give the reader hands-on experience with the methods, difficulties, and challenges associated with the treatment of inverse problems. It also includes examples and figures that illustrate the theory and algorithms.
},
	Address = {Philadelphia},
	Author = {Hansen, P. C.},
	Date-Added = {2015-02-02 13:49:10 +0100},
	Date-Modified = {2015-02-02 13:50:26 +0100},
	Publisher = {SIAM},
	Series = {Fundamentals of Algorithms},
	Title = {Discrete Inverse Problems: Insight and Algorithms},
	Year = {2010}}

@article{ScMoFi09,
	Abstract = {Due to the long imaging times in SPECT, patient motion is inevitable and constitutes a serious problem for any reconstruction algorithm. The measured inconsistent projection data lead to reconstruction artifacts which can significantly affect the diagnostic accuracy of SPECT if not corrected. To address this problem a new approach for motion correction is introduced. It is purely based on the measured SPECT data and therefore belongs to the data-driven motion correction algorithm class. However, it does overcome some of the shortcomings of conventional methods. This is mainly due to the innovative idea to combine reconstruction and motion correction in one optimization problem. The scheme allows for the correction of abrupt and gradual patient motion. To demonstrate the performance of the proposed scheme extensive 3D tests with numerical phantoms for 3D rigid motion are presented. In addition, a test with real patient data is shown. Each test shows an impressive improvement of the quality of the reconstructed image. In this note, only rigid movements are considered. The extension to non-linear motion, as for example breathing or cardiac motion, is straightforward and will be investigated in a forthcoming paper.
},
	Author = {Schumacher, H. and Modersitzki, J. and Fischer, B.},
	Date-Added = {2015-02-02 10:40:51 +0100},
	Date-Modified = {2015-02-02 10:42:05 +0100},
	Journal = {IEEE Transactions in Nuclear Science},
	Pages = {73--80},
	Title = {Combined reconstruction and motion correction in {SPECT} imaging},
	Volume = {56},
	Year = {2009}}

@phdthesis{Br10,
	Abstract = {This thesis contributes to the field of mathematical image processing and inverse problems. An inverse problem is a task, where the values of some model parameters must be computed from observed data. Such problems arise in a wide variety of applications in sciences and engineering, such as medical imaging, biophysics or astronomy. We mainly consider reconstruction problems with Poisson noise in tomography and optical nanoscopy. In the latter case, the task is to reconstruct images from blurred and noisy measurements, whereas in positron emission tomography the task is to visualize physiological processes of a patient. In 3D static image reconstruction standard methods do not incorporate time-dependent information or dynamics, e.g. heart beat or breathing in tomography or cell motion in microscopy. This thesis is a treatise on models, analysis and efficient algorithms to solve 3D and 4D time-dependent inverse problems.},
	Author = {Brune, C.},
	Date-Added = {2015-02-02 10:39:54 +0100},
	Date-Modified = {2015-02-02 10:40:29 +0100},
	School = {University of M{\"u}nster},
	Title = {{4D} Imaging in Tomography and Optical Nanoscopy},
	Year = {2010}}

@article{FiBaDaMe09,
	Abstract = {Respiratory motion correction in positron emission tomography (PET) seeks to incorporate motion information into an image reconstruction algorithm by using the full counting statistics of an acquisition to generate a single, motion-free volume. Here, we present a motion-incorporated ordered subsets expectation maximization (MOSEM) reconstruction based on a device-dedicated tomographic projector in which each matrix element is calculated directly from the voxels' Cartesian coordinates alone. The motion is corrected by updating this projector as a function of the respiratory level. The performance of the reconstruction method was investigated with three datasets: two simulations of a transaxially or axially moving lesion on a patient acquisition and a third acquisition of a moving sphere. After the 16th sub-iteration, the normalized mean square error (NMSE, with a motionless acquisition as reference) was 0.20 for the non-corrected (ungated) image and 0.01 for the MOSEM image with transaxial motion simulation. Likewise, NMSE was 0.30 for the ungated image and 0.03 for MOSEM image with axial motion simulation. For the phantom, ungated reconstruction yielded an error of 0.78, whereas MOSEM yielded 0.43. The error reduction resulted from enhancement and reduced spreading of the moving uptake. Our results show that MOSEM reconstruction yields motion-corrected images which are similar to motionless reference images.},
	Author = {Fin, L. and Bailly, P. and Daouk, J. and Meyer, M.-E.},
	Date-Added = {2015-02-02 09:32:07 +0100},
	Date-Modified = {2015-02-02 09:46:14 +0100},
	Journal = {Computer Methods and Programs in Biomedicine},
	Number = {3},
	Pages = {e1---e9},
	Title = {Motion correction based on an appropriate system matrix for statistical reconstruction of respi- ratory-correlated PET acquisitions},
	Volume = {96},
	Year = {2009}}

@article{KlHu06,
	Abstract = {A four-dimensional deformable motion algorithm is described for use in the motion compensation of gated cardiac positron emission tomography. The algorithm makes use of temporal continuity and a non-uniform elastic material model to provide improved estimates of heart motion between time frames. Temporal continuity is utilized in two ways. First, incremental motion fields between adjacent time frames are calculated to improve estimation of long-range motion between distant time frames. Second, a consistency criterion is used to insure that the image match between distant time frames is consistent with the deformations used to match adjacent time frames. The consistency requirement augments the algorithm's ability to estimate motion between noisy time frames, and the concatenated incremental motion fields improve estimation for large deformations. The estimated motion fields are used to establish a voxel correspondence between volumes and to produce a motion-compensated composite volume.},
	Author = {Klein, G. J. and Huesman, R. H.},
	Date-Added = {2015-02-02 09:20:28 +0100},
	Date-Modified = {2015-02-02 09:21:36 +0100},
	Journal = {Medical Image Analysis},
	Number = {2002},
	Pages = {29--46},
	Title = {Four dimensional processing of deformable cardiac {PET} data},
	Volume = {1},
	Year = {6}}

@article{BaBr11,
	Abstract = {Positron emission tomography (PET) is a molecular imaging technique which provides important functional information about the human body. However, thoracic PET images are often substantially degraded by respiratory motion, which adversely impacts on subsequent diagnosis. In this paper, a motion correction and attenuation correction method is proposed to correct for motion in respiratory gated PET images and to yield an accurate distribution of the radioactivity concentration. Experimental results show that this method can effectively correct for motion and improve PET image quality. The method is able to provide improved diagnostic information without increasing the acquisition time or the radiation burden.},
	Author = {Bai, W. and Brady, M.},
	Date-Added = {2015-02-02 09:08:21 +0100},
	Date-Modified = {2015-02-02 09:09:57 +0100},
	Journal = {IEEE Transactions on Medical Imaging},
	Number = {2},
	Pages = {351---365},
	Title = {Motion correction and attenuation correction for respiratory gated {PET} images},
	Volume = {30},
	Year = {2011}}

@article{WaViBe99,
	Abstract = {PURPOSE:
To measure the duration of the rest period in the cardiac cycle, a parameter vital to data acquisition in coronary magnetic resonance (MR) angiography.

MATERIALS AND METHODS:
Motion of coronary arteries was measured in 13 patients by using breath-hold, biplane, conventional angiography, with frontal and lateral projections of the left and right coronary arteries acquired at 30 frames per second. The time courses of the coordinates of bifurcations of proximal parts of the coronary arteries were measured, from which the rest period (motion < 1 mm in orthogonal axes), velocity, displacement range, motion correlation, and reproducibility from heartbeat to heartbeat were estimated.

RESULTS:
Both the motion pattern and the amplitude varied substantially from patient to patient. The rest period varied from 66 to 333 msec (mean, 161 msec) for the left coronary artery and from 66 to 200 msec (mean, 120 msec) for the right coronary artery.

CONCLUSION:
The rest period for coronary arteries in the cardiac cycle varies substantially from patient to patient, which may cause quality to be inconsistent in current coronary MR angiography. A cardiac motion image prior to coronary data acquisition (preimage) may be used to estimate the optimal duration and timing in the cardiac cycle for coronary MR angiography.},
	Author = {Wang, Y. and Vidan, E. and Bergman, G.},
	Date-Added = {2015-02-02 08:56:04 +0100},
	Date-Modified = {2015-02-02 08:57:03 +0100},
	Journal = {Radiology},
	Number = {3},
	Pages = {751---758},
	Title = {Cardiac motion of coronary arteries: {V}ariability in the rest period and implications for coronary {MR} angiography},
	Volume = {213},
	Year = {1999}}

@article{ScLe00,
	Abstract = {Magnetic resonance spectroscopic imaging (MRSI) studies in the abdomen or breast are acquired in the presence of respiratory motion. This modifies the point spread function (PSF) and hence the reconstructed spectra. We evaluated the quantitative effects of both periodic and aperiodic motion on spectra localized by MRSI. Artefactual signal changes, both the modification of native to a voxel and spurious signals arising elsewhere, depend primarily upon the motion amplitude relative to the voxel dimension. A similar dependence on motion amplitude was observed for simple harmonic motion (SHM), quasi-periodic motion and random displacements. No systematic dependence upon the period or initial phase of SHM or on the array size was found. There was also no significant variation with motion direction relative to the internal and external phase-encoding directions. In measured excursion ranges of 20 breast and abdominal tumours, 70% moved < or = 5 mm, while 30% moved 6-23 mm. The diaphragm and fatty tissues in the gut typically moved approximately 15-20 mm. While tumour/organ excursions less than half the voxel dimension do not substantially affect native signals, the bleeding in of strong lipid signals will be problematic in 1H studies. MRSI studies in the abdomen, even of relatively well-anchored tumours, are thus likely to benefit from the addition of respiratory triggering or other motion compensation strategies.},
	Author = {Schwarz, A. and Leach, M.},
	Date-Added = {2015-02-02 08:54:41 +0100},
	Date-Modified = {2015-02-02 08:55:42 +0100},
	Journal = {Physics in Medicine and Biology},
	Number = {8},
	Pages = {2105---2116},
	Title = {Implications of respiratory motion for the quantification of {2D MR} spectroscopic imaging data in the abdomen},
	Volume = {45},
	Year = {2000}}

@book{Mo09,
	Abstract = {Whenever images taken at different times, from different viewpoints, and/or by different sensors need to be compared, merged, or integrated, image registration is required. Registration, also known as alignment, fusion, or warping, is the process of transforming data into a common reference frame.

This book provides:
- an overview of state-of-the-art registration techniques from theory to practice,
- numerous exercises designed to enhance readers' understanding of the principles and mechanisms of the described techniques, and
- via a supplementary Web page, free access to FAIR.m, a package that is based on the MATLAB{\textregistered} software environment, which enables readers to experiment with the proposed algorithms and explore the presented examples in more depth.
},
	Address = {Philadelphia},
	Author = {Modersitzki, J.},
	Date-Added = {2015-02-02 07:53:43 +0100},
	Date-Modified = {2015-02-02 07:56:17 +0100},
	Publisher = {SIAM},
	Series = {Fundamentals of Algorithms},
	Title = {{FAIR}: {F}lexible Algorithms for Image Registration},
	Year = {2009}}

@article{BuMoRu13,
	Abstract = {Image registration is one of the most challenging problems in image processing, where ill-posedness arises due to noisy data as well as nonuniqueness, and hence the choice of regularization is crucial. This paper presents hyperelasticity as a regularizer and introduces a new and stable numerical implementation. On one hand, hyperelastic registration is an appropriate model for large and highly nonlinear deformations, for which a linear elastic model needs to fail. On the other hand, the hyperelastic regularizer yields very regular and diffeomorphic transformations. While hyperelasticity might be considered as just an additional outstanding regularization option for some applications, it becomes inevitable for applications involving higher order distance measures like mass-preserving registration. The paper gives a short introduction to image registration and hyperelasticity. The hyperelastic image registration problem is phrased in a variational setting, and an existence proof is provided. The focus of the paper, however, is on a robust numerical scheme. A key challenge is an unbiased discretization of hyperelasticity, which enables the numerical monitoring of variations of length, surface, and volume of infinitesimal reference elements. We resolve this issue by using a nodal-based discretization with a special tetrahedral partitioning. The potential of the hyperelastic registration is demonstrated in a direct comparison with a linear elastic registration in an academic example. The paper also presents a real life application from three-dimensional positron emission tomography of the human heart which requires mass preservation, and thus hyperelastic registration is the only option.},
	Author = {Burger, M. and Modersitzki, J. and Ruthotto, L.},
	Date-Added = {2015-02-02 07:48:12 +0100},
	Date-Modified = {2015-02-02 07:53:13 +0100},
	Journal = {SIAM Journal on Scientific Computing},
	Number = {1},
	Pages = {B132--B148},
	Title = {A hyperelastic regularization energy for image registration},
	Volume = {35},
	Year = {2013}}

@article{GiRuBuWoJiSc12,
	Abstract = {Respiratory and cardiac motion leads to image degradation in positron emission tomography (PET) studies of the human heart. In this paper we present a novel approach to motion correction based on dual gating and mass-preserving hyperelastic image registration. Thereby, we account for intensity modulations caused by the highly nonrigid cardiac motion. This leads to accurate and realistic motion estimates which are quantitatively validated on software phantom data and carried over to clinically relevant data using a hardware phantom. For patient data, the proposed method is first evaluated in a high statistic (20 min scans) dual gating study of 21 patients. It is shown that the proposed approach properly corrects PET images for dual-cardiac as well as respiratory-motion. In a second study the list mode data of the same patients is cropped to a scan time reasonable for clinical practice (3 min). This low statistic study not only shows the clinical applicability of our method but also demonstrates its robustness against noise obtained by hyperelastic regularization.},
	Author = {Gigengack, F. and Ruthotto, L. and Burger, M. and Wolters, C. H. and Jiang, X. and Sch{\"a}fers, K. P.},
	Date-Added = {2015-02-02 07:43:31 +0100},
	Date-Modified = {2015-02-02 07:45:04 +0100},
	Journal = {IEEE Transactions on Medical Imaging},
	Number = {3},
	Pages = {698--712},
	Title = {Motion Correction in Dual Gated Cardiac {PET} using Mass-Preserving Image Registration},
	Volume = {31},
	Year = {2012}}

@article{Ne13,
	Abstract = {PET-CT scanners allow robust and synergistic fusion of anatomic and functional information, which has improved sensitivity, specificity, and enhancement in the value of PET and CT when assessing tumor response to therapy. Breathing motion and the difference in time resolutions commonly cause motion artifacts and spatial mismatch between the corresponding image sets. Correction for the breathing-induced artifacts represents a particular challenge. This article summarizes the materials, methods, and results involved in multiple investigations of the correction for respiratory motion in PET-CT imaging of the thorax. Some methods use respiratory-phase data selection, whereas others have adopted sophisticated software techniques.

},
	Author = {Nehmeh, S. A.},
	Date-Added = {2015-02-01 07:29:17 +0000},
	Date-Modified = {2015-02-01 07:29:54 +0000},
	Journal = {PET Clinics},
	Number = {1},
	Pages = {29--36},
	Title = {Respiratory Motion Correction Strategies in Thoracic {PET-CT} Imaging},
	Volume = {8},
	Year = {2013}}

@article{BeDePrGi13,
	Abstract = {Respiratory and cardiac motions represent important sources of image degradation in both PET and computed tomography (CT) studies that need to be taken into account and compensated to improve image quality and quantitative accuracy. This review describes the hardware needed to perform respiratory and cardiac gating with PET and PET/CT systems. In particular, most of the proposed motion-tracking devices for the management of respiratory, cardiac, and multidimensional movements are described and compared. Some advanced applications in PET and PET/CT made possible by the gating technology are considered and analyzed.

},
	Author = {Bettinardi, V. and De Bernardi, E. and Presotto, L. and Gilardi, M. C.},
	Date-Added = {2015-02-01 07:26:09 +0000},
	Date-Modified = {2015-02-01 07:27:12 +0000},
	Journal = {PET Clinics},
	Number = {1},
	Pages = {11--28},
	Title = {Motion-Tracking Hardware and Advanced Applications in {PET} and {PET/CT}},
	Volume = {8},
	Year = {2013}}

@conference{TaHaRa12,
	Abstract = {Involuntary organ movement causes degradation in myocardial perfusion (MP) positron emission tomography (PET) imaging. Respiratory and/or cardiac gating reduces motion while lowering statistics in the reconstructed image frames. The advent of integrated PET/magnetic resonance imaging (MRI) provides opportunities for motion-compensated PET imaging using MRI measured motion. The purpose of this study is to correct respiratory and cardiac motion in dual-gated MP PET imaging, with non-rigid cardiac motion estimated from corresponding cardiac-gated MR images. Using the XCAT phantom, we simulated five-dimensional (5D) dual-gated PET data with and without MP defects and the corresponding 4D cardiac-gated MR images. For each cardiac gate, we performed integrated 4D respiratory motion-corrected image reconstruction to the respiratory-gated data, using the end-expiratory frame as the reference. Then we estimated cardiac motion from the gated MR images using an optical-flow determination algorithm. Using the cardiac motion fields, we warped and summed the respiratory motion-corrected cardiac-gated PET images with the end-diastolic frame as the reference. To evaluate the proposed technique, we performed receiver operating characteristic (ROC) analysis for MP defect detection using a channelized Hotelling observer. The ROC analysis resulted in an area under the curve (AUC) value of .96 $\pm$ .02 from images obtained using the proposed respiratory and cardiac motion compensation technique and an AUC value of .85 $\pm$ .04 from images reconstructed without motion correction. The proposed MRI assisted motion-corrected image reconstruction technique for dual-gated PET imaging is demonstrated to significantly improve the MP defect detection, which is promising for applications especially in emerging integrated PET/MRI.

},
	Author = {Tang, J. and Hall, N. and Rahmim, A.},
	Booktitle = {2012 IEEE Nuclear Science Symposium and Medical Imaging Conference (NSS/MIC)},
	Date-Added = {2015-01-31 22:00:55 +0000},
	Date-Modified = {2015-01-31 22:02:36 +0000},
	Pages = {4054--4057},
	Title = {{MRI} assisted motion correction in dual-gated {5D} myocardial perfusion {PET} imaging},
	Year = {2012}}

@article{GiMaBoal02,
	Abstract = {The primary goal of this work has been to develop a processing method for gated cardiac emission computed tomography (ECT) that simultaneously reconstructs the pixel intensities of the gated images and estimates the motion of the cardiac wall. The simultaneous reconstruction and motion estimation is achieved using conjugate gradient optimization with an objective function that is dependent on the gated reconstructed images at two time frames and the estimated motion of the object between the two frames. The method was evaluated on simulated phantom data both with and without Poisson noise. With noise-free data, the accuracy of the motion estimate and the quality of the reconstructed images were found to be dependent on the hyperparameter selection. With noisy data, the simultaneous method produced reconstructed images with smaller squared error compared with images reconstructed without motion estimation. In a patient gated myocardial perfusion study, the estimated motion between two frames agreed with subjective assessment of wall motion.

},
	Author = {Gilland, D. R. and Mair, B. A. and Bowsher, J. E. and Jaszczak, R. J.},
	Date-Added = {2015-01-31 21:54:02 +0000},
	Date-Modified = {2015-01-31 21:55:33 +0000},
	Journal = {IEEE Transactions in Nuclear Science},
	Number = {5},
	Pages = {2344---2349},
	Title = {Simultaneous reconstruction and motion estimation for gated cardiac {ECT}},
	Volume = {49},
	Year = {2002}}

@article{BlMaKeNaRa10,
	Abstract = {We present a novel intrinsic method for joint reconstruction of both image and motion in positron emission tomography (PET). Intrinsic motion compensation methods exclusively work on the measured data, without any external motion measurements. Most of these methods separate image from motion estimation: They use deformable image registration/optical flow techniques in order to estimate the motion from individually reconstructed gates. Then, the image is estimated based on this motion information. With these methods, a main problem lies in the motion estimation step, which is based on the noisy gated frames. The more noise is present, the more inaccurate the image registration becomes. As we show both visually and quantitatively, joint reconstruction using a simple deformation field motion model can compete with state-of-the-art image registration methods which use robust multilevel B-spline motion models.

},
	Author = {Blume, M. and Martinez-Moller, A. and Keil, A. and Navab, N. and Rafecas, M.},
	Date-Added = {2015-01-31 21:47:51 +0000},
	Date-Modified = {2015-01-31 21:49:08 +0000},
	Journal = {IEEE Transactions on Medical Imaging},
	Number = {11},
	Pages = {1892---1906},
	Title = {Joint reconstruction of image and motion in gated positron emission tomography},
	Volume = {29},
	Year = {2010}}

@conference{JaFe06,
	Abstract = {In previous work, we proposed a Poisson statistical model for gated PET data in which the distribution was parametrized in terms of both image intensity and motion parameters. The motion parameters related the activity image in each gate to that of a base image in some fixed gate. By doing maximum loglikelihood (ML) estimation of all parameters simultaneously, one obtains an estimate of the base gate image that exploits the full set of measured sinogram data. Previously, this joint ML approach was compared, in a highly simplified single-slice setting, to more conventional methods. Performance was measured in terms of the recovery of tracer uptake in a synthetic lung nodule. This paper reports the extension to 3D with much more realistic simulated motion. Furthermore, in addition to pure ML estimation, we consider the use of side information from a breath-hold CT scan to facilitate regularization, while preserving hot lesions of the kind seen in FDG oncology studies

},
	Author = {Jacobson, M. W. and Fessler, J. A.},
	Booktitle = {3rd IEEE International Symposium on Biomedical Imaging: Nano to Macro. Arlington, VA, April 6--9, 2006},
	Date-Added = {2015-01-31 21:46:04 +0000},
	Date-Modified = {2015-01-31 21:47:10 +0000},
	Pages = {275---278},
	Title = {Joint estimation of respiratory motion and activity in {4D} {PET} using {CT} side information},
	Year = {2006}}

@conference{JaFe03,
	Abstract = {We consider an emission tomography reconstruction problem in which projection measurements from several time frames are available. Two strategies for doing motion-corrected image reconstruction are compared. In the first strategy, separate images are reconstructed from the measurements at each time frame. They are then consolidated by post-registration and averaging procedures. In the second strategy, we incorporate parameters to describe the effects of motion into the statistical model of the projections. Joint maximum likelihood estimation of image and motion parameters is then carried out. Each of these strategies involves the minimization of non-convex cost functions. Accordingly, we also propose some relevant optimization algorithm design options.

},
	Author = {Jacobson, M. W. and Fessler, J. A.},
	Booktitle = {2003 IEEE Nuclear Science Symposium Conference Record},
	Date-Added = {2015-01-31 21:43:12 +0000},
	Date-Modified = {2015-01-31 21:44:44 +0000},
	Pages = {3290---3294},
	Title = {Joint estimation of image and deformation parameters in motion-corrected {PET}},
	Year = {2003}}

@inproceedings{GiMaSu05,
	Author = {Gilland, D. R. and Mair, B. A. and Sun, J.},
	Booktitle = {Proceedings of the International Meeting on Fully Three-Dimensional Image Reconstruction in Radi- ology and Nuclear Medicine. Salt Lake City (UT), July 6--9, 2005},
	Date-Added = {2015-01-31 21:41:29 +0000},
	Date-Modified = {2015-01-31 21:42:28 +0000},
	Pages = {303---306},
	Title = {Joint {4D} reconstruction and motion estimation in gated cardiac {ECT}},
	Year = {2005}}

@article{Qi06,
	Abstract = {List-mode image reconstruction is attracting renewed interest in emission tomography as the number of possible lines of response (LORs) in modern scanners becomes more than the number of detected events in one data set. One advantage of list-mode processing is that it eliminates the storage of empty sinogram bins. However, a single back projection of all LORs is still necessary for calculating the sensitivity image. Since the detection sensitivity is dependent on the object attenuation and detector efficiency, the sensitivity image must be computed for each study. Exact computation of the sensitivity image can be a daunting task for modern scanners with a huge number of LORs. Thus, some fast approximate calculation is desirable. Here we use theoretical results that we have derived to analyze a Monte Carlo based backprojection approach for calculating the sensitivity image. We derive expressions for the total number of LORs required in the backprojection and the sampling distribution. Computer simulations show that the nonuniform sampling schemes can reduce reconstruction artifact compared to the uniform sampling approach with the same computation cost

},
	Author = {Qi, J.},
	Date-Added = {2015-01-31 21:40:04 +0000},
	Date-Modified = {2015-01-31 21:41:01 +0000},
	Journal = {IEEE Transactions in Nuclear Science},
	Number = {5},
	Pages = {2746---2751},
	Title = {Calculation of the sensitivity image in list-mode reconstruction for {PET}},
	Volume = {53},
	Year = {2006}}

@conference{CaBaLiJo03,
	Abstract = {The HRRT PET system has the potential to produce human brain images with resolution better than 3 mm. To achieve the best possible accuracy and precision, we have designed MOLAR, a motion-compensation OSEM list-mode algorithm for resolution-recovery reconstruction on a computer cluster with the following features: direct use of list mode data with dynamic motion information (Polaris); exact reprojection of each line-of- response (LOR); system matrix computed from voxel-to-LOR distances (radial and axial); spatially varying resolution model implemented for each event by selection from precomputed line spread functions based on factors including detector obliqueness, crystal layer, and block detector position; distribution of events to processors and to subsets based on order of arrival; removal of voxels and events outside a reduced field-of-view defined by the attenuation map; no pre-corrections to Poisson data, i.e., all physical effects are defined in the model; randoms estimation from singles; model-based scatter simulation incorporated into the iterations; and component-based normalization. Preliminary computation estimates suggest that reconstruction of a single frame in one hour is achievable. Careful evaluation of this system will define which factors play an important role in producing high resolution, low-noise images with quantitative accuracy.

},
	Author = {Carson, R. and Barker, W. C. and Liow, J. S. and Johnson, C. A.},
	Booktitle = {IEEE Nuclear Science Symposium Conference Record, 2003. Portland (OR). October 19--25, 2003},
	Date-Added = {2015-01-31 21:37:16 +0000},
	Date-Modified = {2015-01-31 21:38:45 +0000},
	Pages = {3281---3285},
	Title = {Design of a motion-compensation {OSEM} list-mode algorithm for resolution-recovery reconstruction for the {HRRT}},
	Year = {2003}}

@article{RaBlHoLeMi04,
	Abstract = {With continuous improvements in spatial resolution of positron emission tomography (PET) scanners, small patient movements during PET imaging become a significant source of resolution degradation. This work explores incorporation of motion information into expectation-maximization (EM) reconstruction algorithms. An important issue addressed is the existence of lines-of-response (LORs) corresponding to no actual pairs of detectors and their motion-induced "interaction" with the detectable LORs. An example of this is a scanner design with gaps existing in between the detector heads. It is shown that to properly account for such LORs in histogram-mode and list-mode EM reconstructions, in addition to motion correction of the events, the algorithms themselves must be modified. This modification is implemented by including motion-compensated sensitivity correction factors. We are able to demonstrate experimentally that the proposed approach resolves image artifacts that can appear when the conventional purely event-driven motion correction technique is used. An alternate image-space-based method for calculation of motion-compensated sensitivity factors is also derived, applicable in both histogram-mode and list-mode reconstruction tasks, which has the potential of being considerably faster in presence of frequent motion, especially in high-resolution tomographs.

},
	Author = {Rahmim, A. and Bloomfield, P. M. and Houle, S. and Lenox, M. and Michel, C. and Buckley, K. R. and Ruth, T. J. and Sossi, V.},
	Date-Added = {2015-01-31 21:34:54 +0000},
	Date-Modified = {2015-01-31 21:36:31 +0000},
	Journal = {IEEE Transactions in Nuclear Science},
	Pages = {2588---2596},
	Title = {Motion compensation in histogram-mode and list-mode {EM} reconstructions: beyond the event-driven approach},
	Volume = {51},
	Year = {2004}}

@article{RaDiChShSe08,
	Abstract = {With continuing improvements in spatial resolution of positron emission tomography (PET) scanners, small patient movements during PET imaging become a significant source of resolution degradation. This work develops and investigates a comprehensive formalism for accurate motion-compensated reconstruction which at the same time is very feasible in the context of high-resolution PET. In particular, this paper proposes an effective method to incorporate presence of scattered and random coincidences in the context of motion (which is similarly applicable to various other motion correction schemes). The overall reconstruction framework takes into consideration missing projection data which are not detected due to motion, and additionally, incorporates information from all detected events, including those which fall outside the field-of-view following motion correction. The proposed approach has been extensively validated using phantom experiments as well as realistic simulations of a new mathematical brain phantom developed in this work, and the results for a dynamic patient study are also presented.
},
	Author = {Rahmim, A. and Dinelle, K. and Cheng, J. C. and Shilov, M. A. and Segars, W. P. and Lidstone, S. C. and Blinder, S. and Rousset, O. G. and Vajihollahi. H. and Tsui, B. M. W: and Wong, D. F. and Sossi, V.},
	Date-Added = {2015-01-31 21:31:57 +0000},
	Date-Modified = {2015-01-31 21:34:37 +0000},
	Journal = {IEEE Transactions on Medical Imaging},
	Number = {8},
	Pages = {1018---1033},
	Title = {Accurate event-driven motion compensation in high-resolution {PET} incorporating scattered and random events},
	Volume = {27},
	Year = {2008}}

@article{BlSpReScWe03,
	Abstract = {A method is described to monitor the motion of the head during neurological positron emission tomography (PET) acquisitions and to correct the data post acquisition for the recorded motion prior to image reconstruction. The technique uses an optical tracking system, Polaris, to accurately monitor the position of the head during the PET acquisition. The PET data are acquired in list mode where the events are written directly to disk during acquisition. The motion tracking information is aligned to the PET data using a sequence of pseudo-random numbers, which are inserted into the time tags in the list mode event stream through the gating input interface on the tomograph. The position of the head is monitored during the transmission acquisition, and it is assumed that there is minimal head motion during this measurement. Each event, prompt and delayed, in the list mode event stream is corrected for motion and transformed into the transmission space. For a given line of response, normalization, including corrections for detector efficiency, geometry and crystal interference and dead time are applied prior to motion correction and rebinning in the sinogram. A series of phantom experiments were performed to confirm the accuracy of the method: (a) a point source located in three discrete axial positions in the tomograph field of view, 0 mm, 10 mm and 20 mm from a reference point, (b) a multi-line source phantom rotated in both discrete and gradual rotations through +/- 5 degrees and +/- 15 degrees, including a vertical and horizontal movement in the plane. For both phantom experiments images were reconstructed for both the fixed and motion corrected data. Measurements for resolution, full width at half maximum (FWHM) and full width at tenth maximum (FWTM), were calculated from these images and a comparison made between the fixedand motion corrected datasets. From the point source measurements, the FWHM at each axial position was 7.1 mm in the horizontal direction, and increasing from 4.7 mm at the 0 mm position, to 4.8 mm, 20 mm offset, in the vertical direction. The results from the multi-line source phantom with +/- 5 degrees rotations showed a maximum degradation in FWHM, when compared with the stationary phantom, of 0.6 mm, in the horizontal direction, and 0.3 mm in the vertical direction. The corresponding values for the larger rotation, +/- 15 degrees, were 0.7 mm and 1.1 mm, respectively. The performance of the method was confirmed with a Hoffman brain phantom moved continuously, and a clinical acquisition using [11C]raclopride (normal volunteer). A visual comparison of both the motion and non-motion corrected images of the Hoffman brain phantom clearly demonstrated the efficacy of the method. A sample time-activity curve extracted from the clinical study showed irregularities prior to motion correction, which were removed after correction. A method has been developed to accurately monitor the motion of the head during a neurological PET acquisition, and correct for this motion prior to image reconstruction. The method has been demonstrated to be accurate and does not add significantly to either the acquisition or the subsequent data processing.
},
	Author = {Bloomfield, P. M. and Spinks, T. and Reed, J. and Schnorr, L. and Westrip, A. M. and Livieratos, L. and Fulton, R. and Jones, T.},
	Date-Added = {2015-01-31 21:28:40 +0000},
	Date-Modified = {2015-01-31 21:30:36 +0000},
	Journal = {Physics in Medicine and Biology},
	Number = {8},
	Pages = {959--978},
	Title = {The design and implementation of a motion correction scheme for neurological {PET}},
	Volume = {48},
	Year = {2003}}

@article{MeAtBu96,
	Abstract = {We describe two methods to correct for motion artifacts in head scans in positron emission tomography. The methods are based on 6D motion data that have to be acquired simultaneously during scanning. The data are supposed to represent the rotational and translational deviations of the head as a function of the motion measurement cycle, with respect to the initial head position. The first compensation method is a re-binning procedure by which the lines of response are geometrically transformed according to the current values of the motion data, assuming a cylindrical scanner geometry. An approximation of the re-binning transformations by use of large look-up tables, having the potential of on-line event processing, is presented. The second method describes post-processing of the reconstructed images by unconstrained or constrained deconvolution of the image or image segments with kernels that are generated from the motion data. We use motion data that were acquired with a volunteer in supine position to demonstrate the effects of the compensation methods

},
	Author = {Menke, M. and Atkins, M. S. and Buckley, K. R.},
	Date-Added = {2015-01-31 21:27:29 +0000},
	Date-Modified = {2015-01-31 21:28:29 +0000},
	Journal = {IEEE Transactions in Nuclear Science},
	Number = {1},
	Pages = {310---317},
	Title = {Compensation methods for head motion detected during {PET} imaging},
	Volume = {43},
	Year = {1996}}

@article{DaYaGrCa90,
	Author = {Daube-Witherspoon, M. and Yan, Y. and Green, M. and Carson, R.},
	Date-Added = {2015-01-31 21:23:52 +0000},
	Date-Modified = {2015-01-31 21:26:21 +0000},
	Journal = {Journal of Nuclear Medicine},
	Pages = {816},
	Title = {Correction for motion distortion in {PET} by dynamic monitoring of patient position},
	Volume = {31},
	Year = {1990}}

@article{ReAlBaMaWa02,
	Abstract = {High resolution 3D PET scanners with high count rate performance, such as the quad-HIDAC, place new demands on image reconstruction algorithms due to the large quantities of high precision list-mode data which are produced. Therefore a reconstruction algorithm is required which can, in a practical time frame, reconstruct into very large image arrays (submillimetre voxels, which range over a large field of view) whilst preferably retaining the precision of the data. This work presents an algorithm which meets these demands: Regularized One-Pass List-mode EM (ROPLE). The algorithm operates directly on list-mode data, passes through the data once only, accounts for finite resolution effects in the system model and also includes regularization. The algorithm performs multiple image updates during its single pass through the list-mode data, corresponding to the number of subsets that the data have been split into. The algorithm has been assessed using list-mode data from a quad-HIDAC, and is compared to the analytic reconstruction method 3D RP

},
	Author = {Reader, A. J. and Ally, S. and Bakatselos, F. and Manavaki, R. and Walledge, R. J. and Jeavons, A. P. and Julyan, P. J. and Zhao, S. and Hastings, D. L. and Zweit, J.},
	Date-Added = {2015-01-31 21:21:30 +0000},
	Date-Modified = {2015-01-31 21:23:38 +0000},
	Journal = {IEEE Transactions in Nuclear Science},
	Number = {3},
	Pages = {693--699},
	Title = {One-pass list-mode {EM} algorithm for high-resolution {3-D} {PET} image reconstruction into large arrays},
	Volume = {49},
	Year = {2002}}

@article{MuLeChal94,
	Abstract = {The authors describe conjugate gradient algorithms for reconstruction of transmission and emission PET images. The reconstructions are based on a Bayesian formulation, where the data are modeled as a collection of independent Poisson random variables and the image is modeled using a Markov random field. A conjugate gradient algorithm is used to compute a maximum a posteriori (MAP) estimate of the image by maximizing over the posterior density. To ensure nonnegativity of the solution, a penalty function is used to convert the problem to one of unconstrained optimization. Preconditioners are used to enhance convergence rates. These methods generally achieve effective convergence in 15-25 iterations. Reconstructions are presented of an 18FDG whole body scan from data collected using a Siemens/CTI ECAT931 whole body system. These results indicate significant improvements in emission image quality using the Bayesian approach, in comparison to filtered backprojection, particularly when reprojections of the MAP transmission image are used in place of the standard attenuation correction factors

},
	Author = {Mumcuoglu, E. U. and Leahy, R. M. and Cherry, S. R. and et al.},
	Date-Added = {2015-01-31 21:20:01 +0000},
	Date-Modified = {2015-01-31 21:21:11 +0000},
	Journal = {IEEE Transactions on Medical Imaging},
	Number = {4},
	Pages = {687--701},
	Title = {Fast gradient-based methods for Bayesian reconstruction of transmission and emission {PET} images},
	Volume = {13},
	Year = {1994}}

@article{HeLe90,
	Abstract = {A fast approach to including attenuation in iterative maximum-likelihood and least-squares algorithms for single-photon-emission computed tomography (SPECT) is presented. Ray-tracing and summing of attenuation coefficients are replaced by the use of two lookup tables, one to compute attenuated ray path integrals based on a set of polar grid points and one to perform polar-to-rectangular transformations. The resulting algorithm implements a spatial average which is comparable in accuracy to ray-tracing with rectangular pixels, yet requires less than one sixteenth the CPU time

},
	Author = {Hebert, T. J. and Leahy, R. M.},
	Date-Added = {2015-01-31 21:18:42 +0000},
	Date-Modified = {2015-01-31 21:19:45 +0000},
	Journal = {IEEE Transactions in Nuclear Science},
	Number = {2},
	Pages = {754---758},
	Title = {Fast methods for including attenuation in the {EM} algorithm},
	Volume = {37},
	Year = {1990}}

@article{TaBeRa10,
	Abstract = {Objectives: In the context of dynamic cardiac imaging, gating information is not routinely utilized. Our goal is to introduce gated motion correction within dynamic Rb-82 reconstruction and to evaluate its effect on the quantitative estimation of tracer transport between blood and myocardial tissue.

Methods: Making use of the list-mode acquisition capability along with gating information, we propose to first estimate motion from reconstructed single-frame, multi-gated images obtained by binning the acquired data, starting after initial blood uptake (i.e. from ~0.5min to 2min). Next, starting from time of injection, for each dynamic frame, the gated data would be combined within a 4D EM reconstruction algorithm that incorporates the estimated motion. To validate, stress time activity curves of blood pool, myocardium, and other organs, as extracted from Rb-82 PET images of 5 normal patients, were averaged and smoothed, and used to simulate a 5D NCAT phantom with both gates (8 gates) and dynamic frames (10 frames*12 sec). Analytical simulation was performed to simulate gated, dynamic noise-free PET data, which were scaled to clinical count levels before 30 Poisson noise fluctuations were created. Image reconstruction was performed, without and with motion correction for each dynamic frame. Kinetic rate constants were extracted from the noise-free and noisy reconstructed images using a two-compartment model, and compared between the conventional and proposed methods.

Results: With gate-motion-corrected (4D) reconstruction for each dynamic frame, the estimated K1 values in each of the five regions (anterior, septal, inferior, lateral, and apex) were shown to be significantly increased relative to no motion-correction in both noise-free and noisy cases (p<0.0001). The average increase of K1 in both the noise-free and noisy case was 33% over the 5 regions, with the motion-corrected estimation close to the truth (5% error).

Conclusions: Incorporation of cardiac motion correction within dynamic Rb-82 PET imaging was shown to improve the accuracy of estimated K1 values compared to no motion correction

},
	Author = {Tang, J. and Bengel, F. M. and Rahmim, A.},
	Date-Added = {2015-01-31 21:16:11 +0000},
	Date-Modified = {2015-01-31 21:17:39 +0000},
	Journal = {Journal of Nuclear Medicine},
	Number = {Suppl 2},
	Pages = {123},
	Title = {Cardiac motion-corrected quantitative dynamic {Rb-82} {PET} imaging},
	Volume = {51},
	Year = {2010}}

@article{Gr90,
	Abstract = {A novel method of reconstruction from single-photon emission computerized tomography data is proposed. This method builds on the expectation-maximization (EM) approach to maximum likelihood reconstruction from emission tomography data, but aims instead at maximum posterior probability estimation, which takes account of prior belief about smoothness in the isotope concentration. A novel modification to the EM algorithm yields a practical method. The method is illustrated by an application to data from brain scans

},
	Author = {Green, P. J.},
	Date-Added = {2015-01-31 21:12:03 +0000},
	Date-Modified = {2015-01-31 21:13:05 +0000},
	Journal = {IEEE Transactions on Medical Imaging},
	Number = {1},
	Pages = {84--93},
	Title = {Bayesian reconstructions from emission tomography data using a modified {EM} algorithm},
	Volume = {9},
	Year = {1990}}

@article{LaTs98,
	Abstract = {We introduce a fast block-iterative maximum a posteriori (MAP) reconstruction algorithm and apply it to four-dimensional reconstruction of gated SPECT perfusion studies. The new algorithm, called RBI-MAP, is based on the rescaled block iterative EM (RBI-EM) algorithm. We develop RBI-MAP based on similarities between the RBI-EM, ML-EM and MAP-EM algorithms. RBI-MAP requires far fewer iterations than MAP-EM, and so should result in acceleration similar to that obtained from using RBI-EM or OS-EM as opposed to ML-EM. When complex four-dimensional clique structures are used in the prior, however, evaluation of the smoothing prior dominates the processing time. We show that a simple scheme for updating the prior term in the heart region only for RBI-MAP results in savings in processing time of a factor of six over MAP-EM. The RBI-MAP algorithm incorporating 3D collimator-detector response compensation is demonstrated on a simulated Tc gated perfusion study. Results of RBI-MAP are compared with RBI-EM followed by a 4D linear filter. For the simulated study, we find that RBI-MAP provides consistently higher defect contrast for a given degree of noise smoothing than does filtered RBI-EM. This is an indication that RBI-MAP smoothing does less to degrade resolution gained from 3D detector response compensation than does a linear filter. We conclude that RBI-MAP can provide smooth four-dimensional reconstructions with good visualization of heart structures in clinically realistic processing times.

},
	Author = {Lalush, D. S. and Tsui, B. M. W.},
	Date-Added = {2015-01-31 21:09:57 +0000},
	Date-Modified = {2015-01-31 21:11:15 +0000},
	Journal = {Physics in Medicine and Biology},
	Number = {4},
	Pages = {875---886},
	Title = {Block-iterative techniques for fast {4D} reconstruction using a priori motion models in gated cardiac {SPECT}},
	Volume = {43},
	Year = {1998}}

@conference{LaLiTs96,
	Abstract = {The authors investigate the benefit of incorporating a priori assumptions about cardiac motion in a fully four-dimensional (4D) reconstruction algorithm for gated cardiac SPECT. Previous work has shown that non-motion-specific 4D Gibbs priors enforcing smoothing in time and space can control noise while preserving resolution. Here, the authors evaluate methods for incorporating known heart motion in the Gibbs prior model. The new model is derived by assigning motion vectors to each 4D voxel, defining the movement of that volume of activity into the neighboring time frames. Weights for the Gibbs cliques are computed based on these ``most likely'' motion vectors. To evaluate, the authors employ the mathematical cardiac-torso (MCAT) phantom with a new dynamic heart model that simulates the beating and twisting motion of the heart. Sixteen realistically-simulated gated datasets were generated, with noise simulated to emulate a real Tl-201 gated SPECT study. Reconstructions were performed using several different reconstruction algorithms, all modeling nonuniform attenuation and three-dimensional detector response. These include ML-EM with 4D filtering, 4D MAP-EM without prior motion assumption, and 4D MAP-EM with prior motion assumptions. The prior motion assumptions included both the correct motion model and incorrect models. Results show that reconstructions using the 4D prior model can smooth noise and preserve time-domain resolution more effectively than 4D linear filters. The authors conclude that modeling of motion in 4D reconstruction algorithms can be a powerful tool for smoothing noise and preserving temporal resolution in gated cardiac studies

},
	Author = {Lalush, D. S. and Lin, C. and Tsui, B. M. W.},
	Booktitle = {IEEE Nuclear Science Symposium Conference Record, 1996. Anaheim (CA), November 2--9, 1996},
	Date-Added = {2015-01-31 21:07:45 +0000},
	Date-Modified = {2015-01-31 21:09:41 +0000},
	Pages = {1923--1927},
	Title = {A priori motion models for four-dimensional reconstruction in gated cardiac {SPECT}},
	Year = {1996}}

@article{GrYaJi07,
	Abstract = {In this paper, we propose an approach for the reconstruction of dynamic images from a gated cardiac data acquisition. The goal is to obtain an image sequence that can show simultaneously both cardiac motion and time-varying image activities. To account for the cardiac motion, the cardiac cycle is divided into a number of gate intervals, and a time-varying image function is reconstructed for each gate. In addition, to cope with the under-determined nature of the problem, the time evolution at each pixel is modeled by a B-spline function. The dynamic images for the different gates are then jointly determined using maximum a posteriori estimation, in which a motion-compensated smoothing prior is introduced to exploit the similarity among the different gates. The proposed algorithm is evaluated using a dynamic version of the 4-D gated mathematical cardiac torso phantom simulating a gated single photon emission computed tomography perfusion acquisition with Technitium-99m labeled Teboroxime. We thoroughly evaluated the performance of the proposed algorithm using several quantitative measures, including signal-to-noise ratio analysis, bias-variance plot, and time activity curves. Our results demonstrate that the proposed joint reconstruction approach can improve significantly the accuracy of the reconstruction

},
	Author = {Gravier, E. and Yang, Y. Y. and Jin, M. W.},
	Date-Added = {2015-01-31 21:06:04 +0000},
	Date-Modified = {2015-01-31 21:07:08 +0000},
	Journal = {IEEE Transactions on Image Processing},
	Number = {4},
	Pages = {932---942},
	Title = {Tomographic reconstruction of dynamic cardiac image sequences},
	Volume = {16},
	Year = {2007}}

@article{GrYa05,
	Abstract = {In this paper, we study a motion-compensated approach for simultaneous reconstruction of image frames in a time sequence. We treat the frames in a sequence collectively as a single function of both space and time, and define a temporal prior to account for the temporal correlations in a sequence. This temporal prior is defined in a form of motion-compensation, aimed to follow the curved trajectories of the object motion through space-time. The image frames are then obtained through estimation using the expectation-maximization (EM) algorithm. The proposed algorithm was evaluated using the four-dimensional (4-D) gated mathematical cardiac-torso (gMCAT) D1.01 phantom to simulate gated single photon emission computed tomography (SPECT) perfusion imaging with Tc-99m-sestamibi. Our experimental results demonstrate that the use of motion compensation for reconstruction can lead to significant improvement in image quality and reconstruction accuracy.

},
	Author = {Gravier, E. J. and Yang, Y.},
	Date-Added = {2015-01-31 21:04:06 +0000},
	Date-Modified = {2015-01-31 21:05:09 +0000},
	Journal = {IEEE Transactions in Nuclear Science},
	Number = {1},
	Pages = {51---56},
	Title = {Motion-compensated reconstruction of tomographic image sequences},
	Volume = {52},
	Year = {2005}}

@conference{ReMaSuCoTr06,
	Abstract = {4D PET imaging seeks to estimate kinetic parameters of physiological significance through the generation of a time series of 3D images. Conventionally the time series is reconstructed one frame at a time, and then the kinetic modeling is applied as a post-reconstruction step to estimate the desired parameters. Such a separated approach does not account for the task of kinetic parameter estimation within the reconstruction itself. This work indicates that conventional frame-by-frame maximum likelihood reconstruction in high noise situations is sub-optimal if post-reconstruction kinetic parameter estimation is to be performed. As an alternative, a simple to implement, EM-based iterative reconstruction method is proposed which uses all of the acquired data in every iteration and includes the image-space kinetic parameter estimation process within the reconstruction. The method can accommodate kinetic models of any chosen complexity with relative ease, and can deliver more accurate kinetic parameter estimates than the conventional approach for low-statistics data.

},
	Author = {Reader, A. J. and Matthews, J. C. and Sureau, F. C. and Comtat, C. and Trebossen, R. and Buvat, I.},
	Booktitle = {IEEE Nuclear Science Symposium Conference Record. San Diego, CA, 29 October - 4 November, 2006},
	Date-Added = {2015-01-31 20:58:33 +0000},
	Date-Modified = {2015-01-31 21:00:16 +0000},
	Pages = {1752---1756.},
	Title = {Iterative kinetic parameter estimation within fully {4D PET} image reconstruction},
	Year = {2006}}

@article{KaGu01,
	Abstract = {A 4D ordered-subsets maximum a posteriori (OSMAP) algorithm for dynamic SPECT is described which uses a temporal prior that constrains each voxel's behaviour in time to conform to a compartmental model. No a priori limitations on kinetic parameters are applied; rather, the parameter estimates evolve as the algorithm iterates to a solution. The estimated parameters and time-activity curves are used within the reconstruction algorithm to model changes in the activity distribution as the camera rotates, avoiding artefacts due to inconsistencies of data between projection views. This potentially allows for fewer, longer-duration scans to be used and may have implications for noise reduction. The algorithm was evaluated qualitatively using dynamic 99mTc-teboroxime SPECT scans in two patients, and quantitatively using a series of simulated phantom experiments. The OSMAP algorithm resulted in images with better myocardial uniformity and definition, gave time-activity curves with reduced noise variations, and provided wash-in parameter estimates with better accuracy and lower statistical uncertainty than those obtained from conventional ordered-subsets expectation-maximization (OSEM) processing followed by compartmental modelling. The new algorithm effectively removed the bias in k21 estimates due to inconsistent projections for sampling schedules as slow as 60 s per timeframe, but no improvement in wash-out parameter estimates was observed in this work. The proposed dynamic OSMAP algorithm provides a flexible framework which may benefit a variety of dynamic tomographic imaging applications.
},
	Author = {Kadrmas, D. J. and Gullberg, G. T.},
	Date-Added = {2015-01-31 20:57:03 +0000},
	Date-Modified = {2015-01-31 20:58:13 +0000},
	Journal = {Physics in Medicine and Biology},
	Number = {5},
	Pages = {1553---1574},
	Title = {{4D} maximum a posteriori reconstruction in dynamic {SPECT} using a compartmental model-based prior},
	Volume = {46},
	Year = {2001}}

@article{MuTh04,
	Abstract = {In this paper, we study the resolution properties of those algorithms where a filtering step is applied after every iteration. As concrete examples we take filtered preconditioned gradient descent algorithms for the Poisson log likelihood for PET emission data. For nonlinear estimators, resolution can be characterized in terms of the linearized local impulse response (LLIR). We provide analytic approximations for the LLIR for the class of algorithms mentioned above. Our expressions clearly show that when interiteration filtering (with linear filters) is used, the resolution properties are, in most cases, spatially varying, object dependent and asymmetric. These nonuniformities are solely due to the interaction between the filtering step and the Poisson noise model. This situation is similar to penalized likelihood reconstructions as studied previously in the literature. In contrast, nonregularized and postfiltered maximum-likelihood expectation maximization (MLEM) produce images with nearly "perfect" uniform resolution when convergence is reached. We use the analytic expressions for the LLIR to propose three different approaches to obtain nearly object independent and uniform resolution. Two of them are based on calculating filter coefficients on a pixel basis, whereas the third one chooses an appropriate preconditioner. These three approaches are tested on simulated data for the filtered MLEM algorithm or the filtered separable paraboloidal surrogates algorithm. The evaluation confirms that images obtained using our proposed regularization methods have nearly object independent and uniform resolution.

},
	Author = {Mustafovic, S. and Thielemans, K.},
	Date-Added = {2015-01-31 20:55:30 +0000},
	Date-Modified = {2015-01-31 20:56:45 +0000},
	Journal = {IEEE Transactions on Medical Imaging},
	Number = {4},
	Pages = {433--446},
	Title = {Object dependency of resolution in reconstruction algorithms with inter-iteration filtering applied to {PET} data},
	Volume = {23},
	Year = {2004}}

@article{JaLeBeThSp00,
	Abstract = {We present an enhancement of the OSEM (ordered set expectation maximization) algorithm for 3D PET reconstruction, which we call the inter-update Metz filtered OSEM (IMF-OSEM). The IMF-OSEM algorithm incorporates filtering action into the image updating process in order to improve the quality of the reconstruction. With this technique, the multiplicative correction image--ordinarily used to update image estimates in plain OSEM--is applied to a Metz-filtered version of the image estimate at certain intervals. In addition, we present a software implementation that employs several high-speed features to accelerate reconstruction. These features include, firstly, forward and back projection functions which make full use of symmetry as well as a fast incremental computation technique. Secondly, the software has the capability of running in parallel mode on several processors. The parallelization approach employed yields a significant speed-up, which is nearly independent of the amount of data. Together, these features lead to reasonable reconstruction times even when using large image arrays and non-axially compressed projection data. The performance of IMF-OSEM was tested on phantom data acquired on the GE Advance scanner. Our results demonstrate that an appropriate choice of Metz filter parameters can improve the contrast-noise balance of certain regions of interest relative to both plain and post-filtered OSEM, and to the GE commercial reprojection algorithm software.
},
	Author = {Jacobson, M. and Levkovitz, R. and Ben-Tal, A. and Thielemans, K. and Spinks, T. and Belluzzo, D. and Pagani, E. and Bettinardi, V. and Gilardi, M. C. and Zverovich, A. and Mitra, G.},
	Date-Added = {2015-01-31 20:52:46 +0000},
	Date-Modified = {2015-01-31 20:55:02 +0000},
	Journal = {Physics in Medicine and Biology},
	Number = {8},
	Pages = {2417---2439},
	Title = {Enhanced {3D PET OSEM} reconstruction using inter-update {M}etz filtering},
	Volume = {45},
	Year = {2000}}

@article{BrYaWe05,
	Abstract = {In this paper we present a spatiotemporal processing approach, based on deformable mesh modeling, for noise reduction in gated cardiac single-photon emission computed tomography images. Because of the partial volume effect (PVE), clinical cardiac-gated perfusion images exhibit a phenomenon known as brightening-the myocardium appears to become brighter as the heart wall thickens. Although brightening is an artifact, it serves as an important diagnostic feature for assessment of wall thickening in clinical practice. Our proposed processing algorithm aims to preserve this important diagnostic feature while reducing the noise level in the images. The proposed algorithm is based on the use of a deformable mesh for modeling the cardiac motion in a gated cardiac sequence, based on which the images are processed by smoothing along space-time trajectories of object points while taking into account the PVE. Our experiments demonstrate that the proposed algorithm can yield significantly more-accurate results than several existing methods.
},
	Author = {Brankov, J. G. and Yang, Y. and Wernick, M. N.},
	Date-Added = {2015-01-31 20:50:49 +0000},
	Date-Modified = {2015-01-31 20:52:09 +0000},
	Journal = {Medical Physics},
	Number = {9},
	Pages = {2839---2849.},
	Title = {Spatiotemporal processing of gated cardiac {SPECT} images using deformable mesh modeling},
	Volume = {32},
	Year = {2005}}

@article{MaBr10,
	Abstract = {PURPOSE: 
In this article, the authors present a motion-compensated spatiotemporal processing algorithm to reduce noise in cardiac gated SPECT. Cardiac gated SPECT data are particularly noisy because the acquired photon data are divided among a number of time frames (gates). Classical spatial reconstruction and processing techniques offer noise reduction but they are usually applied on each frame separately and fail to utilize temporal correlation between frames.
METHODS:
In this work, the authors present a motion-compensated spatiotemporal postreconstruction filter offering noise reduction while minimizing motion-blur artifacts. The proposed method can be used regardless of the type of image-reconstruction method (analytical or iterative). The between-frame volumetric myocardium motion is estimated using a deformable mesh model based on the model of the myocardial surfaces. The estimated motion is then used to perform spatiotemporal filtering along the motion trajectories. Both the motion-estimation and spatiotemporal filtering methods seek to maintain the wall brightening seen during cardiac contraction. Wall brightening is caused by the partial volume effect, which is usually viewed as an artifact; however, wall brightening is a useful signature in clinical practice because it allows the clinician to visualize wall thickening. Therefore, the authors seek in their method to preserve the brightening effect.
RESULTS:
The authors find that the proposed method offers better noise reduction than several existing methods as quantitatively evaluated by signal-to-noise ratio, bias-variance plots, and ejection fraction analysis as well as on tested clinical data.
CONCLUSIONS:
The proposed method mitigates for noise in cardiac gated SPECT images using a postreconstruction motion-compensated filtering approach. Visual as well as quantitative evaluation show considerable improvement in image quality.
},
	Author = {Marin, T. and Brankov, J. G.},
	Date-Added = {2015-01-31 20:49:29 +0000},
	Date-Modified = {2015-01-31 20:50:23 +0000},
	Journal = {Medical Physics},
	Number = {10},
	Pages = {5471---5481},
	Title = {Deformable left-ventricle mesh model for motion-compensated filtering in cardiac gated {SPECT}},
	Volume = {37},
	Year = {2010}}

@conference{BrYaNaWe02,
	Abstract = {In this paper ire present two 4D motion-compensated reconstruction algorithms for gated cardiac SPECT. The algorithms are based on a deformable content-adaptive mesh model, a compact image representation that is well suited for motion tracking of non-rigid objects like the heart. In this paper we introduce a reconstruction method using inter-iteration smoothing, in which motion estimation, reconstruction, and smoothing are all performed in the mesh domain. We also describe a pixel-based post-reconstruction temporal smoothing approach. In these methods, we modify our previous approaches to account for apparent brightening due to wall thickening (tin effect of finite resolution). Our experiments show that the new algorithms produce image sequences that closely match the expected intensity variations at the spatial resolutions of interest. In gated SPECT, these variations are a useful and important clinical indicator of wall thickening, so it is beneficial to preserve them. In our experiments, the proposed methods outperform those that do not take into account the brightening effect.

},
	Author = {Brankov, J. G. and Yang, Y. and Narayanan, M. V. and Wermck, M. N.},
	Booktitle = {IEEE Nuclear Science Symposium Conference Record, 2002. Norfolk (VA), November 10--16, 2002},
	Date-Added = {2015-01-31 20:47:33 +0000},
	Date-Modified = {2015-01-31 20:49:15 +0000},
	Pages = {1380--1384},
	Title = {Motion-compensated {4D} processing of gated {SPECT} perfusion studies},
	Year = {2002}}

@article{BrYaWe04,
	Abstract = {In this paper, we explore the use of a content-adaptive mesh model (CAMM) for tomographic image reconstruction. In the proposed framework, the image to be reconstructed is first represented by a mesh model, an efficient image description based on nonuniform sampling. In the CAMM, image samples (represented as mesh nodes) are placed most densely in image regions having fine detail. Tomographic image reconstruction in the mesh domain is performed by maximum-likelihood (ML) or maximum a posteriori (MAP) estimation of the nodal values from the measured data. A CAMM greatly reduces the number of unknown parameters to be determined, leading to improved image quality and reduced computation time. We demonstrated the method in our experiments using simulated gated single photon emission computed tomography (SPECT) cardiac-perfusion images. A channelized Hotelling observer (CHO) was used to evaluate the detectability of perfusion defects in the reconstructed images, a task-based measure of image quality. A minimum description length (MDL) criterion was also used to evaluate the effect of the representation size. In our application, both MDL and CHO suggested that the optimal number of mesh nodes is roughly five to seven times smaller than the number of projection bins. When compared to several commonly used methods for image reconstruction, the proposed approach achieved the best performance, in terms of defect detection and computation time. The research described in this paper establishes a foundation for future development of a (four-dimensional) space-time reconstruction framework for image sequences in which a built-in deformable mesh model is used to track the image motion.
},
	Author = {Brankov, J. G. and Yang, Y. and Wernick, M. N.},
	Date-Added = {2015-01-31 20:45:04 +0000},
	Date-Modified = {2015-01-31 20:46:39 +0000},
	Journal = {IEEE Transactions on Medical Imaging},
	Number = {2},
	Pages = {202---212},
	Title = {Tomographic image reconstruction based on a content-adaptive mesh model},
	Volume = {23},
	Year = {2004}}

@article{ChBrLoTs12,
	Abstract = {Objectives: The objectives are to develop a four-dimensional (4D) PET image reconstruction method with respiratory and cardiac motion compensation and evaluate its performance with patient list-mode data from cardiac PET scans.

Methods: The respiratory motion (RM) amplitude signal was first extracted from the list-mode data and used to rebin the data into 6 frames of amplitude-based respiratory-gated sinograms. A maximum likelihood estimation method was applied to the respiratory-gated sinograms to estimate a PET image at a pre-selected reference frame and patient RM from the reference frame to the other respiratory-gated frames. We then utilized the data-driven RM signal and ECG triggers embedded in the list-mode data for simultaneous respiratory and cardiac gating to bin the data into 48 sinograms corresponding to 8 cardiac phases and 6 respiratory amplitudes respectively. For each cardiac phase, we reconstructed a PET image with RM compensation from the 6 respiratory-gated sinograms using a 4D ML-EM image reconstruction algorithm with modeling the previously estimated RM. Then we registered the 8 RM-compensated cardiac-gated PET images to every cardiac phase by a non-rigid image registration method for cardiac motion (CM) compensation. The final outputs of our 4D image reconstruction method were 8 cardiac-gated PET images, each was reconstructed using the entire PET dataset and with both RM and CM compensation. This approach was evaluated with both FDG(n=1) and NH3(n=1) stress studies.

Results: The myocardium-to-chamber contrast was improved by 15% using our method compared to a conventional method without any motion compensation. The noise level of each 4D PET image by our method was 60% lower than cardiac-gated images reconstructed by the conventional method. As a result, the cardiac motion defect became more prominent with our method.

Conclusions: Our 4D PET image reconstruction method with RM and CM compensation has a great potential to improve the image quaity for cardiac-gated PET imaging .},
	Author = {Chen, S. and Bravo, P. and Lodge, M. and Tsui, B. M. W.},
	Date-Added = {2015-01-31 20:20:26 +0000},
	Date-Modified = {2015-01-31 21:08:15 +0000},
	Journal = {Journal of Nuclear Medicine},
	Number = {1},
	Pages = {106},
	Title = {Four-dimensional {PET} image reconstruction with respiratory and cardiac motion compensation from list-mode data},
	Volume = {53},
	Year = {2012}}

@article{LaCrSaLeRe07,
	Abstract = {Respiratory motion is a source of artefacts and reduced image quality in PET. Proposed methodology for correction of respiratory effects involves the use of gated frames, which are however of low signal-to-noise ratio. Therefore a method accounting for respiratory motion effects without affecting the statistical quality of the reconstructed images is necessary. We have implemented an affine transformation of list mode data for the correction of respiratory motion over the thorax. The study was performed using datasets of the NCAT phantom at different points throughout the respiratory cycle. List mode data based PET simulated frames were produced by combining the NCAT datasets with a Monte Carlo simulation. Transformation parameters accounting for respiratory motion were estimated according to an affine registration and were subsequently applied on the original list mode data. The corrected and uncorrected list mode datasets were subsequently reconstructed using the one-pass list mode EM (OPL-EM) algorithm. Comparison of corrected and uncorrected respiratory motion average frames suggests that an affine transformation in the list mode data prior to reconstruction can produce significant improvements in accounting for respiratory motion artefacts in the lungs and heart. However, the application of a common set of transformation parameters across the imaging field of view does not significantly correct the respiratory effects on organs such as the stomach, liver or spleen.

},
	Author = {Lamare, F. and Cresson, T. and Savean, J. and Le Rest, C. C. and Reader, A. J. and Visvikis, D.},
	Date-Added = {2015-01-31 20:18:23 +0000},
	Date-Modified = {2015-01-31 20:20:00 +0000},
	Journal = {Physics in Medicine and Biology},
	Number = {1},
	Pages = {121--140},
	Title = {Respiratory motion correction for {PET} oncology applications using affine transformation of list mode data},
	Volume = {52},
	Year = {2007}}

@article{KlReHu01,
	Abstract = {The heart position shifts considerably due to motion associated with the respiratory cycle, and this motion can degrade the image quality of cardiac-gated positron emission tomography (PET) studies. One method to combat this motion-induced blur is a respiratory-gated acquisition followed by recombination of registered image volumes using a rigid-body motion assumption; however, nonrigid deformation of the heart from respiratory motion may reduce the effectiveness of this procedure. We have investigated a 12-parameter global affine motion model for registration of different respiratory gates in an end-diastolic cardiac PET sequence. To obtain robust estimates of motion, a four-dimensional registration model was devised that encouraged smoothly varying motion between adjacent respiratory time frames. Registration parameters were iteratively calculated using a cost function that combined a least squares voxel difference measure with a penalty obtained from a prediction prior. The prior was calculated from adjacent time frames assuming constant velocity and an affine model. After registration, the principal extension ratios were calculated to measure the degree of nonrigid motion. In data from ten subjects, extension ratios of over 5% were common, indicating that an affine model may provide better registrations and in turn, better motion-corrected composite volumes than could a technique restricted to the six-parameter rigid body assumption

},
	Author = {Klein, G. J. and Reutter, R. W. and Huesman, R. H.},
	Date-Added = {2015-01-31 20:15:29 +0000},
	Date-Modified = {2015-01-31 20:17:16 +0000},
	Journal = {IEEE Transactions in Nuclear Science},
	Number = {3},
	Pages = {756---760},
	Title = {Four-dimensional affine registration models for respiratory-gated {PET}},
	Volume = {48},
	Year = {2001}}

@article{AnVi84,
	Abstract = {The effects of quiet respiration on assessment of left ventricular function by two-dimensional echocardiography were investigated in 12 healthy men. End-diastolic area in the parasternal short-axis view decreased with inspiration (from 17.3 +/- 2.1 [mean +/- SD] to 16.0 +/- 2.1 cm2, p less than .01), while end-systolic area did not change (from 7.6 +/- 1.4 to 7.7 +/- 1.5 cm2; NS). A fixed cursor that was located through the center of the left ventricular area at end-expiration made a tangential cut of the area at end-inspiration were smaller along the cursor than through the center of the short-axis area both at end-diastole (1.9 +/- 1.7 mm; p less than .01) and end-systole (3.8 +/- 4.0 mm; p less than .01). Our results suggest a need for standardization with regard to respiratory phases in assessment of left ventricular function by two-dimensional echocardiography and indicate the occurrence of inspiratory reduction of left ventricular stroke volume associated with decreased diastolic filling. Motion of the heart relative to the echo beam may play a part in the respiratory variations in left ventricular dimensions assessed by M mode echocardiography.
},
	Author = {Andersen, K. and Vik-Mo, H.},
	Date-Added = {2015-01-31 20:14:11 +0000},
	Date-Modified = {2015-01-31 20:15:01 +0000},
	Journal = {Circulation},
	Number = {5},
	Pages = {874--879},
	Title = {Effects of spontaneous respiration on left ventricular function assessed by echocardiography},
	Volume = {69},
	Year = {1984}}

@article{KoEhRial92,
	Abstract = {Despite the fact that respiratory motion is a major factor limiting the image quality of MR examinations in the upper abdomen, little quantitative information is available about the kinematics of visceral motion during respiration. The objective of this study was to obtain a measure of the relative longitudinal and transverse displacements of the upper abdominal organs during breathing using an MR line scan technique.
},
	Author = {Korin, H. W. and Ehman, R. L. and Riederer, S. J. and Felmlee, J. P. and Grimm, R. C.},
	Date-Added = {2015-01-31 20:12:05 +0000},
	Date-Modified = {2015-01-31 20:13:48 +0000},
	Journal = {Magnetic Resonance in Medicine},
	Number = {1},
	Pages = {172---178},
	Title = {Respiratory kinematics of the upper abdominal organs --- a quantitative study},
	Volume = {23},
	Year = {1992}}

@conference{SeMoChTs07,
	Abstract = {The current 4D NCAT phantom includes a flexible, parameterized respiratory model based on respiratory-gated CT data of a normal subject. A limitation of this model is that it is based on only one realization of the normal respiratory motion. The data upon which it was based also had a resolution lower than that offered by more advanced CT scanners and consisted of only four time frames that did not adequately cover normal tidal breathing. We further develop the 4D NCAT to more accurately model normal and abnormal states of respiration. Over two-hundred sets of 4D respiratory gated CT image data from normal and abnormal patients obtained from the Massachusetts General Hospital were used to characterize variations in the respiratory motion. Each dataset contains twenty time frames over the respiratory cycle with the patient breathing normally. With the improved resolution and better coverage of tidal breathing, this data was used to improve the respiratory model of the 4D NCAT phantom. Using automatic and semiautomatic techniques, the different respiratory structures were segmented from each time frame of each CT dataset. The time series of segmented structures were used to characterize the respiratory motion in each case. From an analysis of all normal and abnormal patient datasets, we determined the range of sizes and shapes of the right and left lungs and the range in motion (expansion in the anterior and inferior (diaphragm) directions) in the different lung regions. This analysis was used to further parameterize the general respiratory model of the 4D NCAT to more realistically model normal and abnormal variations in anatomy and in the respiratory motion. With the ability to model variations in the respiratory motion indicative of a patient population, the phantom will be a great resource to investigate the effects of respiratory motion on medical imaging and to develop compensation methods for these effects.

},
	Author = {Segars, W. P. and Mori, S. and Chen, G. T. Y. and Tsui, B. M. W.},
	Booktitle = {Nuclear Science Symposium Conference Record, 2007. NSS '07. IEEE},
	Date-Added = {2015-01-31 20:09:20 +0000},
	Date-Modified = {2015-01-31 20:11:44 +0000},
	Pages = {2677---2679},
	Title = {Modeling respiratory motion variations in the {4D NCAT} phantom},
	Year = {2007}}

@article{LaLeCrKoSa07,
	Abstract = {Respiratory motion in emission tomography leads to reduced image quality. Developed correction methodology has been concentrating on the use of respiratory synchronized acquisitions leading to gated frames. Such frames, however, are of low signal-to-noise ratio as a result of containing reduced statistics. In this work, we describe the implementation of an elastic transformation within a list-mode-based reconstruction for the correction of respiratory motion over the thorax, allowing the use of all data available throughout a respiratory motion average acquisition. The developed algorithm was evaluated using datasets of the NCAT phantom generated at different points throughout the respiratory cycle. List-mode-data-based PET-simulated frames were subsequently produced by combining the NCAT datasets with Monte Carlo simulation. A non-rigid registration algorithm based on B-spline basis functions was employed to derive transformation parameters accounting for the respiratory motion using the NCAT dynamic CT images. The displacement matrices derived were subsequently applied during the image reconstruction of the original emission list mode data. Two different implementations for the incorporation of the elastic transformations within the one-pass list mode EM (OPL-EM) algorithm were developed and evaluated. The corrected images were compared with those produced using an affine transformation of list mode data prior to reconstruction, as well as with uncorrected respiratory motion average images. Results demonstrate that although both correction techniques considered lead to significant improvements in accounting for respiratory motion artefacts in the lung fields, the elastic-transformation-based correction leads to a more uniform improvement across the lungs for different lesion sizes and locations.
},
	Author = {Lamare, F. and Ledesma Carbayo, M. J. and Cresson, T. and Kontaxakis, G. and Santos, A. and Le Rest, C. C. and Reader, A. J. and Visvikis, D.},
	Date-Added = {2015-01-31 09:16:09 +0000},
	Date-Modified = {2015-01-31 09:17:51 +0000},
	Journal = {Physics in Medicine and Biology},
	Number = {17},
	Pages = {5187--5204},
	Title = {List-mode-based reconstruction for respiratory motion correction in {PET} using non-rigid body transformations},
	Volume = {52},
	Year = {2007}}

@article{BrKiPr02,
	Abstract = {During normal breathing, heart motion is about 15 mm along the body axis in humans. We propose a method to track and to correct this motion after a list-mode acquisition which involves the recording of a signal proportional to respiratory volume. We use the dynamic MCAT (DMCAT) chest phantom to simulate 24 temporal frames regularly spaced during the respiratory cycle, for 60 projection angles over 360$\,^{\circ}$. A 15-mm respiratory translation motion is simulated for the heart, liver and spleen. Thresholding of projections is used to reduce the influence of static activity on calculation of the axial center-of-mass (aCOM). Variation in the impact of attenuation as a function of projections and noise in the low-count projections rebinned from list-mode acquisitions is seen to limit ones ability to track respiratory motion using the aCOM. By including the recording of a signal proportional to the relative respiratory volume with the list-mode acquisition counts from different respiratory cycles can be combined to produce projections with common respiratory volumes. We have determined that the aCOMs determined from summing these common-volume based projections over the anterior to left-anterior oblique projection angles can be used to track respiratory motion as a function of the volume signal. Using this information on the variation of the aCOM as a function of the volume signal, the entire list-mode acquisition can then be rebinned into a projection set which is corrected for respiratory motion. After motion tracking, the mean absolute difference between the true motion curve and the aCOM curve is 0.10 cm for noisy studies. After correction no heart motion is visible on a cine display of projections. The polar map of myocardial MIBI uptake after motion correction is closer to that obtained when no respiratory motion is present than without correction.

},
	Author = {Bruyant, P. P. and King, M. A. and Pretorius, P. H.},
	Date-Added = {2015-01-31 09:13:56 +0000},
	Date-Modified = {2015-01-31 09:15:26 +0000},
	Journal = {IEEE Transactions on Nuclear Science},
	Number = {5},
	Pages = {2159---2166},
	Title = {Correction of the respiratory motion of the heart by tracking of the center of mass of thresholded projections: a simulation study using the dynamic {MCAT} phantom},
	Volume = {49},
	Year = {2002}}

@article{PoRiJuEn08,
	Abstract = {The image quality in a conventional positron emission tomography (PET)/computed tomography (CT) scanner is degraded by respiratory motion because of erroneous attenuation correction when three-dimensional image acquisition is used. To overcome this problem, time-resolved data acquisition (4D) is required. For this, a Siemens Biograph 16 PET/CT scanner has been modified and its normal capability has been extended to a true 4D-PET/4D-CT imaging device including phase-correlated attenuation correction. To verify the correct functionality of this device, experiments on a respiratory motion phantom that allowed movement in two dimensions have been performed. The measurements showed good spatial correlation as well as good time synchronization between the PET and CT data. Furthermore, the motion pattern of the phantom and the shape of the activity distribution have been examined, and the volume of the reconstructed PET images has been analyzed. The results demonstrate the feasibility of such a procedure, and we therefore recommend that 4D-PET data should be reconstructed using 4D-CT data, which can be acquired on the same machine.

},
	Author = {P{\"o}nisch, F. and Richter, C. and Just, U. and Enghardt, W.},
	Date-Added = {2015-01-31 09:10:25 +0000},
	Date-Modified = {2015-02-02 14:13:47 +0100},
	Journal = {Physics in Medicine and Biology},
	Number = {13},
	Pages = {N259--N268},
	Title = {Attenuation correction of four dimensional {(4D) PET} using phase-correlated {4D}-computed tomography},
	Volume = {53},
	Year = {2008}}

@article{LiStBlScBa05,
	Abstract = {High-resolution cardiac PET imaging with emphasis on quantification would benefit from eliminating the problem of respiratory movement during data acquisition. Respiratory gating on the basis of list-mode data has been employed previously as one approach to reduce motion effects. However, it results in poor count statistics with degradation of image quality. This work reports on the implementation of a technique to correct for respiratory motion in the area of the heart at no extra cost for count statistics and with the potential to maintain ECG gating, based on rigid-body transformations on list-mode data event-by-event. A motion-corrected data set is obtained by assigning, after pre-correction for detector efficiency and photon attenuation, individual lines-of-response to new detector pairs with consideration of respiratory motion. Parameters of respiratory motion are obtained from a series of gated image sets by means of image registration. Respiration is recorded simultaneously with the list-mode data using an inductive respiration monitor with an elasticized belt at chest level. The accuracy of the technique was assessed with point-source data showing a good correlation between measured and true transformations. The technique was applied on phantom data with simulated respiratory motion, showing successful recovery of tracer distribution and contrast on the motion-corrected images, and on patient data with C15O and 18FDG. Quantitative assessment of preliminary C15O patient data showed improvement in the recovery coefficient at the centre of the left ventricle.

},
	Author = {Livieratos, L. and Stegger, L. and Bloomfield, P. M. and Sch{\"a}fers, K. P. and Bailey, D. L. and Camici, P. G.},
	Date-Added = {2015-01-31 09:06:50 +0000},
	Date-Modified = {2015-02-06 15:52:29 +0000},
	Journal = {Physics in Medicine and Biology},
	Number = {14},
	Pages = {3313--3322},
	Title = {Rigid-body transformation of list-mode projection data for respiratory motion correction in cardiac {PET}},
	Volume = {50},
	Year = {2005}}

@conference{PaChLeal11,
	Abstract = {The goal is to generate and evaluate a simulated 4D Rb-82 PET dataset that realistically models simultaneous respiratory and cardiac motions for use to study the effects of the motions and their compensation using various gating schemes. Normal cardiac and respiratory (C&R) motions were simulated separately using the realistic 4D XCAT phantoms. The C&R motion cycles were divided into 24 and 48 equally-spaced time points, respectively. The simultaneous dual motions were modeled by 24×48 phantoms with different combinations of C&R motion phases. Almost noise-free projections of the heart, blood pool, lungs, liver, stomach, spleen, and the remaining body were simulated separately using the combined SimSET and GATE Monte Carlo simulation program which is 12 times faster than GATE alone. The projections were scaled and combined to simulate a typical Rb-82 myocardial perfusion (MP) PET patient study. The no gating, 6-frame respiratory gating only, 8-frame cardiac gating only, and simultaneous 6-frame respiratory and 8-frame cardiac gating schemes were applied. Each gated projection dataset was reconstructed using a 2D OS-EM without and with attenuation correction (AC) using an averaged and gated attenuation maps. The reconstructed images were evaluated in terms of artifactual non-uniformity in the MP polar map. Significant artifactual non-uniformity was found in the MP polar map over all gating scheme without AC. With AC, the artifactual decreases in both the anterior and inferior regions were reduced with respiratory gating. Cardiac motion alone did not cause significant artifactual non-uniformity. In addition, the combination of dual gating and AC using the gated attenuation map provided the most uniform MP polar map. We demonstrated the flexibility and utility of the 4D XCAT phantom set with simultaneous C&R motions. It is a powerful tool to study motion effects on MP PET studies and to evaluate C&R gating schemes, AC and quantitative - D PET image reconstruction methods.

},
	Author = {Park, M. J. and Chen, S. and Lee, T. S. and Fung, G. S. K. and Lodge, M. and Tsui, B. M. W.},
	Booktitle = {IEEE Nuclear Science Symposium and Medical Imaging Conference (NSS/MIC). Valencia, Spain, October 23--29, 2011},
	Date-Added = {2015-01-31 09:01:17 +0000},
	Date-Modified = {2015-01-31 09:02:54 +0000},
	Pages = {3327--3330},
	Title = {Generation and evaluation of a simultaneous cardiac and respiratory gated {Rb-82} {PET} simulation},
	Year = {2011}}

@conference{RaTaAyal10,
	Abstract = {Routine clinical myocardial perfusion (MP) PET imaging involves the use of cardiac gating only. Nonetheless, respiratory motion of the heart can considerably degrade the quality of MP images and the quantitative accuracy of myocardial uptake estimates. We first performed a quantitative evaluation of the degrading contributions of cardiac (C) and respiratory (R) motion, as well as non-motion factors of Rb-82 positron range, photon non-collinearity, crystal scattering and penetration. For a normal human simulated phantom, we showed that the combination of all above factors resulted in -48% underestimation of myocardial activity, while corrections for all non-motion factors resulted in 21%, 36% and 41% underestimated myocardial activities in the presence of C, R and C&R motion. This means that compensation for respiratory motion must be considered as critical towards achieving overall motion compensation and/or resolution modeling. To achieve respiratory motion compensation, we used translation motion vectors to first match respiratory-only gated images to the end-expiration reference frame. Next, for each cardiac gate, a 4D EM reconstruction algorithm was applied to the R-gated data within that cardiac phase. Three techniques were compared involving reconstructions of (a) a single R-gate only, and all R-gates (b) without and (c) with respiratory motion correction (MC). Using simulated PET data, quantitative comparisons of noise vs. bias trace-off curves indicated notable improvements for the proposed 4D respiratory MC method. Using CHO analysis as applied to the task of perfusion defect detection, ROC analysis of the three methods resulted in AUC values of 0.610$\pm$0.039, 0.645$\pm$0.038 and 0.821$\pm$0.029. The CLABROC statistical test revealed that the proposed MC technique significantly outperformed the other two methods in the task of defect detection.

},
	Author = {Rahmim, A. and Tang, J. and Ay, M. R. and Bengel, F. M.},
	Booktitle = {IEEE Nuclear Science Symposium Conference Record. Knoxville, TN, 30 October - 6 November, 2010},
	Date-Added = {2015-01-31 08:58:22 +0000},
	Date-Modified = {2015-01-31 09:00:07 +0000},
	Pages = {3312---3316},
	Title = {{4D} respiratory motion-corrected {Rb-82} myocardial perfusion {PET} imaging},
	Year = {2010}}

@article{Ta14,
	Abstract = {The constant motion of the beating heart presents an obstacle to clear optical imaging, especially 3D imaging, in small animals where direct optical imaging would otherwise be possible. Gating techniques exploit the periodic motion of the heart to computationally "freeze" this movement and overcome motion artifacts. Optically gated imaging represents a recent development of this, where image analysis is used to synchronize acquisition with the heartbeat in a completely non-invasive manner. This article will explain the concept of optical gating, discuss a range of different implementation strategies and their strengths and weaknesses. Finally we will illustrate the usefulness of the technique by discussing applications where optical gating has facilitated novel biological findings by allowing 3D in vivo imaging of cardiac myocytes in their natural environment of the beating heart.
},
	Author = {Taylor, J. M.},
	Date-Added = {2015-01-31 08:53:47 +0000},
	Date-Modified = {2015-01-31 08:56:09 +0000},
	Journal = {Frontiers in Physiology},
	Number = {5},
	Pages = {481},
	Title = {Optically gated beating-heart imaging},
	Volume = {11},
	Year = {2014}}

@article{LiRaStScBa06,
	Abstract = {PURPOSE:
Respiratory motion has been identified as a source of artefacts in most medical imaging modalities. This paper reports on respiratory gating as a means to eliminate motion-related inaccuracies in PET imaging.
METHODS:
Respiratory gating was implemented in list mode with physiological signal recorded every millisecond together with the PET data. Respiration was monitored with an inductive respiration monitor using an elasticised belt around the patient's chest. Simultaneous ECG gating can be maintained independently by encoding ECG trigger signal into the list-mode data. Respiratory gating is performed in an off-line workstation with gating parameters defined retrospectively. The technique was applied on a preliminary set of patient data with C(15)O.
RESULTS:
Motion was visually observed in the cine displays of the sagittal and coronal views of the reconstructed respiratory gated images. Significant changes in the cranial-caudal position of the heart could be observed. The centroid of the cardiac blood pool showed an excursion of 4.5-16.5 mm (mean 8.5+/-4.8 mm) in the cranial-caudal direction, with more limited excursion of 1.1-7.0 mm (mean 2.5+/-2.2 mm) in the horizontal direction and 1.3-3.7 mm (mean 2.4+/-0.9 mm) in the vertical direction.
CONCLUSION:
These preliminary data show that the extent of motion involved in respiration is comparable to myocardial wall thickness, and respiratory gating may be considered in order to reduce this effect in the reconstructed images.
},
	Author = {Livieratos, L. and Rajappan, K. and Stegger, L. and Sch{\"a}fers, K. P. and Bailey, D. L. and Camici, P. G.},
	Date-Added = {2015-01-31 08:50:05 +0000},
	Date-Modified = {2015-02-06 15:52:02 +0000},
	Journal = {European Journal of Nuclear Medicine and Molecular Imaging},
	Number = {5},
	Pages = {584---588},
	Title = {Respiratory gating of cardiac {PET} data in list-mode acquisition},
	Volume = {33},
	Year = {2006}}

@incollection{BeLaZa08,
	Abstract = {Several iterative methods are available for solving the ill-posed problem of image reconstruction. They are motivated by different approaches and may derive from methods used for the solution of linear equations or the minimization of suitable functionals. In this paper we adopt the approach flowing from maximum likelihood to Bayesian formulation of image reconstruction and providing a generalization of the classical regularization theory. This approach leads to the minimization of functionals derived from properties of the noise and, possibly, from additional information on the solution. We investigate a class of scaled gradient methods, based on a suitable decomposition of the gradient, and we show that this class contains some of the methods used for the solution of maximum likelihood problems in image reconstruction. We also obtain very simple regularized versions of these methods. Constraints of non-negativity and flux conservation are taken into account by considering scaled gradient projection (SGP) methods, derived from the previous approach, and for them a convergence proof can be given. Numerical experience on a particular problem shows that SGP can provide a considerable increase in efficiency with respect to the standard algorithm used for that problem. Work is in progress in order to understand whether a similar gain can be achieved in other cases.},
	Author = {Bertero, M. and Lanteri, H. and Zanni, L.},
	Booktitle = {Mathematical Methods in Biomedical Imaging and Intensity-Modulated Radiation Therapy (IMRT)},
	Date-Added = {2015-01-31 08:47:05 +0000},
	Date-Modified = {2015-01-31 08:47:13 +0000},
	Editor = {Censor, Y. and Jiang, M. and Louis, A. K.},
	Number = {7},
	Pages = {37--63},
	Publisher = {Springer Verlag},
	Series = {Publications of the Scuola Normale Superiore: CRM series},
	Title = {Iterative image reconstruction: a point of view},
	Year = {2008}}

@article{SaGoScAaSc11,
	Abstract = {Medical investigations targeting a quantitative analysis of the position emission tomography (PET) images require the incorporation of additional knowledge about the photon attenuation distribution in the patient. Today, energy range adapted attenuation maps derived from computer tomography (CT) scans are used to effectively compensate for image quality degrading effects, such as attenuation and scatter. Replacing CT by magnetic resonance (MR) is considered as the next evolutionary step in the field of hybrid imaging systems. However, unlike CT, MR does not measure the photon attenuation and thus does not provide an easy access to this valuable information. Hence, many research groups currently investigate different technologies for MR-based attenuation correction (MR-AC). Typically, these approaches are based on techniques such as special acquisition sequences (alone or in combination with subsequent image processing), anatomical atlas registration, or pattern recognition techniques using a data base of MR and corresponding CT images. We propose a generic iterative reconstruction approach to simultaneously estimate the local tracer concentration and the attenuation distribution using the segmented MR image as anatomical reference. Instead of applying predefined attenuation values to specific anatomical regions or tissue types, the gamma attenuation at 511 keV is determined from the PET emission data. In particular, our approach uses a maximum-likelihood estimation for the activity and a gradient-ascent based algorithm for the attenuation distribution. The adverse effects of scattered and accidental gamma coincidences on the quantitative accuracy of PET, as well as artifacts caused by the inherent crosstalk between activity and attenuation estimation are efficiently reduced using enhanced decay event localization provided by time-of-flight PET, accurate correction for accidental coincidences, and a reduced number of unknown attenuation coefficients. First results achieved with measured whole body PET data and reference segmentation from CT showed an absolute mean difference of 0.005 cm⁻¹ (< 20%) in the lungs, 0.0009 cm⁻¹ (< 2%) in case of fat, and 0.0015 cm⁻¹ (< 2%) for muscles and blood. The proposed method indicates a robust and reliable alternative to other MR-AC approaches targeting patient specific quantitative analysis in time-of-flight PET/MR.
},
	Author = {Salomon, A. and Goedicke, A. and Schweizer, B. and Aach, T. and Schulz, V.},
	Date-Added = {2015-01-31 07:56:28 +0000},
	Date-Modified = {2015-01-31 07:57:29 +0000},
	Journal = {IEEE Transactions on Medical Imaging},
	Number = {3},
	Pages = {804--813},
	Title = {Simultaneous reconstruction of activity and attenuation for {PET}/{MR}},
	Volume = {30},
	Year = {2011}}

@article{WaKaMoHe13,
	Abstract = {In current combined PET/MR systems, PET attenuation correction is based on MRI, since the small bore inside MRI systems and the strong magnetic field do not permit a rotating PET transmission source or a CT device to be integrated. Unlike CT measurements in PET/CT scanners, the MR signal is not directly correlated to tissue density and thus cannot be converted by a simple transformation of intensity values. Various approaches have been developed based on templates, atlas information, direct segmentation of T1-weighted MR images, or segmentation of images from special MR sequences. The advantages and disadvantages of these approaches as well as additional challenges will be discussed in this review.

},
	Author = {Wagenknecht, G. and Kaiser, H.-J. and Mottaghy, F. M. and Herzog, H.},
	Date-Added = {2015-01-31 07:53:56 +0000},
	Date-Modified = {2015-01-31 07:55:10 +0000},
	Journal = {MAGMA},
	Number = {1},
	Pages = {99--113},
	Title = {{MRI} for attenuation correction in {PET}: methods and challenges},
	Volume = {26},
	Year = {2013}}

@article{BeStMaStWe15,
	Abstract = {Measurements of spatial and temporal resolution for ECG-gated scanning of a stationary and moving heart phantom with a 16-row MDCT were performed. A resolution phantom with cylindrical holes from 0.4 to 3.0 mm diameter was mounted to a cardiac phantom, which simulates the motion of a beating heart. Data acquisition was performed with 16×0.75 mm at various heart rates (HR, 60--120 bpm), pitches (0.15--0.30) and scanner rotation times (RT, 0.42 and 0.50 s). Raw data were reconstructed using a multi-cycle real cone-beam reconstruction algorithm at multiple phases of the RR interval. Multi-planar reformations (MPR) were generated and analyzed. Temporal resolution and cardiac cycles used for image reconstruction were calculated. In 97.2% (243/250) of data obtained with the stationary phantom, the complete row of holes with 0.6 mm was visible. These results were independent of heart rate, pitch, scanner rotation time and phase point of reconstruction. For the dynamic phantom, spatial resolution was determined during phases of minimal motion (116/250). In 40.5% (47/116), the resolution was 0.6 mm and in 37.1% (43/116) 0.7 mm. Temporal resolution varied between 63 and 205 ms, using 1.5--4.37 cardiac cycles for image reconstruction.
},
	Author = {Begemann, P. G. and van Stevendaal, U. and Manzke, R. and Stork, A. and Weiss, F. and Nolte-Ernsting, C. and Grass, M. and Adam, G.},
	Date-Added = {2015-01-31 07:51:45 +0000},
	Date-Modified = {2015-01-31 07:53:17 +0000},
	Journal = {European Radiology},
	Number = {5},
	Pages = {1015--1026},
	Title = {Evaluation of spatial and temporal resolution for {ECG}-gated 16-row multidetector {CT} using a dynamic cardiac phantom},
	Volume = {15},
	Year = {2005}}

@article{DeClRyDeVa05,
	Abstract = {Various automatic algorithms are now being developed to calculate left ventricular (LV) and right ventricular (RV) ejection fraction from tomographic radionuclide ventriculography. We tested the performance of 4 of these algorithms in estimating LV and RV volume and ejection fraction using a dynamic 4-chamber cardiac phantom.
METHODS:
We developed a realistic physical, dynamic 4-chamber cardiac phantom and acquired 25 tomographic radionuclide ventriculography images within a wide range of end-diastolic volumes, end-systolic volumes, and stroke volumes. We assessed the ability of 4 algorithms (QBS, QUBE, 4D-MSPECT, and BP-SPECT) to calculate LV and RV volume and ejection fraction.
RESULTS:
For the left ventricle, the correlations between reference and estimated volumes (0.93, 0.93, 0.96, and 0.93 for QBS, QUBE, 4D-MSPECT, and BP-SPECT, respectively; all with P < 0.001) and ejection fractions (0.90, 0.93, 0.88, and 0.92, respectively; all with P < 0.001) were good, although all algorithms underestimated the volumes (mean difference [+/-2 SDs] from Bland-Altman analysis: -39.83 +/- 43.12 mL, -33.39 +/- 38.12 mL, -33.29 +/- 40.70 mL, and -16.61 +/- 39.64 mL, respectively). The underestimation by QBS, QUBE, and 4D-MSPECT was greater for higher volumes. QBS, QUBE, and BP-SPECT could also be tested for the right ventricle. Correlations were good for the volumes (0.93, 0.95, and 0.97 for QBS, QUBE, and BP-SPECT, respectively; all with P < 0.001). In terms of absolute volume estimation, the mean differences (+/-2 SDs) from Bland-Altman analysis were -41.28 +/- 43.66 mL, 11.13 +/- 49.26 mL, and -13.11 +/- 28.20 mL, respectively. Calculation of RV ejection fraction correlated well with true values (0.84, 0.92, and 0.94, respectively; all with P < 0.001), although an overestimation was seen for higher ejection fractions.
CONCLUSION:
Calculation of LV and RV ejection fraction based on tomographic radionuclide ventriculography was accurate for all tested algorithms. All algorithms underestimated LV volume; estimation of RV volume seemed more difficult, with different results for each algorithm. The more irregular shape and inclusion of a relatively hypokinetic RV outflow tract in the right ventricle seemed to cause the greater difficulty with delineation of the right ventricle, compared with the left ventricle.},
	Author = {De Bondt, P. and Claessens, T. and Rys, B. and De Winter, O. and Vandenberghe, S. and Segers, P. and Verdonck, P. and Dierckx, R. A.},
	Date-Added = {2015-01-31 07:49:51 +0000},
	Date-Modified = {2015-01-31 07:51:05 +0000},
	Journal = {Journal of Nuclear Medicine},
	Number = {1},
	Pages = {165--171},
	Title = {Accuracy of 4 different algorithms for the analysis of tomographic radionuclide ventriculography using a physical, dynamic 4-chamber cardiac phantom},
	Volume = {46},
	Year = {2005}}

@article{FiStBaAnGu05,
	Abstract = {Respiratory motion continues to present challenges in the delivery of radiation therapy to tumors in the thorax and abdomen by causing movement of structures within those areas. Several approaches to account for this movement in the planning and delivery of treatment have been developed over the past several years. To assist in the development and assessment of various techniques for respiration-correlated radiation therapy, a platform capable of programmable irregular longitudinal motion has been designed and fabricated to simulate intrafractional respiratory motion. A sliding platform and the base on which it was mounted were constructed from polycarbonate plastic, and a stepper motor provided platform motion. Respiratory motion data, either artificially generated on a spreadsheet or extracted from respiratory monitoring files, were converted to a format appropriate for driving the stepper motor. Various phantoms were placed on top of the platform and used in studies related to respiration-correlated radiation therapy. Several applications of the platform were demonstrated, such as improving the quality of acquisition of time-dependent computed tomography image datasets, comparing various methods of acquiring such datasets, and implementing feedback-guided breath hold treatment delivery procedures. This study showed that a platform capable of programmable irregular motion is a useful tool for the development and assessment of procedures related to the effects of respiratory motion in radiation therapy.
},
	Author = {Fitzpatrick, M. J. and Starkschall, G. and Balter, P. and Antolak, J. A. and Guerrero, T. and Nelson, C. and Keall, P. and Mohan, R.},
	Date-Added = {2015-01-31 07:45:06 +0000},
	Date-Modified = {2015-01-31 07:46:52 +0000},
	Journal = {Journal of Applied Clinical Medical Physics},
	Number = {1},
	Pages = {13---21},
	Title = {A novel platform simulating irregular motion to enhance assessment of respiration-correlated radiation therapy procedures},
	Volume = {6},
	Year = {2005}}

@article{KaHuKeal07,
	Abstract = {The purpose of this study was to investigate the feasibility of a simple deformable phantom as a QA tool for testing and validation of deformable image registration algorithms. A diagnostic thoracic imaging phantom with a deformable foam insert was used in this study. Small plastic markers were distributed through the foam to create a lattice with a measurable deformation as the ground truth data for all comparisons. The foam was compressed in the superior-inferior direction using a one-dimensional drive stage pushing a flat "diaphragm" to create deformations similar to those from inhale and exhale states. Images were acquired at different compressions of the foam and the location of every marker was manually identified on each image volume to establish a known deformation field with a known accuracy. The markers were removed digitally from corresponding images prior to registration. Different image registration algorithms were tested using this method. Repeat measurement of marker positions showed an accuracy of better than 1 mm in identification of the reference marks. Testing the method on several image registration algorithms showed that the system is capable of evaluating errors quantitatively. This phantom is able to quantitatively assess the accuracy of deformable image registration, using a measure of accuracy that is independent of the signals that drive the deformation parameters.
},
	Author = {Kashani, R. and Hub, M. and Kessler, M. L. and Balter, J. M.},
	Date-Added = {2015-01-31 07:43:26 +0000},
	Date-Modified = {2015-01-31 07:44:36 +0000},
	Journal = {Medical Physics},
	Number = {7},
	Pages = {2785---2788},
	Title = {Technical note: a physical phantom for assessment of accuracy of deformable alignment algorithms},
	Volume = {34},
	Year = {2007}}

@article{VeSeWeal06,
	Abstract = {The four-dimensional (4-D) NURBS-based cardiac-torso (NCAT) phantom, which provides a realistic model of the normal human anatomy and cardiac and respiratory motions, is used in medical imaging research to evaluate and improve imaging devices and techniques, especially dynamic cardiac applications. One limitation of the phantom is that it lacks the ability to accurately simulate altered functions of the heart that result from cardiac pathologies such as coronary artery disease (CAD). The goal of this work was to enhance the 4-D NCAT phantom by incorporating a physiologically based, finite-element (FE) mechanical model of the left ventricle (LV) to simulate both normal and abnormal cardiac motions. The geometry of the FE mechanical model was based on gated high-resolution X-ray multislice computed tomography (MSCT) data of a healthy male subject. The myocardial wall was represented as a transversely isotropic hyperelastic material, with the fiber angle varying from -90deg at the epicardial surface, through 0deg at the midwall, to 90deg at the endocardial surface. A time-varying elastance model was used to simulate fiber contraction, and physiological intraventricular systolic pressure-time curves were applied to simulate the cardiac motion over the entire cardiac cycle. To demonstrate the ability of the FE mechanical model to accurately simulate the normal cardiac motion as well as the abnormal motions indicative of CAD, a normal case and two pathologic cases were simulated and analyzed. In the first pathologic model, a subendocardial anterior ischemic region was defined. A second model was created with a transmural ischemic region defined in the same location. The FE-based deformations were incorporated into the 4-D NCAT cardiac model through the control points that define the cardiac structures in the phantom which were set to move according to the predictions of the mechanical model. A simulation study was performed using the FE-NCAT combination to investigate how- - the differences in contractile function between the subendocardial and transmural infarcts manifest themselves in myocardial Single photon emission computed tomography (SPECT) images. The normal FE model produced strain distributions that were consistent with those reported in the literature and a motion consistent with that defined in the normal 4-D NCAT beating heart model based on tagged magnetic resonance imaging (MRI) data. The addition of a subendocardial ischemic region changed the average transmural circumferential strain from a contractile value of -0.09 to a tensile value of 0.02. The addition of a transmural ischemic region changed average circumferential strain to a value of 0.13, which is consistent with data reported in the literature. Model results demonstrated differences in contractile function between subendocardial and transmural infarcts and how these differences in function are documented in simulated myocardial SPECT images produced using the 4-D NCAT phantom. Compared with the original NCAT beating heart model, the FE mechanical model produced a more accurate simulation for the cardiac motion abnormalities. Such a model, when incorporated into the 4-D NCAT phantom, has great potential for use in cardiac imaging research. With its enhanced physiologically based cardiac model, the 4-D NCAT phantom can be used to simulate realistic, predictive imaging data of a patient population with varying whole-body anatomy and with varying healthy and diseased states of the heart that will provide a known truth from which to evaluate and improve existing and emerging 4-D imaging techniques used in the diagnosis of cardiac disease

},
	Author = {Veress, A. I. and Segars, W. P. and Weiss, J. A. and Gullberg, G. T.},
	Date-Added = {2015-01-31 07:38:10 +0000},
	Date-Modified = {2015-01-31 07:39:35 +0000},
	Journal = {IEEE Transactions on Medical Imaging},
	Number = {12},
	Pages = {1604---1616},
	Title = {Normal and pathological {NCAT} image and phantom data based on physiologically realistic left ventricle finite-element models},
	Volume = {25},
	Year = {2006}}

@article{VeSeTsGu11,
	Abstract = {The 4D extended cardiac-torso (XCAT) phantom was developed to provide a realistic and flexible model of the human anatomy and cardiac and respiratory motions for use in medical imaging research. A prior limitation to the phantom was that it did not accurately simulate altered functions of the heart that result from cardiac pathologies such as coronary artery disease (CAD). We overcame this limitation in a previous study by combining the phantom with a finite-element (FE) mechanical model of the left ventricle (LV) capable of more realistically simulating regional defects caused by ischemia. In the present work, we extend this model giving it the ability to accurately simulate motion abnormalities caused by myocardial infarction (MI), a far more complex situation in terms of altered mechanics compared with the modeling of acute ischemia. The FE model geometry is based on high resolution CT images of a normal male subject. An anterior region was defined as infarcted and the material properties and fiber distribution were altered, according to the bio-physiological properties of two types of infarction, i.e., fibrous and remodeled infarction (30% thinner wall than fibrous case). Compared with the original, surface-based 4D beating heart model of the XCAT, where regional abnormalities are modeled by simply scaling down the motion in those regions, the FE model was found to provide a more accurate representation of the abnormal motion of the LV due to the effects of fibrous infarction as well as depicting the motion of remodeled infarction. In particular, the FE models allow for the accurate depiction of dyskinetic motion. The average circumferential strain results were found to be consistent with measured dyskinetic experimental results. Combined with the 4D XCAT phantom, the FE model can be used to produce realistic multimodality sets of imaging data from a variety of patients in which the normal or abnormal cardiac function is accurately represented.
},
	Author = {Veress, A. I. and Segars, W. P. and Tsui, B. M. W. and Gullberg, G. T.},
	Date-Added = {2015-01-31 07:36:23 +0000},
	Date-Modified = {2015-01-31 21:08:46 +0000},
	Journal = {IEEE Transactions on Medical Imaging},
	Number = {4},
	Pages = {915--927},
	Title = {Incorporation of a left ventricle finite element model defining infarction into the {XCAT} imaging phantom},
	Volume = {30},
	Year = {2011}}

@article{Pag04,
	Abstract = {Dynamic radiation therapy techniques require that the treatment head setup varies in time and space during dose delivery. Examples are leaf motion in intensity modulated x-ray therapy, modulator wheel rotation in conventional proton therapy and variable magnet settings in proton beam scanning. Consequently, for Monte Carlo dose calculation the results of several independent simulations usually have to be combined. Depending on the complexity and the required accuracy this can become cumbersome. We present a technique to simulate time-dependent geometries within a single four-dimensional Monte Carlo simulation using the GEANT4 Monte Carlo package. Results for proton therapy applications are shown.

},
	Author = {Paganetti, H.},
	Date-Added = {2015-01-31 07:32:35 +0000},
	Date-Modified = {2015-01-31 07:33:46 +0000},
	Journal = {Physics in Medicine and Biology},
	Number = {6},
	Pages = {N75---N81},
	Title = {Four-dimensional Monte Carlo simulation of time-dependent geometries},
	Volume = {49},
	Year = {2004}}

@article{ZaXu07,
	Abstract = {The widespread availability of high-performance computing and popularity of simulations stimulated the development of computational anthropomorphic models of the human anatomy for medical imaging modalities and dosimetry calculations. The widespread interest in molecular imaging spurred the development of more realistic three- to five-dimensional computational models based on the actual anatomy and physiology of individual humans and small animals. These can be defined by either mathematical (analytical) functions or digital (voxel-based) volume arrays (or a combination of both), thus allowing the simulation of medical imaging data that are ever closer to actual patient data. The paradigm shift away from the stylized human models is imminent with the development of more than 30 voxel-based tomographic models in recent years based on anatomical medical images. We review the fundamental and technical challenges of designing computational models of the human anatomy, and focus particularly on the latest developments and future directions of their application in the simulation of radiological imaging systems and dosimetry calculations.
},
	Author = {Zaidi, H. and Xu, X. G.},
	Date-Added = {2015-01-31 07:30:56 +0000},
	Date-Modified = {2015-01-31 07:32:18 +0000},
	Journal = {Annual Review of Biomedical Engineering},
	Number = {1},
	Pages = {471---500},
	Title = {Computational anthropomorphic models of the human anatomy: the path to realistic Monte Carlo modeling in medical imaging},
	Volume = {9},
	Year = {2007}}

@article{DaBuJiSc08,
	Abstract = {The problem of motion is well known in positron emission tomography (PET) studies. The PET images are formed over an elongated period of time. As the patients cannot hold breath during the PET acquisition, spatial blurring and motion artifacts are the natural result. These may lead to wrong quantification of the radioactive uptake. We present a solution to this problem by respiratory-gating the PET data and correcting the PET images for motion with optical flow algorithms. The algorithm is based on the combined local and global optical flow algorithm with modifications to allow for discontinuity preservation across organ boundaries and for application to 3-D volume sets. The superiority of the algorithm over previous work is demonstrated on software phantom and real patient data.
},
	Author = {Dawood, M. and Buther, F. and Jiang, X. and Sch{\"a}fers, K. P.},
	Date-Added = {2015-01-31 07:25:25 +0000},
	Date-Modified = {2015-02-06 15:51:10 +0000},
	Journal = {IEEE Transactions on Medical Imaging},
	Number = {8},
	Pages = {1164--1175},
	Title = {Respiratory motion correction in {3-D PET} data with advanced optical flow algorithms},
	Volume = {27},
	Year = {2008}}

@article{KoKiLeJoKi13,
	Abstract = {BACKGROUND: 
We investigated the effect of the respiratory motion on attenuation-corrected (AC) SPECT images for three different SPECT systems, each using a different approach in obtaining attenuation maps: scanning-line sources (SLS) acquired simultaneous with emission; slow cone-beam CT (CBCT) acquired sequentially to emission; and fast helical CT (HCT) acquired sequentially to emission.
METHODS:
A torso phantom filled with (99m)Tc was used to model a cardiac perfusion study. Stationary baseline acquisitions were followed by the acquisitions with the phantom moving axially using a computer-controlled motion platform to simulate breathing. In addition, HCT acquisitions were made simulating breath-hold at different extents of misalignment between CT and emission. HCT images were also used to simulate the Average-CT method. Acquisitions were repeated with added breast attachments, and the heart insert in two different orientations. Visual comparison was made of AC maps, AC emission slices and polar maps. Quantitative comparisons were made of global uniformity based on the percent fractional standard deviation (%FSD) of the polar map segment values, and the ratio of the segment values in the Anterior and Inferior walls divided by that of the Lateral and Septal walls (AI/LS ratio).
RESULTS:
The AC maps for the SLS were inferior to the CT's, and most impacted by added large breast attachment. Motion artifacts seen on CBCT slices were minimized in the derived attenuation maps. AC maps obtained from HCT showed inconsistent organ sizes depending on the direction of respiration at the time of acquisition. Both visually and quantitatively CBCT resulted in the best uniformity (up to 3.4 % lower in %FSD) for all the stationary acquisitions, and for the motion acquisition of the female phantom with large breast attachment (up to 4.0 % lower). For the motion acquisition of the male phantoms, HCT resulted in slightly better uniformity (<0.5 % lower) than CBCT. Breath-hold at end-expiration slightly improved (up to 1.1 %) the uniformity over the HCT acquired during regular breathing. Further improvement was achieved with the Average-CT method. For all the systems, phantom respiratory motion reduced the AI/LS ratio compared to when the phantoms were stationary.
CONCLUSIONS:
The CBCT approach resulted in the best uniformity of the AC emission images. For the female phantom with larger breast attachment, HCT and SLS were truncated at some projection angles introducing artifacts into the AC emission images. The emission image artifacts observed with HCT could be mitigated by performing breath-hold acquisition at end-expiration or Average-CT type acquisitions.},
	Author = {K{\"o}nik, A. and Kikut, J. and Lew, R. and Johnson, K. and King, M. A.},
	Date-Added = {2015-01-31 07:21:35 +0000},
	Date-Modified = {2015-01-31 07:23:17 +0000},
	Journal = {Journal of Nuclear Cardiology},
	Number = {6},
	Pages = {1093--1107},
	Title = {Comparison of methods of acquiring attenuation maps for cardiac {SPECT} in the presence of respiratory motion},
	Volume = {20},
	Year = {2013}}

@article{GrYaKiJi06,
	Abstract = {In this paper, we investigate the benefits of a spatiotemporal approach for reconstruction of image sequences. In the proposed approach, we introduce a temporal prior in the form of motion compensation to account for the statistical correlations among the frames in a sequence, and reconstruct all the frames collectively as a single function of space and time. The reconstruction algorithm is derived based on the maximum a posteriori estimate, for which the one-step late expectation-maximization algorithm is used. We demonstrated the method in our experiments using simulated single photon emission computed tomography (SPECT) cardiac perfusion images. The four-dimensional (4D) gated mathematical cardiac-torso phantom was used for simulation of gated SPECT perfusion imaging with Tc-99m-sestamibi. In addition to bias-variance analysis and time activity curves, we also used a channelized Hotelling observer to evaluate the detectability of perfusion defects in the reconstructed images. Our experimental results demonstrated that the incorporation of temporal regularization into image reconstruction could significantly improve the accuracy of cardiac images without causing any significant cross-frame blurring that may arise from the cardiac motion. This could lead to not only improved detection of perfusion defects, but also improved reconstruction of the heart wall which is important for functional assessment of the myocardium.
},
	Author = {Gravier, E. and Yang, Y. and King, M. A. and Jin, M.},
	Date-Added = {2015-01-31 07:19:44 +0000},
	Date-Modified = {2015-01-31 07:24:20 +0000},
	Journal = {Physics in Medicine and Biology},
	Number = {18},
	Pages = {4603---4619.},
	Title = {Fully {4D} motion-compensated reconstruction of cardiac {SPECT} images},
	Volume = {51},
	Year = {2006}}

@article{CaGiMaJa03,
	Abstract = {The primary goal of this work was to develop and evaluate a new method for simultaneous three-dimensional motion estimation and image reconstruction for gated cardiac emission computed tomography (ECT). The method employs a two-step iterative procedure for obtaining the motion and reconstructed image estimates. The method was evaluated using both simulated and physical phantoms designed to mimic myocardial perfusion imaging in ECT. In both the simulated and physical phantom studies, the images reconstructed by the simultaneous method showed improved noise characteristics relative to a standard iterative algorithm. The percent error of the motion estimate ranged from 16% to 59% for the simulated phantom study, and 45% to 65% for the physical phantom study, depending on the position within the myocardium.

},
	Author = {Cao, Z. and Gilland, D. R. and Mair, B. A. and Jaszczak, R. J.},
	Date-Added = {2015-01-31 07:17:06 +0000},
	Date-Modified = {2015-01-31 07:19:06 +0000},
	Journal = {IEEE Transactions on Nuclear Science},
	Number = {3},
	Pages = {384--388},
	Title = {Three-dimensional motion estimation with image reconstruction for gated cardiac {ECT}},
	Volume = {50},
	Year = {2003}}

@article{MaGiSu06,
	Abstract = {In this paper, we propose and test a new iterative algorithm to simultaneously estimate the nonrigid motion vector fields and the emission images for a complete cardiac cycle in gated cardiac emission tomography. We model the myocardium as an elastic material whose motion does not generate large amounts of strain. As a result, our method is based on minimizing an objective function consisting of the negative logarithm of a maximum likelihood image reconstruction term, the standard biomechanical model of strain energy, and an image matching term that ensures a measure of agreement of intensities between frames. Simulations are obtained using data for the four-dimensional (4-D) NCAT phantom. The data models realistic noise levels in a typical gated myocardial perfusion SPECT study. We show that our simultaneous algorithm produces images with improved spatial resolution characteristics and noise properties compared with those obtained from postsmoothed 4-D maximum likelihood methods. The simulations also demonstrate improved motion estimates over motion estimation using independently reconstructed images

},
	Author = {Mair, B. A. and Gilland, D. R. and Sun, J.},
	Date-Added = {2015-01-31 07:15:06 +0000},
	Date-Modified = {2015-01-31 07:16:16 +0000},
	Journal = {IEEE Transactions on Medical Imaging},
	Number = {9},
	Pages = {1130--1144},
	Title = {Estimation of images and nonrigid deformations in gated emission {CT}},
	Volume = {25},
	Year = {2006}}

@article{ChFe13,
	Abstract = {Motion-compensated image reconstruction (MCIR) methods incorporate motion models to improve image quality in the presence of motion. MCIR methods differ in terms of how they use motion information and they have been well studied separately. However, there have been less theoretical comparisions of different MCIR methods. This paper compares the theoretical noise properties of three popular MCIR methods assuming known nonrigid motion. We show the relationship among three MCIR methods-motion-compensated temporal regularization (MTR), the parametric motion model (PMM), and post-reconstruction motion correction (PMC)-for penalized weighted least square cases. These analyses show that PMM and MTR are matrix-weighted sums of all registered image frames, while PMC is a scalar-weighted sum. We further investigate the noise properties of MCIR methods with Poisson models and quadratic regularizers by deriving accurate and fast variance prediction formulas using an "analytical approach." These theoretical noise analyses show that the variances of PMM and MTR are lower than or comparable to the variance of PMC due to the statistical weighting. These analyses also facilitate comparisons of the noise properties of different MCIR methods, including the effects of different quadratic regularizers, the influence of the motion through its Jacobian determinant, and the effect of assuming that total activity is preserved. Two-dimensional positron emission tomography simulations demonstrate the theoretical results.
},
	Author = {Chun, S. Y. and Fessler, J. A.},
	Date-Added = {2015-01-31 07:13:13 +0000},
	Date-Modified = {2015-01-31 07:14:16 +0000},
	Journal = {IEEE Transactions on Medical Imaging},
	Number = {2},
	Pages = {141--152},
	Title = {Noise properties of motion-compensated tomographic image reconstruction methods},
	Volume = {32},
	Year = {2013}}

@article{KlHu02,
	Abstract = {A four-dimensional deformable motion algorithm is described for use in the motion compensation of gated cardiac positron emission tomography. The algorithm makes use of temporal continuity and a non-uniform elastic material model to provide improved estimates of heart motion between time frames. Temporal continuity is utilized in two ways. First, incremental motion fields between adjacent time frames are calculated to improve estimation of long-range motion between distant time frames. Second, a consistency criterion is used to insure that the image match between distant time frames is consistent with the deformations used to match adjacent time frames. The consistency requirement augments the algorithm's ability to estimate motion between noisy time frames, and the concatenated incremental motion fields improve estimation for large deformations. The estimated motion fields are used to establish a voxel correspondence between volumes and to produce a motion-compensated composite volume.

},
	Author = {Klein, G. J. and Huesman, R. H.},
	Date-Added = {2015-01-31 07:11:33 +0000},
	Date-Modified = {2015-01-31 07:12:43 +0000},
	Journal = {Medical Image Analysis},
	Number = {1},
	Pages = {29---46},
	Title = {Four-dimensional processing of deformable cardiac {PET} data},
	Volume = {6},
	Year = {2002}}

@article{ZhSyLeSo95,
	Abstract = {We present a new method for computing the internal displacement fields associated with permanent deformations of 3D composite objects with complex internal structure for fields satisfying the small displacement gradient approximation of continuum mechanics. We compute the displacement fields from a sequence of 3D X-ray computed tomography (CT) images. By assuming that the intensity of the tomographic images represents a conserved property which is incompressible, we develop a constrained nonlinear regression model for estimation of the displacement field. Successive linear approximation is then employed and each linear subsidiary problem is solved using variational calculus. We approximate the resulting Euler-Lagrange equations using a finite set of linear equations using finite differencing methods. We solve these equations using a conjugate gradient algorithm in a multiresolution framework. We validate our method using pairs of synthetic images of plane shear flow. Finally, we determine the 3D displacement field in the interior of a cylindrical asphalt/aggregate core loaded to a state of permanent deformation.

},
	Author = {Zhou, Z. Y. and Synolakis, C. E. and Leahy, R. M. and Song, S. M.},
	Date-Added = {2015-01-31 07:07:48 +0000},
	Date-Modified = {2015-01-31 07:10:50 +0000},
	Journal = {Proceeding of the Royal Society of London A: Mathematical and Physical Sciences},
	Number = {1937},
	Pages = {537---554},
	Title = {Calculation of {3D} internal displacement-fields from {3D} X-ray computer tomographic-images},
	Volume = {449},
	Year = {1995}}

@article{SoLe91,
	Abstract = {A method of computing the three-dimensional (3-D) velocity field from 3-D cine computer tomographs (CTs) of a beating heart is proposed. Using continuum theory, the authors develop two constraints on the 3-D velocity field generated by a beating heart. With these constraints, the computation of the 3-D velocity field is formulated as an optimization problem and a solution to the optimization problem is developed using the Euler-Lagrange method. The solution is then discretized for computer implementation. The authors present the results for both simulated images and clinical cine CT images of a beating heart.
},
	Author = {Song, S. M. and Leahy, R. M.},
	Date-Added = {2015-01-31 07:05:40 +0000},
	Date-Modified = {2015-01-31 07:06:58 +0000},
	Journal = {IEEE Transactions on Medical Imaging},
	Number = {3},
	Pages = {295---306},
	Title = {Computation of {3-D} velocity fields from {3-D} cine {CT} images of a human heart},
	Volume = {10},
	Year = {1991}}

@article{ZhGhBr05,
	Abstract = {Dynamic inverse problems, which occur in medical imaging and other fields, are inverse problems in which the quantities to be reconstructed vary in time, although they are related to the measurements through spatial operators only. Traditional methods solve these problems by frame-by-frame reconstruction, then extract temporal behaviour of the objects or regions of interest through curve fitting and other image-based processing. These approaches solve the inverse problem while exploiting only the spatial relationship between the object and the measurement data at each time instant, without using any temporal dynamics of the underlying process, and thus are not optimal unless the solution is temporally uncorrelated. If the spatial operators are linear, and if one, by contrast, solves the whole spatio-temporal process jointly, it falls into the category of general linear least-squares problems. Such approaches are generally difficult, both due to the challenge of modelling the temporal dynamics appropriately as well as to the high dimensionality of the associated large linear system. Several recent reports have approached this problem in different ways, making different prior assumptions on the spatial and temporal behaviour. In this paper we discuss three such approaches, which have been introduced from different points of view, in a common statistical regularization framework, and illuminate their relationships. The three methods are a state-space model, the separability condition and a multiple constraints model. The key result is that there is a clear relationship among the three methods; specifically, the inverse of the spatio-temporal autocovariance matrix has a block tri-diagonal form, a Kronecker product form or a Kronecker sum form, respectively. Some simple simulation examples are presented to illustrate the theoretical analysis.

},
	Author = {Zhang, Y. and Ghodrati, A. and Brooks, D. H.},
	Date-Added = {2015-01-30 23:17:21 +0000},
	Date-Modified = {2015-01-30 23:20:04 +0000},
	Journal = {Inverse Problems},
	Number = {1},
	Pages = {357--382},
	Title = {An analytical comparison of three spatio-temporal regularization methods for dynamic linear inverse problems in a common statistical framework},
	Volume = {21},
	Year = {2005}}

@article{AmVe93,
	Abstract = {We present a novel approach to study the intricate motion of the heart using the optical flow technique applied to cine MR images. The method uses image brightness variations between consecutive frames to compute in-plane displacements or velocities of moving image features. The dense velocity field thereby obtained throughout the myocardial wall allows to not only characterize segmental left ventricular wall motion but also to resolve "fine" motion such as intramural differences in displacements and therefore in thickening. Other subtle features of systole like the descent of the base towards the apex or the counterclockwise rotation of the apex with respect to the base can also be detected by the algorithm. Contrary to other techniques proposed earlier, this noninvasive method presents the additional advantages of not requiring any special pulse sequence nor well defined endocardial and epicardial outlines.
},
	Author = {Amartur, S. C. and Vesselle, H. J.},
	Date-Added = {2015-01-30 23:08:23 +0000},
	Date-Modified = {2015-01-30 23:09:34 +0000},
	Journal = {Magnetic Resonance in Medicine},
	Number = {1},
	Pages = {59---67},
	Title = {A new approach to study cardiac motion: the optical flow of cine {MR} images},
	Volume = {29},
	Year = {1993}}

@article{MaBlBePe87,
	Abstract = {In two-dimensional echocardiography the study of the motion of the heart, especially of the left ventricle, is of central interest. Although the clinician can see this motion on real-time two-dimensional echocardiograms, its quantification can still be greatly improved. This paper presents and tests a computer method to quantify the motion of the heart from digitized image sequences. This method computes on every point of an image the two-dimensional velocity vector which characterizes its motion from this image to the next. This approach has the following advantages: 1) border recognition algorithms are no longer needed, 2) motion is not restricted to its radial component, and 3) motion information is available on every point of the image.

},
	Author = {Mailloux, G. E. and Bleau, A. and Bertrand, M. and Petitclerc, R.},
	Date-Added = {2015-01-30 23:06:11 +0000},
	Date-Modified = {2015-01-30 23:10:10 +0000},
	Journal = {IEEE Transactions on Biomedical Engineering},
	Number = {5},
	Pages = {356---364},
	Title = {Computer analysis of heart motion from two-dimensional echocardiograms},
	Volume = {34},
	Year = {1987}}

@article{HoSc81,
	Abstract = {Optical flow cannot be computed locally, since only one independent measurement is available from the image sequence at a point, while the flow velocity has two components. A second constraint is needed. A method for finding the optical flow pattern is presented which assumes that the apparent velocity of the brightness pattern varies smoothly almost everywhere in the image. An iterative implementation is shown which successfully computes the optical flow for a number of synthetic image sequences. The algorithm is robust in that it can handle image sequences that are quantized rather coarsely in space and time. It is also insensitive to quantization of brightness levels and additive noise. Examples are included where the assumption of smoothness is violated at singular points or along lines in the image.
},
	Author = {Horn, B. K. and Schunck, B. G.},
	Date-Added = {2015-01-30 23:03:20 +0000},
	Date-Modified = {2015-01-30 23:04:24 +0000},
	Journal = {Artificial Intelligence},
	Number = {1--3},
	Pages = {185---203},
	Title = {Determining optical flow},
	Volume = {17},
	Year = {1981}}

@article{OsMcPr00,
	Abstract = {This paper describes a new image processing technique for rapid analysis and visualization of tagged cardiac magnetic resonance (MR) images. The method is based on the use of isolated spectral peaks in spatial modulation of magnetization (SPAMM)-tagged magnetic resonance images. We call the calculated angle of the complex image corresponding to one of these peaks a harmonic phase (HARP) image and show that HARP images can be used to synthesize conventional tag lines, reconstruct displacement fields for small motions, and calculate two-dimensional (2-D) strain. The performance of this new approach is demonstrated using both real and simulated tagged MR images. Potential for use of HARP images in fast imaging techniques and three-dimensional (3-D) analyses are discussed.},
	Author = {Osman, N. F. and McVeigh, E. R. and Prince, J. L.},
	Date-Added = {2015-01-30 14:00:26 +0100},
	Date-Modified = {2015-01-30 14:01:40 +0100},
	Journal = {IEEE Transactions on Medical Imaging},
	Number = {3},
	Pages = {186---202},
	Title = {Imaging heart motion using harmonic phase {MRI}},
	Volume = {19},
	Year = {2000}}

@article{OzMc00,
	Abstract = {In MRI tagging, magnetic tags-spatially encoded magnetic saturation planes-are created within tissues acting as temporary markers. Their deformation pattern provides useful qualitative and quantitative information about the functional properties of underlying tissue and allows non-invasive analysis of mechanical function. The measured displacement at a given tag point contains only unidirectional information; in order to track the full 3D motion, these data have to be combined with information from other orthogonal tag sets over all time frames. Here, we provide a method to describe the motion of the heart using a four-dimensional tensor product of B-splines. In vivo validation of this tracking algorithm is performed using different tagging sets on the same heart. Using the validation results, the appropriate control point density was determined for normal cardiac motion tracking. Since our motion fields are parametric and based on an image plane based Cartesian coordinate system, trajectories or other derived values (velocity, acceleration, strains ...) can be calculated for any desired point within the volume spanned by the control points. This method does not rely on specific chamber geometry, so the motion of any tagged structure can be tracked. Examples of displacement and strain analysis for both ventricles are also presented.},
	Author = {{\"O}zt{\"u}rk, C. and McVeigh, E. R.},
	Date-Added = {2015-01-30 13:56:01 +0100},
	Date-Modified = {2015-01-30 13:57:21 +0100},
	Journal = {Physics in Medicine and Biology},
	Number = {6},
	Pages = {1683--1702},
	Title = {Four-dimensional B-spline based motion analysis of tagged {MR} images: introduction and in vivo validation},
	Volume = {45},
	Year = {2000}}

@article{PaMeYo96,
	Abstract = {We present a new method for analyzing the motion of the heart's left ventricle (LV) from tagged magnetic resonance imaging (MRI) data. Our technique is based on the development o f a new class of physics-based deformable models whose parameters arefunctions. They allow the definition of new parameterized primitives and parameterized deformations which can capture the local shape variation of a complex object. Furthermore, these parameters are intuitive and require no complex post-processing in order to be used hy a physician. Using a physics-based approach, we convert the geometric models into dynamic models that deform due to forces exerted from the datapoints and conform Lo the given dataset. We present experiments involving the extraction of the shape and motion of the LV's mid-wall during systole from tagged MRI data based on a few parameter functions. Furthermore, by plotting the variations over time of the extracted LV model parameters from normal and abnormal heart data along the long axis, we are able to quantitatively characterize their differences.},
	Author = {Park, J. and Metaxas, D. and Young, A. A.},
	Date-Added = {2015-01-30 13:37:24 +0100},
	Date-Modified = {2015-02-02 13:28:40 +0100},
	Journal = {IEEE Transactions on Medical Imaging},
	Number = {3},
	Pages = {278---289},
	Title = {Deformable models with parameter functions for cardiac motion analysis from tagged {MRI} data},
	Volume = {15},
	Year = {1996}}

@article{YoKrDo95,
	Abstract = {Magnetic resonance tissue tagging allows noninvasive in vivo measurement of soft tissue deformation. Planes of magnetic saturation are created, orthogonal to the imaging plane, which form dark lines (stripes) in the image. The authors describe a method for tracking stripe motion in the image plane, and show how this information can be incorporated into a finite element model of the underlying deformation. Human heart data were acquired from several imaging planes in different orientations and were combined using a deformable model of the left ventricle wall. Each tracked stripe point provided information on displacement orthogonal to the original tagging plane, i.e., a one-dimensional (1-D) constraint on the motion. Three-dimensional (3-D) motion and deformation was then reconstructed by fitting the model to the data constraints by linear least squares. The average root mean squared (rms) error between tracked stripe points and predicted model locations was 0.47 mm (n=3,100 points). In order to validate this method and quantify the errors involved, the authors applied it to images of a silicone gel phantom subjected to a known, well-controlled, 3-D deformation. The finite element strains obtained were compared to an analytic model of the deformation known to be accurate in the central axial plane of the phantom. The average rms errors were 6% in both the reconstructed shear strains and 16% in the reconstructed radial normal strain.},
	Author = {Young, A. A. and Kraitchman, D. L. and Dougherty, L. and Axel, L.},
	Date-Added = {2015-01-30 13:34:44 +0100},
	Date-Modified = {2015-01-30 13:35:55 +0100},
	Journal = {IEEE Transactions on Medical Imaging},
	Number = {3},
	Pages = {413---421},
	Title = {Tracking and finite-element analysis of stripe deformation in magnetic-resonance tagging},
	Volume = {14},
	Year = {1995}}

@article{RaRoZa07,
	Abstract = {With the arrival of increasingly higher-resolution PET systems, small amounts of motion can cause significant blurring in the resulting images compared with the intrinsic resolution of the PET scanner. The authors review advanced correction methods for unwanted patient motion and for motion due to cardiac and respiratory cycles. A general theme in motion correction methods is the use of increasingly sophisticated software to make use of existing advanced hardware. In this sense, the field is open to future novel ideas (hardware and especially software) aimed at improving motion detection, characterization, and compensation.},
	Author = {Rahmim, A. and Rousset, O. G. and Zaidi, H.},
	Date-Added = {2015-01-30 13:31:09 +0100},
	Date-Modified = {2015-01-30 13:32:21 +0100},
	Journal = {PET Clinics},
	Pages = {251--266},
	Title = {Strategies for motion tracking and correction in {PET}},
	Volume = {2},
	Year = {2007}}

@article{NeEr08,
	Abstract = {The development of positron emission tomography/computed tomography (PET/CT) scanners has allowed not only straightforward but also synergistic fusion of anatomical and functional information. Combined PET/CT imaging yields an increased sensitivity and specificity beyond that which either of the 2 modalities possesses separately and therefore provides improved diagnostic accuracy. Because attenuation correction in PET is performed with the use of CT images, with CT used in the localization of disease, accurate spatial registration of PET and CT image sets is required. Correcting for the spatial mismatch caused by respiratory motion represents a particular challenge for the requisite registration accuracy as a result of differences in temporal resolution between the 2 modalities. This review provides a brief summary of the materials, methods, and results involved in multiple investigations of the correction for respiratory motion in PET/CT imaging of the thorax, with the goal of improving image quality and quantitation. Although some schemes use respiratory-phase data selection to exclude motion artifacts, others have adopted sophisticated software techniques. The various image artifacts associated with breathing motion are also described.},
	Author = {Nehmeh, S. A. and Erdi, Y. E.},
	Date-Added = {2015-01-30 13:29:59 +0100},
	Date-Modified = {2015-01-30 13:30:56 +0100},
	Journal = {Seminars in Nuclear Medicine},
	Number = {3},
	Pages = {167---176},
	Title = {Respiratory motion in positron emission tomography/computed tomography: a review},
	Volume = {38},
	Year = {2008}}

@article{QiPaClMa06,
	Abstract = {Cardiac and respiratory motion artefacts in PET imaging have been traditionally resolved by acquiring the data in gated mode. However, gated PET images are usually characterized by high noise content due to their low photon statistics. In this paper, we present a novel 4D model for the PET imaging system, which can incorporate motion information to generate a motion-free image with all acquired data. A computer simulation and a phantom study were conducted to test the performance of this approach. The computer simulation was based on a digital phantom that was continuously scaled during data acquisition. The phantom study, on the other hand, used two spheres in a tank of water, all of which were filled with 18F water. One of the spheres was stationary while the other moved in a sinusoidal fashion to simulate tumour motion in the thorax. Data were acquired using both 4D CT and gated PET. Motion information was derived from the 4D CT images and then used in the 4D PET model. Both studies showed that this 4D PET model had a good motion-compensating capability. In the phantom study, this approach reduced quantification error of the radioactivity concentration by 95% when compared to a corresponding static acquisition, while signal-to-noise ratio was improved by 210% when compared to a corresponding gated image.
},
	Author = {Qiao, F. and Pan, T. and Clark, J. W. and Mawlawi, O. R.},
	Date-Added = {2015-01-30 13:14:48 +0100},
	Date-Modified = {2015-01-30 13:16:01 +0100},
	Journal = {Physics in Medicine and Biology},
	Pages = {3769--3783},
	Title = {A motion-incorporated reconstruction method for gated {PET} studies},
	Volume = {51},
	Year = {2006}}

@inproceedings{QiHu02,
	Abstract = {Motion artifacts can be a significant factor that limits the image quality in high-resolution PET. Surveillance systems have been developed to track the movements of the subject during a scan. Development of reconstruction algorithms that are able to compensate for the subject motion will increase the potential of PET. In this paper we present a list mode likelihood reconstruction algorithm with the ability of motion compensation. The subject motion is explicitly modeled in the likelihood function. The detections of each detector pair are modeled as a Poisson process with time-varying rate function. The proposed method has several advantages over the existing methods. It uses all detected events and does not introduce any interpolation error. Computer simulations show that the proposed method can compensate simulated subject movements and that the reconstructed images have no visible motion artifacts.
},
	Author = {Qi, J. and Huesman, R. H.},
	Booktitle = {Proceedings. 2002 IEEE International Symposium on Biomedical Imaging, 2002.},
	Date-Added = {2015-01-30 13:12:35 +0100},
	Date-Modified = {2015-01-30 13:14:17 +0100},
	Pages = {413--416},
	Title = {List mode reconstruction for {PET} with motion compensation: {A} simulation study},
	Year = {2002}}

@article{FuMeEbPfCo02,
	Abstract = {Methods capable of correcting for head motion in all six degrees of freedom have been proposed for positron emission tomography (PET) brain imaging but not yet demonstrated in human studies. These methods rely on the accurate measurement of head motion in relation to the reconstruction coordinate frame. We present methodology for the direct calibration of an optical motion-tracking system to the reconstruction coordinate frame using paired coordinate measurements obtained simultaneously from a PET scanner and tracking system. We also describe the implementation of motion correction, based on the multiple acquisition frame method originally described by Picard and Thompson (1997), using data provided by the motion tracking system. Effective compensation for multiple six-degree-of-freedom movements is demonstrated in dynamic PET scans of the Hoffman brain phantom and a normal volunteer. We conclude that reduced distortion and improved quantitative accuracy can be achieved with this method in PET brain studies degraded by head movements
},
	Author = {Fulton, R. R. and Meikle, S.R. and Eberl, S. and Pfeiffer, J. and Constable, C. J.},
	Date-Added = {2015-01-30 13:10:14 +0100},
	Date-Modified = {2015-01-30 13:11:59 +0100},
	Journal = {IEEE Transactions on Nuclear Science},
	Number = {1},
	Pages = {116--123},
	Title = {Correction for head movements in positron emission tomography using an optical motion-tracking system},
	Volume = {49},
	Year = {2002}}

@article{BuJuWiKoHo04,
	Abstract = {A method is presented to correct positron emission tomography (PET) data for head motion during data acquisition. The method is based on simultaneous acquisition of PET data in list mode and monitoring of the patient's head movements with a motion tracking system. According to the measured head motion, the line of response (LOR) of each single detected PET event is spatially transformed, resulting in a spatially fully corrected data set. The basic algorithm for spatial transformation of LORs is based on a number of assumptions which can lead to spatial artifacts and quantitative inaccuracies in the resulting images. These deficiencies are discussed, demonstrated and methods for improvement are presented. Using different kinds of phantoms the validity and accuracy of the correction method is tested and its applicability to human studies is demonstrated as well.},
	Author = {B{\"u}hler, P. and Just, U. and Will, E. and Kotzerke, J. and van den Hoff, J.},
	Date-Added = {2015-01-30 13:08:22 +0100},
	Date-Modified = {2015-01-30 13:09:48 +0100},
	Journal = {IEEE Transactions on Medical Imaging},
	Number = {9},
	Pages = {1176--1185},
	Title = {An accurate method for correction of head movement in {PET}},
	Volume = {23},
	Year = {2004}}

@article{RaTaZa13,
	Author = {Rahmim, A. and Tang, J. and Zaidi, H.},
	Date-Added = {2015-01-30 12:57:07 +0100},
	Date-Modified = {2015-01-30 12:58:18 +0100},
	Journal = {PET Clinics},
	Number = {1},
	Pages = {51--67},
	Title = {Four-Dimensional Image Reconstruction Strategies in Cardiac-Gated and Respiratory- Gated {PET} Imaging},
	Volume = {8},
	Year = {2013}}

@article{ReVe14,
	Abstract = {An overview of the theory of 4D image reconstruction for emission tomography is given along with a review of the current state of the art, covering both positron emission tomography and single photon emission computed tomography (SPECT). By viewing 4D image reconstruction as a matter of either linear or non-linear parameter estimation for a set of spatiotemporal functions chosen to approximately represent the radiotracer distribution, the areas of so-called 'fully 4D' image reconstruction and 'direct kinetic parameter estimation' are unified within a common framework. Many choices of linear and non-linear parameterization of these functions are considered (including the important case where the parameters have direct biological meaning), along with a review of the algorithms which are able to estimate these often non-linear parameters from emission tomography data. The other crucial components to image reconstruction (the objective function, the system model and the raw data format) are also covered, but in less detail due to the relatively straightforward extension from their corresponding components in conventional 3D image reconstruction. The key unifying concept is that maximum likelihood or maximum a posteriori (MAP) estimation of either linear or non-linear model parameters can be achieved in image space after carrying out a conventional expectation maximization (EM) update of the dynamic image series, using a Kullback-Leibler distance metric (comparing the modeled image values with the EM image values), to optimize the desired parameters. For MAP, an image-space penalty for regularization purposes is required. The benefits of 4D and direct reconstruction reported in the literature are reviewed, and furthermore demonstrated with simple simulation examples. It is clear that the future of reconstructing dynamic or functional emission tomography images, which often exhibit high levels of spatially correlated noise, should ideally exploit these 4D approaches.},
	Author = {Reader, A. J. and Verhaeghe, J.},
	Date-Added = {2015-01-30 12:44:19 +0100},
	Date-Modified = {2015-01-30 12:45:21 +0100},
	Journal = {Physics in Medicine and Biology},
	Number = {22},
	Pages = {R371--R418},
	Title = {{4D} image reconstruction for emission tomography},
	Volume = {59},
	Year = {2014}}

@article{OuLiEl13,
	Abstract = {PET image quality is limited by patient motion. Emission data are blurred due to cardiac and/or respiratory motion. Although spatial resolution is 4 mm for standard clinical whole-body PET scanners, the effective resolution can be a low as 1 cm due to motion. Additionally, the deformation of attenuation medium causes image artifacts. Previously, gating is used to ``freeze'' the motion, but leads to significantly increased noise level. Simultaneous PET-MR modality offers a new way to perform PET motion correction. MR can be used to measure 3D motion fields, which can then be incorporated into the iterative PET reconstruction to obtain motion corrected PET images. In this report, we present MR imaging techniques to acquire dynamic images, a non-rigid image registration algorithm to extract motion fields from acquired MR images, and a PET reconstruction algorithm with motion correction. We also present results from both phantom and in-vivo animal PET-MR studies. We demonstrate that MR-based PET motion correction using simultaneous PET-MR improves image quality and lesion detectability compared to gating and to no motion correction.},
	Author = {Ouyang, J. and Li, Q. and El Fakhri, G.},
	Date-Added = {2015-01-30 11:59:56 +0100},
	Date-Modified = {2015-01-30 12:01:01 +0100},
	Journal = {Seminars in Nuclear Medicine},
	Number = {1},
	Pages = {60--67},
	Title = {{MR}-based Motion Correction for {PET} Imaging},
	Volume = {43},
	Year = {2013}}

@techreport{AlDuKu14,
	Abstract = {In this paper we introduce a diffeomorphic constraint on the deformations considered in the deformable Bayesian Mixed Effect (BME) Template model. Our approach is built on a generic group of diffeomorphisms, which is parametrized by an arbitrary set of control point positions and momentum vectors. This enables to estimate the optimal positions of control points together with a template image and parameters of the deformation distribution which compose the atlas. We propose to use a stochastic version of the Expectation-Maximization (EM) algorithm where the simulation is performed using the Anisotropic Metropolis Adjusted Langevin Algorithm (AMALA). We propose also an extension of the model including a sparsity constraint to select an optimal number of control points with relevant positions. Experiments are carried out on the USPS database and on mandibles of mice.},
	Author = {Allassonni{\`e}re, S. and Durrleman, S. and Kuhn, E.},
	Date-Added = {2015-01-30 11:26:58 +0100},
	Date-Modified = {2015-01-30 11:28:49 +0100},
	Institution = {UR341 Math{\'e}matiques et Informatique Appliqu{\'e}es, INRA},
	Number = {R2014-2},
	Title = {Bayesian mixed effect atlas estimation with a diffeomorphic deformation model},
	Type = {Research report},
	Year = {2014}}

@book{Yo10,
	Abstract = {Shapes are complex objects, which are difficult to apprehend as mathematical entities, in ways that can also be amenable to computerized analysis and interpretation. This volume provides the background that is required for this purpose, including different approaches that can be used to model shapes, and algorithms that are available to analyze them. It explores, in particular, the interesting connections between shapes and the objects that naturally act on them, diffeomorphisms. The book is, as far as possible, self-contained, with an appendix that describes a series of classical topics in mathematics (Hilbert spaces, differential equations, Riemannian manifolds) and sections that represent the state of the art in the analysis of shapes and their deformations.

A direct application of what is presented in the book is a branch of the computerized analysis of medical images, called computational anatomy.},
	Author = {Younes, L.},
	Date-Added = {2015-01-30 11:13:06 +0100},
	Date-Modified = {2015-01-30 11:13:24 +0100},
	Publisher = {Springer-Verlag},
	Series = {Applied Mathematical Sciences},
	Title = {Shapes and Diffeomorphisms},
	Volume = {171},
	Year = {2010}}

@incollection{FaShElFa14,
	Abstract = {Shapes describe objects in terms of information invariant to scale, translation and rotation. Depending of the data source, shapes may be represented by object contours or representation/transformations that sustain the objects characteristics, such as the signed distance function. Biomedical objects have inherent plasticity due to movement and changes over time. Elastic registration is a fundamental image analysis step for tracking anatomical structures, diseases, progress of treatment and in image-guided interventions. Variational level set methods (LSM) represent objects' contours through an implicit function that enables tracking the objects' topologies. This chapter provides an overview of variational shape modeling as applied to the registration and segmentation problems. The chapter evaluates similarity/dissimilarity measures and common energy functional representations used in elastic shape registration. Common numerical methods to solve the optimization involved are studied. In addition, the chapter discusses clinical applications for which shape-based models enable robust performance with respect to occlusion and other image degradation.},
	Author = {Farag, A. A. and Shalaby, A. and El Munim, H. A. and Farag, A.},
	Booktitle = {Shape Analysis in Medical Image Analysis},
	Chapter = {95--122},
	Date-Added = {2015-01-30 11:10:31 +0100},
	Date-Modified = {2015-01-30 11:11:34 +0100},
	Editor = {Li, S. and Tavares, J. M. T. S.},
	Publisher = {Springer-Verlag},
	Series = {Lecture Notes in Computational Vision and Biomechanics},
	Title = {Variational Shape Representation for Modeling, Elastic Registration and Segmentation},
	Volume = {14},
	Year = {2014}}

@incollection{WeWaZaPe14,
	Abstract = {The recognition and segmentation of organs and anatomical structures in medical images is the basis for an efficient workflow and quantitative measurements in diagnostic and interventional applications. Numerous methods have been developed in the past for specific applications, and many of them are based on variants and extensions of active contours or active shape methods. We present an overview over shape-constrained deformable models that have specifically been developed for organ segmentation in 3D medical images. They rely on a pre-defined shape space like active shape models, but preserve some flexibility of active contour approaches as they allow deviations from the shape space. In particular, we describe our approach to shape parametrization and the concept of ``Simulated Search'' that we use to train boundary detection. Fully automatic segmentation is achieved by a segmentation chain comprising a localization step based on the Generalized Hough Transformation and subsequent model adaptation with increasing degrees of freedom. Finally, we show how shape-constrained deformable models allow to address clinical applications in cardiology and neurology.},
	Author = {Weese, J. and W{\"a}chter-Stehle,I. and Zagorchev, L. and Peters, J.},
	Booktitle = {Shape Analysis in Medical Image Analysis},
	Chapter = {151--184},
	Date-Added = {2015-01-30 11:08:43 +0100},
	Date-Modified = {2015-01-30 11:09:37 +0100},
	Editor = {Li, S. and Tavares, J. M. T. S.},
	Publisher = {Springer-Verlag},
	Series = {Lecture Notes in Computational Vision and Biomechanics},
	Title = {Shape-Constrained Deformable Models and Applications in Medical Imaging},
	Volume = {14},
	Year = {2014}}

@incollection{GaSh14,
	Abstract = {Molecular imaging is a new research discipline enabling the visualization, characterization and quantification of biologic processes taking place at the cellular and subcellular levels within intact living subjects. Applications of molecular imaging techniques will benefit various clinical practices including classification and tracking of chemotherapy and treatment planning of radiotherapy, as well as drug discovery and development. Molecular imaging typically includes two or three dimensional imaging with quantification over time, and is often applied on molecular imaging modalities, such as Positron Emission Tomography (PET), Single Photon Emission Computed Tomography (SPECT) etc. Image series acquired with spatiotemporal distribution of molecular biomarkers must be carefully analyzed to estimate the underlying physiology-related metabolic parameters. Shape analysis is one of the most powerful tools to analyze the geometrical properties from similar shapes or different groups, and can be applied to estimate both the concentration of biomarkers and interaction between biomarkers and tissue/organs. However, some limitations from molecular imaging modalities and clinical practices still hinder the quantitative accuracy of shape analysis, e.g. the low spatial and temporal resolution in PET scan, the inaccuracy of blood samplings from patients, the low Signal-to-Noise (SNR) ratio of measurement data in dynamic PET/CT scan. In this chapter, firstly, we will introduce the definition of molecular imaging, the clinical advantages and limitations of various molecular imaging modalities, secondly, we will review the challenges in data analysis based on the data processing procedure, and explain how data corrections affect the accuracy of static and dynamic PET imaging, thirdly, the general frameworks of image processing in PET and SPECT are reviewed with focus on image reconstruction, at last, we will show some recent advancements and give examples of clinical applications.},
	Author = {Gao, F. and Shi, P.},
	Booktitle = {Shape Analysis in Medical Image Analysis},
	Chapter = {51--93},
	Date-Added = {2015-01-30 11:05:29 +0100},
	Date-Modified = {2015-01-30 11:08:37 +0100},
	Editor = {Li, S. and Tavares, J. M. T. S.},
	Publisher = {Springer-Verlag},
	Series = {Lecture Notes in Computational Vision and Biomechanics},
	Title = {Shape Analysis in Molecular Imaging},
	Volume = {14},
	Year = {2014}}

@book{LiTa14,
	Abstract = {This book contains thirteen contributions from invited experts of international recognition addressing important issues in shape analysis in medical image analysis, including techniques for image segmentation, registration, modelling and classification, and applications in biology, as well as in cardiac, brain, spine, chest, lung and clinical practice.

This volume treats topics such as, anatomic and functional shape representation and matching; shape-based medical image segmentation; shape registration; statistical shape analysis; shape deformation; shape-based abnormity detection; shape tracking and longitudinal shape analysis; machine learning for shape modeling and analysis; shape-based computer-aided-diagnosis; shape-based medical navigation; benchmark and validation of shape representation, analysis and modeling algorithms.

This work will be of interest to researchers, students, and manufacturers in the fields of artificial intelligence, bioengineering, biomechanics, computational mechanics, computational vision, computer sciences, human motion, mathematics, medical imaging, medicine, pattern recognition and physics.},
	Date-Added = {2015-01-30 11:02:59 +0100},
	Date-Modified = {2015-01-30 11:04:14 +0100},
	Editor = {Li, S. and Tavares, J. M. T. S.},
	Publisher = {Springer-Verlag},
	Series = {Lecture Notes in Computational Vision and Biomechanics},
	Title = {Shape Analysis in Medical Image Analysis},
	Volume = {14},
	Year = {2014}}

@book{Pa04,
	Author = {Palamodov, V.},
	Date-Added = {2015-01-30 06:24:58 +0000},
	Date-Modified = {2015-01-30 06:25:28 +0000},
	Publisher = {Springer-Verlag},
	Series = {Monographs in Mathematics},
	Title = {Reconstructive Integral Geometry},
	Volume = {98},
	Year = {2004}}

@article{WaYeYu07,
	Author = {Wang, D. and Ye, Y. and Yu, H.},
	Date-Added = {2015-01-30 06:22:20 +0000},
	Date-Modified = {2015-01-30 06:23:18 +0000},
	Journal = {Physics in Medicine and Biology},
	Number = {6},
	Pages = {R1--R13},
	Title = {Approximate and exact cone-beam reconstruction with standard and non-standard spiral scanning},
	Volume = {52},
	Year = {2007}}

@article{Ka08,
	Abstract = {In this paper we develop local tomography (LT) for image reconstruction from motion contaminated data. It is assumed that motion is known. We propose a new LT function fΛ, which is related to an original object f via an operator \mathcal B : f_\Lambda=\mathcal B f . Because of motion, \mathcal B  may fail to be a pseudo-differential operator (PDO). We obtain the conditions that guarantee that \mathcal B  is a PDO. Under these conditions, similarly to the classical LT in {\bb R}^2, \mathcal B  is a PDO of order 1. Computation of fΛ depends on a weight function Φ. We show that Φ can be chosen in such a way that the operator \mathcal B  has principal symbol |ξ|. This result has an interesting corollary for conventional exact reconstruction. It suggests a novel frequency-split approach to finding f from motion contaminated data. In practice tomographic data are discrete, and derivatives are usually replaced by their mollified analogs. We consider how mollification affects the singularities of the LT function fΛ. Using this approach we develop an algorithm for finding values of jumps of f using LT. We also consider various aspects of numerical implementation of LT and show the results of numerical experiments.},
	Author = {Katsevich, A.},
	Date-Added = {2015-01-30 06:20:54 +0000},
	Date-Modified = {2015-01-30 06:21:36 +0000},
	Journal = {Inverse Problems},
	Number = {4},
	Pages = {045012},
	Title = {Motion compensated local tomography},
	Volume = {24},
	Year = {2008}}

@article{PaXiZoYu04,
	Abstract = {A circular scanning trajectory is and will likely remain a popular choice of trajectory in computed tomography (CT) imaging because it is easy to implement and control. Filtered-backprojection (FBP)-based algorithms have been developed previously for approximate and exact reconstruction of the entire image or a region of interest within the image in circular cone-beam and fan-beam cases. Recently, we have developed a 3D FBP-based algorithm for image reconstruction on PI-line segments in a helical cone-beam scan. In this work, we demonstrated that the 3D FBP-based algorithm indeed provided a rather general formulation for image reconstruction from divergent projections (such as cone-beam and fan-beam projections). On the basis of this formulation we derived new approximate or exact algorithms for image reconstruction in circular cone-beam or fan-beam scans, which can be interpreted as special cases of the helical scan. Existing algorithms corresponding to the derived algorithms were identified. We also performed a preliminary numerical study to verify our theoretical results in each of the cases. The results in the work can readily be generalized to other non-circular trajectories.

},
	Author = {Pan, X. and Xia, D. and Zou, Y. and Yu, L.},
	Date-Added = {2015-01-30 06:18:04 +0000},
	Date-Modified = {2015-01-30 06:19:48 +0000},
	Journal = {Physics in Medicine and Biology},
	Number = {18},
	Pages = {4349--4369},
	Title = {A unified analysis of {FBP}-based algorithms in helical cone-beam and circular cone- and fan-beam scans},
	Volume = {49},
	Year = {2004}}

@book{Ze10,
	Author = {Zeng, L.},
	Date-Added = {2015-01-30 06:16:19 +0000},
	Date-Modified = {2015-01-30 06:16:41 +0000},
	Publisher = {Springer-Verlag},
	Title = {Medical Image Reconstruction. A Conceptual Tutorial},
	Year = {2010}}

@article{YuYeZhWa06,
	Abstract = {Inspired by Katsevich's exact helical cone-beam formula, Palamodov made the first attempt to perform exact image reconstruction from cone-beam data collected along a general scanning trajectory [1]. In contrast to the well-known general exact cone-beam reconstruction schemes formulated by Tuy, Smith, Grangeat and Katsevich, respectively, Palamodov's algorithm was intended to work with truncated data and without the need to specify any weighting function. In this paper, after reformulating Palamodov's formula and comparing it with Katsevich's helical cone-beam formula, we find that Palamodov's algorithm is not theoretically exact due to an inaccurate estimate. Then, we numerically implement it using a planar detector array and simulate the case of cone-beam CT with a nonstandard saddle curve as the scanning trajectory. The reconstruction results suggest that Palamodov's algorithm is an attractive algorithm for approximate cone-beam reconstruction in the case of general cone-beam scanning.

},
	Author = {Yu, H. and Ye, Y. and Zhao, S. and Wang, G.},
	Date-Added = {2015-01-30 06:14:11 +0000},
	Date-Modified = {2015-01-30 06:15:32 +0000},
	Journal = {Inverse Problems},
	Number = {2},
	Pages = {447--460},
	Title = {Studies on {P}alamodov's algorithm for cone-beam {CT} along a general curve},
	Volume = {22},
	Year = {2006}}

@incollection{StMuBe11,
	Author = {Starck, J.-L. and Murtagh, F. and Bertero, M.},
	Booktitle = {Handbook of Mathematical Methods in Imaging},
	Chapter = {34},
	Date-Added = {2015-01-30 06:12:10 +0000},
	Date-Modified = {2015-01-30 06:13:29 +0000},
	Editor = {Scherzer, O.},
	Pages = {1490--1531},
	Publisher = {Springer-Verlag},
	Title = {Starlet Transform in Astronomical Data Processing},
	Year = {2011}}

@article{KaZaSi09,
	Abstract = {We propose an approximate approach to use redundant data outside the 1PI window within the exact Katsevich reconstruction framework. The proposed algorithm allows a flexible selection of the helical pitch, which is useful for clinical applications. Our idea is an extension of the one proposed by K{\"o}hler, Bontus, and Koken (2006). It is based on optimizing the contribution weights of convolution families used in exact Katsevich 3PI algorithms, so that the total weight of each Radon plane is as close to 1 as possible. Optimization is based on solving a least squares problem subject to linear constrains. Numerical evaluation shows good noise and artifact reduction properties of the proposed algorithm.},
	Author = {Katsevich, A. and Zamyatin, A. A. and Silver, M. D.},
	Date-Added = {2015-01-30 06:09:45 +0000},
	Date-Modified = {2015-01-30 06:11:06 +0000},
	Journal = {IEEE Transactions on Medical Imaging},
	Number = {7},
	Pages = {982--990},
	Title = {Optimized Reconstruction Algorithm for Helical {CT} With Fractional Pitch Between {1PI} and {3PI}},
	Volume = {28},
	Year = {2009}}

@inproceedings{ZhWa04,
	Abstract = {In this paper, we perform numerical studies on Feldkamp-type and Katsevich-type algorithms for cone-beam reconstruction with a nonstandard spiral locus to develop an electron-beam micro-CT scanner. Numerical results are obtained using both the approximate and exact algorithms in terms of image quality. It is observed that the two algorithms produce similar quality if the cone angle is not large and/or there is no sharp density change along the z-direction. The Katsevich-type algorithm is generally preferred due to its nature of exactness. Keywords: Computed tomography (CT), cone-beam, nonstandard spiral scanning, Feldkamp-type algorithm, Katsevisch-type algorithm. 
},
	Author = {Zhu, J. and Zhao, S. and Yu, H. and Ye, Y. and Lee, S. W. and Wang, G.},
	Booktitle = {Developments in X-Ray Tomography IV, (26 October 2004)},
	Date-Added = {2015-01-30 06:05:29 +0000},
	Date-Modified = {2015-01-30 06:09:24 +0000},
	Editor = {Bonse, U.},
	Pages = {8 p.},
	Series = {Proceedings of SPIE},
	Title = {Numerical studies on {F}eldkamp-type and {K}atsevich-type algorithms for cone-beam scanning along nonstandard spirals},
	Volume = {5535},
	Year = {2004}}

@article{LuKaZhYuWa10,
	Abstract = {Cardiac computed tomography (CT) has been improved over past years, but it still needs improvement for higher temporal resolution in the cases of high or irregular cardiac rates. Given successful applications of dual-source cardiac CT scanners, triple-source cone-beam CT seems a promising mode for cardiac CT. In this paper, we propose two filtered-backprojection algorithms for triple-source helical cone-beam CT. The first algorithm utilizes two families of filtering lines. These lines are parallel to the tangent of the scanning trajectory and the so-called L lines. The second algorithm utilizes two families of filtering lines tangent to the boundaries of the Zhao window and L lines, respectively, but it eliminates the filtering paths along the tangent of the scanning trajectory, thus reducing the required detector size greatly. The first algorithm is theoretically exact for r < 0.265 R and quasi-exact for 0.265 R <= r < 0.495 R, and the second algorithm is quasi-exact for r < 0.495 R, where r and R denote the object radius and the trajectory radius, respectively. Both algorithms are computationally efficient. Numerical results are presented to verify and showcase the proposed algorithms.

},
	Author = {Lu, Y. and Katsevich, A. and Zhao, J. and Yu, H. and Wang, G.},
	Date-Added = {2015-01-30 06:01:56 +0000},
	Date-Modified = {2015-01-30 06:04:40 +0000},
	Journal = {IEEE Transactions on Medical Imaging},
	Number = {3},
	Pages = {756--770},
	Title = {Fast Exact/Quasi-Exact {FBP} Algorithms for Triple-Source Helical Cone-Beam {CT}},
	Volume = {29},
	Year = {2010}}

@article{Ka03,
	Abstract = {Given a rather general weight function n_0, we derive a new cone beam transform inversion formula. The derivation is explicitly based on Grangeat's formula (1990) and the classical 3D Radon transform inversion. The new formula is theoretically exact and is represented by a 2D integral. We show that if the source trajectory C is complete in the sense of Tuy (1983) (and satisfies two other very mild assumptions), then substituting the simplest weight n_0=1 gives a convolution-based FBP algorithm. However, this easy choice is not always optimal from the point of view of practical applications. The weight n_0=1 works well for closed trajectories, but the resulting algorithm does not solve the long object problem if C is not closed. In the latter case one has to use the flexibility in choosing n_0 and find the weight that gives an inversion formula with the desired properties. We show how this can be done for spiral CT. It turns out that the two inversion algorithms for spiral CT proposed earlier by the author are particular cases of the new formula. For general trajectories the choice of weight should be done on a case-by-case basis.},
	Author = {Katsevich, A.},
	Date-Added = {2015-01-30 05:59:03 +0000},
	Date-Modified = {2015-01-30 06:00:32 +0000},
	Journal = {International Journal of Mathematics and Mathematical Sciences},
	Number = {21},
	Pages = {1305--1321},
	Title = {A general scheme for constructing inversion algorithms for cone beam {CT}},
	Volume = {2003},
	Year = {2003}}

@article{Ka06,
	Abstract = {Proposed are two theoretically exact convolution-based filtered backprojection inversion algorithms for helical computer tomography. An important feature of the algorithms is that they operate in the 3PI mode, which allows use of redundant data for the purpose of improving image quality. Results of a numerical experiment are presented.

},
	Author = {Katsevich, A.},
	Date-Added = {2015-01-30 05:57:40 +0000},
	Date-Modified = {2015-01-30 05:58:35 +0000},
	Journal = {Advances in Applied Mathematics},
	Number = {3},
	Pages = {213--250},
	Title = {3PI algorithms for helical computer tomography},
	Volume = {36},
	Year = {2006}}

@article{BeLaThFeLa08,
	Abstract = {In 1993, Snyder et al investigated the maximum-likelihood (ML) approach to the deconvolution of images acquired by a charge-coupled-device camera and proved that the iterative method proposed by Llacer and Nun ̃ez in 1990 can be derived from the expectation-maximization method of Dempster et al for the solution of ML problems. The utility of the approach was shown on the reconstruction of images of the Hubble space Telescope. This problem deserves further investigation because it can be important in the deconvolution of images of faint objects provided by next-generation ground-based telescopes that will be characterized by large collecting areas and advanced adaptive optics. In this paper, we first prove the existence of solutions of the ML problem by investigating the properties of the negative log of the likelihood function. Next, we show that the iterative method proposed by the above-mentioned authors is a scaled gradient method for the constrained minimization of this function in the closed and convex cone of the non-negative vectors and that, if it is convergent, the limit is a solution of the constrained ML problem. Moreover, by looking for the asymptotic behavior in the regime of high numbers of photons, we find an approximation that, as proved by numerical experiments, works well for any number of photons, thus providing an efficient implementation of the algorithm. In the case of image deconvolution, we also extend the method to take into account boundary effects and multiple images of the same object. The approximation proposed in this paper is tested on a few numerical examples.},
	Author = {Benvenuto, F. and La Camera, A. and Theys, C. and Ferrari, A. and Lant{\'e}ri, H. and Bertero, M.},
	Date-Added = {2015-01-29 15:17:57 +0100},
	Date-Modified = {2015-01-29 15:17:57 +0100},
	Journal = {Inverse Problems},
	Pages = {035016 (20pp)},
	Title = {The study of an iterative method for the reconstruction of images corrupted by {P}oisson and {G}aussian noise},
	Volume = {24},
	Year = {2008}}

@incollection{SaBrMuBu09,
	Address = {New York},
	Author = {Sawatzky, A. and Brune, C. and M{\"u}ller, J. and Burger, M.},
	Booktitle = {Computer Analysis of Images and Patterns: Proceedings of the 13th International Conference, CAIP 2009, M{\"u}nster, Germany, September 2-4, 2009},
	Date-Added = {2015-01-29 15:16:09 +0100},
	Date-Modified = {2015-01-29 15:16:09 +0100},
	Editor = {Jiang, X. and Petkov, N.},
	Pages = {533--540},
	Publisher = {Springer Verlag},
	Series = {Lecture Notes in Computer Science Volume},
	Title = {Total Variation Processing of Images with {P}oisson Statistics},
	Volume = {5702},
	Year = {2009}}

@book{ScGrGrHaLe09,
	Abstract = {This book is devoted to the study of variational methods in imaging. The presentation is mathematically rigorous and covers a detailed treatment of the approach from an inverse problems point of view.

Key Features:

- Introduces variational methods with motivation from the deterministic, geometric, and stochastic point of view

- Bridges the gap between regularization theory in image analysis and in inverse problems

- Presents case examples in imaging to illustrate the use of variational methods e.g. denoising, thermoacoustics, computerized tomography

- Discusses link between non-convex calculus of variations, morphological analysis, and level set methods

- Analyses variational methods containing classical analysis of variational methods, modern analysis such as G-norm properties, and non-convex calculus of variations

- Uses numerical examples to enhance the theory

This book is geared towards graduate students and researchers in applied mathematics. It can serve as a main text for graduate courses in image processing and inverse problems or as a supplemental text for courses on regularization. Researchers and computer scientists in the area of imaging science will also find this book useful.},
	Author = {Scherzer, O. and Grasmair, M. and Grossauer, H. and Haltmeier, M. and Lenzen, F.},
	Date-Added = {2015-01-29 14:53:00 +0100},
	Date-Modified = {2015-01-29 14:53:00 +0100},
	Number = {167},
	Publisher = {Springer},
	Series = {Applied Mathematical Sciences},
	Title = {Variational Methods in Imaging},
	Year = {2009}}

@inproceedings{BoFrSaSe13,
	Author = {Bond, J. and Frush, D. and Samei, E. and Segars, W. P.},
	Booktitle = {Medical Imaging 2013: Physics of Medical Imaging},
	Date-Added = {2015-01-29 10:37:48 +0100},
	Date-Modified = {2015-01-29 10:41:34 +0100},
	Editor = {Nishikawa, R. M. and Whiting, B. R. and Hoeschen, C.},
	Series = {Proceedings of SPIE},
	Title = {Simulation of anatomical texture in voxelized {XCAT} phantoms},
	Volume = {8668},
	Year = {2013}}

@article{GaHeDa11,
	Abstract = {Considerable recent activity is aimed at reconstructing images from a few projections. Images in any application area are not random samples of all possible images, but have some common attributes. If these attributes are reflected in the smallness of an objective function, then the aim of satisfying the projections can be complemented with the aim of having a small objective value. One widely investigated objective function is total variation (TV), it leads to quite good reconstructions from a few mathematically ideal projections. However, when applied to measured projections that only approximate the mathematical ideal, TV-based reconstructions from a few projections may fail to recover important features in the original images. It has been suggested that this may be due to TV not being the appropriate objective function and that one should use the ℓ1-norm of the Haar transform instead. The investigation reported in this paper contradicts this. In experiments simulating computerized tomography (CT) data collection of the head, reconstructions whose Haar transform has a small ℓ1-norm are not more efficacious than reconstructions that have a small TV value. The search for an objective function that provides diagnostically efficacious reconstructions from a few CT projections remains open.
},
	Author = {Gardu{\~n}o, E. and Herman, G. T. and Davidi, R.},
	Date-Added = {2015-01-29 10:30:13 +0100},
	Date-Modified = {2015-01-29 10:31:57 +0100},
	Journal = {Inverse Problems},
	Number = {5},
	Pages = {055006},
	Title = {Reconstruction from a few projections by $\ell_1$-minimization of the Haar tr ansform},
	Volume = {27},
	Year = {2011}}

@article{HeDa08,
	Abstract = {Image reconstruction from projections suffers from an inherent difficulty: there are different images that have identical projections in any finite number of directions. However, by identifying the type of image that is likely to occur in an application area, one can design algorithms that may be efficacious in that area even when the number of projections is small. One such approach uses total variation minimization. We report on an algorithm based on this approach, and show that sometimes it produces medically-desirable reconstructions in computerized tomography (CT) even from a small number of projections. However, we also demonstrate that such a reconstruction is not guaranteed to provide the medically-relevant information: when data are collected by an actual CT scanner for a small number of projections, the noise in such data may very well result in a tumor in the brain not being visible in the reconstruction.
},
	Author = {Herman, G. T. and Davidi, R.},
	Date-Added = {2015-01-29 10:25:07 +0100},
	Date-Modified = {2015-01-29 10:36:28 +0100},
	Journal = {Inverse Problems},
	Number = {4},
	Pages = {045011},
	Title = {Image reconstruction from a small number of projections},
	Volume = {24},
	Year = {2008}}

@article{SiPa08,
	Abstract = {An iterative algorithm, based on recent work in compressive sensing, is developed for volume image reconstruction from a circular cone-beam scan. The algorithm minimizes the total variation (TV) of the image subject to the constraint that the estimated projection data is within a specified tolerance of the available data and that the values of the volume image are non-negative. The constraints are enforced by the use of projection onto convex sets (POCS) and the TV objective is minimized by steepest descent with an adaptive step-size. The algorithm is referred to as adaptive-steepest-descent-POCS (ASD-POCS). It appears to be robust against cone-beam artifacts, and may be particularly useful when the angular range is limited or when the angular sampling rate is low. The ASD-POCS algorithm is tested with the Defrise disk and jaw computerized phantoms. Some comparisons are performed with the POCS and expectation-maximization (EM) algorithms. Although the algorithm is presented in the context of circular cone-beam image reconstruction, it can also be applied to scanning geometries involving other x-ray source trajectories.
},
	Author = {Sidky, E. Y. and Pan, X.},
	Date-Added = {2015-01-29 10:22:47 +0100},
	Date-Modified = {2015-01-29 10:24:10 +0100},
	Journal = {Physics in Medicine and Biology},
	Number = {17},
	Pages = {4777--4807},
	Title = {Image reconstruction in circular cone-beam computed tomography by constrained, total-variation minimization},
	Volume = {53},
	Year = {2008}}

@article{SiKaPa06,
	Abstract = {In practical applications of tomographic imaging, there are often challenges for image reconstruction due to under-sampling and insufficient data. In computed tomography (CT), for example, image reconstruction from few views would enable rapid scanning with a reduced x-ray dose delivered to the patient. Limited-angle problems are also of practical significance in CT. In this work, we develop and investigate an iterative image reconstruction algorithm based on the minimization of the image total variation (TV) that applies to divergent-beam CT. Numerical demonstrations of our TV algorithm are performed with various insufficient data problems in fan-beam CT. The TV algorithm can be generalized to cone-beam CT as well as other tomographic imaging modalities.},
	Author = {Sidky, E. Y. and Kao, C.-M. and Pan, X.},
	Date-Added = {2015-01-29 10:20:41 +0100},
	Date-Modified = {2015-01-29 10:22:14 +0100},
	Journal = {Journal of X-Ray Science and Technology},
	Number = {2},
	Pages = {119--139},
	Title = {Accurate image reconstruction from few-views and limited-angle data in divergent-beam {CT}},
	Volume = {14},
	Year = {2006}}

@article{QiChZh14,
	Abstract = {Radiation dose reduction without losing CT image quality has been an increasing concern. Reducing the number of X-ray projections to reconstruct CT images, which is also called sparse-projection reconstruction, can potentially avoid excessive dose delivered to patients in CT examination. To overcome the disadvantages of total variation (TV) minimization method, in this work we introduce a novel adaptive TpV regularization into sparse-projection image reconstruction and use FISTA technique to accelerate iterative convergence. The numerical experiments demonstrate that the proposed method suppresses noise and artifacts more efficiently, and preserves structure information better than other existing reconstruction methods.},
	Author = {Qi, H. and Chen, Z. and Zhou, L.},
	Date-Added = {2015-01-29 09:55:49 +0100},
	Date-Modified = {2015-01-29 09:57:44 +0100},
	Journal = {Computational and Mathematical Methods in Medicine},
	Pages = {354869},
	Title = {{CT} Image Reconstruction from Sparse Projections Using Adaptive {TpV} Regularization},
	Year = {2014}}

@article{EvAhSiScYeKl13,
	Abstract = {Computerized tomography (CT) plays an important role in medical imaging for diagnosis and therapy. However, CT imaging is connected with ionization radiation exposure of patients. Therefore, the dose reduction is an essential issue in CT. In 2011, the Expectation Maximization and Total Variation Based Model for CT Reconstruction (EM+TV) was proposed. This method can reconstruct a better image using less CT projections in comparison with the usual filtered back projection (FBP) technique. Thus, it could significantly reduce the overall dose of radiation in CT. This work reports the results of an independent numerical simulation for cone beam CT geometry with alternative virtual phantoms. As in the original report, the 3D CT images of 128 x 128 x128 virtual phantoms were reconstructed. It was not possible to implement phantoms with lager dimensions because of the slowness of code execution even by the CORE i7 CPU.},
	Author = {Evseev, I. and Ahmann, F. and da Silva, H. P. and Schelin, H. R. and Yevseyeva, O. and Klock, M. C. L.},
	Date-Added = {2015-01-29 09:30:17 +0100},
	Date-Modified = {2015-01-29 09:48:57 +0100},
	Journal = {AIP Conference Proceedings},
	Number = {1},
	Pages = {79--81},
	Title = {Test of {3D CT} reconstructions by {EM + TV} algorithm from undersampled data},
	Volume = {1529},
	Year = {2013}}

@article{DeFeChHeVoWe14,
	Abstract = {Compressive sensing (CS) theory has great potential for reconstructing CT images from sparse-views projection data. Currently, total variation (TV-) based CT reconstruction method is a hot research point in medical CT field, which uses the gradient operator as the sparse representation approach during the iteration process. However, the images reconstructed by this method often suffer the smoothing problem; to improve the quality of reconstructed images, this paper proposed a hybrid reconstruction method combining TV and non-aliasing Contourlet transform (NACT) and using the Split-Bregman method to solve the optimization problem. Finally, the simulation results show that the proposed algorithm can reconstruct high-quality CT images from few-views projection using less iteration numbers, which is more effective in suppressing noise and artefacts than algebraic reconstruction technique (ART) and TV-based reconstruction method.},
	Author = {Deng, L.-Z. and Feng, P. and Chen, M.-Y. and He, P. and Vo, Q.-S. and Wei, B.},
	Date-Added = {2015-01-29 09:25:23 +0100},
	Date-Modified = {2015-01-29 09:26:40 +0100},
	Journal = {Computational and Mathematical Methods in Medicine},
	Pages = {753615},
	Title = {A {CT} Reconstruction Algorithm Based on Non-Aliasing Contourlet Transform and Compressive Sensing},
	Year = {2014}}

@article{NaGuLeLiXiGa14,
	Abstract = {Purpose:
Inspired by compressive sensing, sparsity regularized iterative reconstruction method has been extensively studied. However, its utility pertinent to multislice helical 4D CT for radiotherapy with respect to imaging quality, dose, and time has not been thoroughly addressed. As the beginning of such an investigation, this work carries out the initial comparison of reconstructed imaging quality between sparsity regularized iterative method and analytic method through static phantom studies using a state-of-art 128-channel multi-slice Siemens helical CT scanner.

Methods:
In our iterative method, tensor framelet (TF) is chosen as the regularization method for its superior performance from total variation regularization in terms of reduced piecewise-constant artifacts and improved imaging quality that has been demonstrated in our prior work. On the other hand, X-ray transforms and its adjoints are computed on-the-fly through GPU implementation using our previous developed fast parallel algorithms with O(1) complexity per computing thread. For comparison, both FDK (approximate analytic method) and Katsevich algorithm (exact analytic method) are used for multislice helical CT image reconstruction.

Results:
The phantom experimental data with different imaging doses were acquired using a state-of-art 128-channel multi-slice Siemens helical CT scanner. The reconstructed image quality was compared between TF-based iterative method, FDK and Katsevich algorithm with the quantitative analysis for characterizing signal-to-noise ratio, image contrast, and spatial resolution of high-contrast and low-contrast objects.

Conclusion:
The experimental results suggest that our tensor framelet regularized iterative reconstruction algorithm improves the helical CT imaging quality from FDK and Katsevich algorithm for static experimental phantom studies that have been performed.
},
	Author = {Nam, H. and Guo, M. and Lee, K. and Li, R: and Xing, L. and Gao, H.},
	Date-Added = {2015-01-29 09:21:51 +0100},
	Date-Modified = {2015-01-29 10:07:56 +0100},
	Journal = {Medical Physics},
	Pages = {152},
	Title = {{SU-E-I-93}: {I}mproved Imaging Quality for Multislice Helical {CT} Via Sparsity Regularized Iterative Image Reconstruction Method Based On Tensor Framelet},
	Volume = {41},
	Year = {2014}}

@inproceedings{VaVaGoPiPhBeSt11,
	Abstract = {Total variation minimization has been extensively researched for image denoising and sparse view reconstruction. These methods show superior denoising performance for simple images with little texture, but result in texture information loss when applied to more complex images. It could thus be beneficial to use other regularizers within medical imaging. We propose a general regularization method, based on a split-Bregman approach. We show results for this framework combined with a total variation denoising operator, in comparison to ASD-POCS. We show that sparse-view reconstruction and noise regularization is possible. This general method will allow us to investigate other regularizers in the context of regularized CT reconstruction, and decrease the acquisition times in micro-CT.},
	Author = {Vandeghinste, B. and Vandenberghe, S. and Goossens, B. and Pizurica, A. and Philips, W. and de Beenhouwer, J. and Staelens, S.},
	Booktitle = {Proceedings of the 11th International meeting on Fully Three-Dimensional Image Reconstruction in Radiology and Nuclear Medicine (Fully 3D '11)},
	Date-Added = {2015-01-29 09:14:29 +0100},
	Date-Modified = {2015-01-29 09:17:10 +0100},
	Pages = {431--434},
	Title = {Split-{B}regman-based sparse-view {CT} reconstruction},
	Year = {2011}}

@inproceedings{No96,
	Author = {Noll, D.},
	Booktitle = {Recent Advances in Optimization: Proceedings of the 8th French-German Conference on Optimization, Trier, July 21-26, 1996},
	Date-Added = {2015-01-28 16:13:46 +0100},
	Date-Modified = {2015-01-28 16:13:46 +0100},
	Editor = {Gritzmann, P. and Fandel, G. and Trockel, W. and Horst, R. and Sachs, E.},
	Pages = {229--245},
	Publisher = {Springer Verlag},
	Series = {Lecture Notes in Economics and Mathematical Systems},
	Title = {Variational methods in image restoration},
	Volume = {452},
	Year = {1996}}

@article{BoLeNo96,
	Abstract = {Maximum entropy spectral density estimation is a technique for reconstructing an unknown density function from some known measurements by maximizing a given measure of entropy of the estimate. Here we present a variety of new entropy measures which attempt to control derivative values of the densities. Our models apply among others to the inference problem based on the averaged Fisher information measure. The duality theory we develop resembles models used in convex optimal control problems. We present a variety of examples, including relaxed moment matching with Fisher information and best interpolation on a strip.

},
	Author = {Borwein, J. M. and Lewis, A. and Noll, D.},
	Date-Added = {2015-01-28 16:13:27 +0100},
	Date-Modified = {2015-02-02 22:31:50 +0000},
	Journal = {Mathematics of Operations Research},
	Pages = {442--468},
	Title = {Maximum entropy reconstruction using derivative information 1: {F}isher's information.},
	Volume = {21},
	Year = {1996}}

@article{HeNo00,
	Abstract = {We present a class of nonlinear adaptive image restoration filters which may be steered to preserve sharp edges and contrasts in the restorations. From a theoretical point of view we discuss the associated variational problems and prove existence of solutions in certain Sobolev spaces W^(1,p) or in a BV-space. The degree of regularity of the solution may be understood as a mathematical explanation of the heuristic properties of the designed filters.},
	Author = {Hermann, U. and Noll, D.},
	Date-Added = {2015-01-28 16:13:22 +0100},
	Date-Modified = {2015-02-02 22:32:30 +0000},
	Journal = {SIAM Journal on Control and Optimization},
	Number = {4},
	Pages = {1223--1240},
	Title = {Adaptive Image Reconstruction Using Information Measures},
	Volume = {38},
	Year = {2000}}

@article{No97b,
	Author = {Noll, D.},
	Date-Added = {2015-01-28 16:13:12 +0100},
	Date-Modified = {2015-01-28 16:13:16 +0100},
	Journal = {Journal of Global Optimization},
	Pages = {91--103},
	Title = {Restoration of degraded images with maximum entropy},
	Volume = {10},
	Year = {1997}}

@article{No97a,
	Author = {Noll, D.},
	Date-Added = {2015-01-28 16:12:53 +0100},
	Date-Modified = {2015-01-28 16:13:10 +0100},
	Journal = {Advances in Mathematical Sciences and Applications},
	Number = {2},
	Pages = {789--808},
	Title = {Consistency of a nonlinear deconvolution method with applications in image restoration},
	Volume = {7},
	Year = {1997}}

@article{Re05,
	Author = {Resmerita, E.},
	Date-Added = {2015-01-28 16:12:46 +0100},
	Date-Modified = {2015-01-28 16:12:46 +0100},
	Journal = {Inverse Problems},
	Pages = {1303--1314},
	Title = {Regularization of ill-posed problems in {B}anach spaces: convergence rates},
	Volume = {21},
	Year = {2005}}

@article{EnLa93,
	Author = {Engl, H. W. and Landl, G.},
	Date-Added = {2015-01-28 16:12:41 +0100},
	Date-Modified = {2015-01-28 16:12:41 +0100},
	Journal = {SIAM Journal on Numerical Analysis},
	Month = {October},
	Number = {5},
	Pages = {1509--1536},
	Title = {Convergence rates for maximum entropy regularization},
	Volume = {30},
	Year = {1993}}

@article{Eg93,
	Author = {Eggermont, P. P. B.},
	Date-Added = {2015-01-28 16:12:32 +0100},
	Date-Modified = {2015-01-28 16:12:32 +0100},
	Journal = {SIAM Journal on Mathematical Analysis},
	Month = {November},
	Number = {6},
	Pages = {1557--1576},
	Title = {Maximum entropy regularization for {F}redholm integral equations of the first kind},
	Volume = {24},
	Year = {1993}}

@article{MaBu89,
	Author = {Macaulay, V. A. and Buck, B.},
	Date-Added = {2015-01-28 16:12:27 +0100},
	Date-Modified = {2015-01-28 16:12:27 +0100},
	Journal = {Inverse Problems},
	Pages = {859--874},
	Title = {Linear inversion by the method of maximum entropy},
	Volume = {5},
	Year = {1989}}

@book{KaSo05,
	Author = {Kaipio, J. and Somersalo, E.},
	Date-Added = {2015-01-28 16:12:21 +0100},
	Date-Modified = {2015-01-28 16:12:21 +0100},
	Publisher = {Springer Verlag},
	Series = {Applied Mathematical Sciences},
	Title = {Statistical and Computational Inverse Problems},
	Volume = {160},
	Year = {2005}}

@article{SnScOS92,
	Author = {Snyder, D. L. and Schutz, T. J. and O'Sullivan, J. A.},
	Date-Added = {2015-01-28 16:12:15 +0100},
	Date-Modified = {2015-01-28 16:12:15 +0100},
	Journal = {IEEE Transactions on Signal Processing},
	Pages = {1143--1150},
	Title = {Deblurring subject to Nonnegative Constraint},
	Volume = {40},
	Year = {1992}}

@article{Cs91,
	Author = {Csiszar, I.},
	Date-Added = {2015-01-28 16:12:11 +0100},
	Date-Modified = {2015-01-28 16:12:11 +0100},
	Journal = {Annals of Statistics},
	Number = {4},
	Pages = {2032--2066},
	Title = {Why least squares and maximum entropy? {A}n axiomatic approach to inference for linear inverse problems},
	Volume = {19},
	Year = {1991}}

@book{EbSaSa98,
	Address = {Singapore},
	Author = {Ebanks, B. and Sahoo, P. and Sander, W.},
	Date-Added = {2015-01-28 16:12:05 +0100},
	Date-Modified = {2015-01-28 16:12:05 +0100},
	Publisher = {World Scientific},
	Title = {Characterization of information measures},
	Year = {1998}}

@inproceedings{VaGoVaVaPiVaSt12,
	Author = {Vandeghinste, B. and Goossens, B. and Van Holen, R. and Vanhove, C. and Pizurica, A. and Vandenberghe, S. and Staelens, S.},
	Booktitle = {Proceedings of the 2nd International Meeting on image formation in X-ray Computed Tomography, Utah, USA},
	Date-Added = {2015-01-28 15:50:53 +0100},
	Date-Modified = {2015-01-28 15:52:57 +0100},
	Title = {Combined shearlet and {TV} regularization in sparse-view {CT} reconstruction},
	Year = {2012}}

@article{ReSuCoTrBu06,
	Abstract = {A fully 4D joint-estimation approach to reconstruction of temporal sequences of 3D positron emission tomography (PET) images is proposed. The method estimates both a set of temporal basis functions and the corresponding coefficient for each basis function at each spatial location within the image. The joint estimation is performed through a fully 4D version of the maximum likelihood expectation maximization (ML-EM) algorithm in conjunction with two different models of the mean of the Poisson measured data. The first model regards the coefficients of the temporal basis functions as the unknown parameters to be estimated and the second model regards the temporal basis functions themselves as the unknown parameters. The fully 4D methodology is compared to the conventional frame-by-frame independent reconstruction approach (3D ML-EM) for varying levels of both spatial and temporal post-reconstruction smoothing. It is found that using a set of temporally extensive basis functions (estimated from the data by 4D ML-EM) significantly reduces the spatial noise when compared to the independent method for a given level of image resolution. In addition to spatial image quality advantages, for smaller regions of interest (where statistical quality is often limited) the reconstructed time-activity curves show a lower level of bias and a lower level of noise compared to the independent reconstruction approach. Finally, the method is demonstrated on clinical 4D PET data.
},
	Author = {Reader, A. J. and Sureau, F. C. and Comtat, C. and Tr{\'e}bossen, R. and Buvat, I.},
	Date-Added = {2015-01-28 07:38:23 +0000},
	Date-Modified = {2015-01-28 07:39:30 +0000},
	Journal = {Physics in Medicine and Biology},
	Number = {21},
	Pages = {5455--5474},
	Title = {Joint estimation of dynamic {PET} images and temporal basis functions using fully {4D ML-EM}},
	Volume = {51},
	Year = {2006}}

@article{KaBoMoSa05,
	Abstract = {Our goal in this paper is the estimation of kinetic model parameters for each voxel corresponding to a dense three-dimensional (3-D) positron emission tomography (PET) image. Typically, the activity images are first reconstructed from PET sinogram frames at each measurement time, and then the kinetic parameters are estimated by fitting a model to the reconstructed time-activity response of each voxel. However, this "indirect" approach to kinetic parameter estimation tends to reduce signal-to-noise ratio (SNR) because of the requirement that the sinogram data be divided into individual time frames. In 1985, Carson and Lange proposed, but did not implement, a method based on the expectation-maximization (EM) algorithm for direct parametric reconstruction. The approach is "direct" because it estimates the optimal kinetic parameters directly from the sinogram data, without an intermediate reconstruction step. However, direct voxel-wise parametric reconstruction remained a challenge due to the unsolved complexities of inversion and spatial regularization. In this paper, we demonstrate and evaluate a new and efficient method for direct voxel-wise reconstruction of kinetic parameter images using all frames of the PET data. The direct parametric image reconstruction is formulated in a Bayesian framework, and uses the parametric iterative coordinate descent (PICD) algorithm to solve the resulting optimization problem. The PICD algorithm is computationally efficient and is implemented with spatial regularization in the domain of the physiologically relevant parameters. Our experimental simulations of a rat head imaged in a working small animal scanner indicate that direct parametric reconstruction can substantially reduce root-mean-squared error (RMSE) in the estimation of kinetic parameters, as compared to indirect methods, without appreciably increasing computation.
},
	Author = {Kamasak, M. E. and Bouman, C. A. and Morris, E. D. and Sauer, K.},
	Date-Added = {2015-01-28 07:36:36 +0000},
	Date-Modified = {2015-01-28 07:37:45 +0000},
	Journal = {IEEE Transactions on Medical Imaging},
	Number = {5},
	Pages = {636--650},
	Title = {Direct reconstruction of kinetic parameter images from dynamic {PET} data},
	Volume = {24},
	Year = {2005}}

@article{LiThScYaXi06,
	Abstract = {Positron emission tonography (PET) is useful in diagnosis and radiation treatment planning for a variety of cancers. For patients with cancers in thoracic or upper abdominal region, the respiratory motion produces large distortions in the tumor shape and size, affecting the accuracy in both diagnosis and treatment. Four-dimensional (4D) (gated) PET aims to reduce the motion artifacts and to provide accurate measurement of the tumor volume and the tracer concentration. A major issue in 4D PET is the lack of statistics. Since the collected photons are divided into several frames in the 4D PET scan, the quality of each reconstructed frame degrades as the number of frames increases. The increased noise in each frame heavily degrades the quantitative accuracy of the PET imaging. In this work, we propose a method to enhance the performance of 4D PET by developing a new technique of 4D PET reconstruction with incorporation of an organ motion model derived from 4D-CT images. The method is based on the well-known maximum-likelihood expectation-maximization (ML-EM) algorithm. During the processes of forward- and backward-projection in the ML-EM iterations, all projection data acquired at different phases are combined together to update the emission map with the aid of deformable model, the statistics is therefore greatly improved. The proposed algorithm was first evaluated with computer simulations using a mathematical dynamic phantom. Experiment with a moving physical phantom was then carried out to demonstrate the accuracy of the proposed method and the increase of signal-to-noise ratio over three-dimensional PET. Finally, the 4D PET reconstruction was applied to a patient case.
},
	Author = {Li, T. and Thorndyke, B. and Schreibmann, E. and Yang, Y. and Xing, L.},
	Date-Added = {2015-01-28 07:32:18 +0000},
	Date-Modified = {2015-01-28 07:33:07 +0000},
	Journal = {Medical Physics},
	Number = {5},
	Pages = {1288--1298},
	Title = {Model-based image reconstruction for four-dimensional {PET}},
	Volume = {35},
	Year = {2006}}

@article{NiYaJiWeKi11,
	Abstract = {PURPOSE:
In gated cardiac single photon emission computed tomography (SPECT), image reconstruction is often hampered by various degrading factors including depth-dependent spatial blurring, attenuation, scatter, motion blurring, and low data counts. Consequently, there has been significant development in image reconstruction methods for improving the quality of reconstructed images. The goal of this work is to investigate how these degrading factors will impact the reconstructed myocardium when different reconstruction methods are used.
METHODS:
The authors conduct a comparative study of the effects of these degrading factors on the accuracy of myocardium by several reconstruction algorithms, including (1) a clinical spatiotemporal processing method, (2) maximum likelihood (ML) estimation, (3) 3D maximum a posteriori (MAP) estimation, (4) 3D MAP with posttemporal filtering, and (5) motion-compensated spatiotemporal (4D) reconstruction. To quantify the reconstruction results, the authors use the following measures on different aspects of the myocardium: (1) overall error level in the myocardium, (2) regional accuracy of the left ventricle (LV) wall, (3) uniformity of the LV, (4) accuracy of regional time activity curves by normalized cross-correlation coefficient, and (5) perfusion defect detectability. The authors also assess the effectiveness of degrading corrections in reconstruction by considering an upper bound for each reconstruction method, which represents what would be achieved by each method if the acquired data were free from attenuation and scatter degradations. In the experiments the authors use Monte Carlo simulated cardiac gated SPECT imaging based on the 4D NURBS-based cardiac-torso (NCAT) phantom with different patient geometry and lesion settings, in which the simulated ground truth is known for the purpose of quantitative evaluation.
RESULTS:
The results demonstrate that use of temporal processing in reconstruction (Methods 1, 4, and 5 above) can greatly improve the reconstructed myocardium in terms of both error level and perfusion defect detection. In low-count gated studies, it can have even greater impact than other degrading factors. Both attenuation and scatter corrections can lead to reduced error levels in the myocardium in all methods; in particular, with 4D the bias can be reduced by as much as four-fold compared to no correction. There is a slight increase in noise level observed with scatter correction. A significant improvement in heart wall appearance is demonstrated in reconstruction results from three sets of clinical acquisitions as correction for degradations is combined with refinement of temporal filtering.
CONCLUSIONS:
Correction for degrading factors such as resolution, attenuation, scatter, and motion blur can all lead to improved image quality in cardiac gated SPECT reconstruction. However, their effectiveness could also vary with the reconstruction algorithms used. Both attenuation and scatter corrections can effectively reduce the bias level of the reconstructed LV wall, though scatter correction is also observed to increase the variance level. Use of temporal processing in reconstruction can have greater impact on the accuracy of the myocardium than correction of other degrading factors. Overall, use of degrading corrections in 4D reconstruction is shown to be most effective for improving both reconstruction accuracy of the myocardium and detectability of perfusion defects in gated images.},
	Author = {Niu, X. and Yang, Y. and Jin, M. and Wernick, M. N. and King, M. A.},
	Date-Added = {2015-01-28 07:27:09 +0000},
	Date-Modified = {2015-01-28 07:28:52 +0000},
	Journal = {Medical Physics},
	Number = {12},
	Pages = {6571--6584},
	Title = {Effects of motion, attenuation, and scatter corrections on gated cardiac {SPECT} reconstruction},
	Volume = {38},
	Year = {2011}}

@article{LeTs15,
	Abstract = {We developed a realistic simulation dataset for simultaneous respiratory and cardiac (R&C) gated SPECT/CT using the 4D NURBS-based Cardiac-Torso (NCAT) Phantom and Monte Carlo simulation methods, and evaluated it for a sample application study. The 4D NCAT phantom included realistic respiratory motion and beating heart motion based on respiratory gated CT and cardiac tagged MRI data of normal human subjects. To model the respiratory motion, a set of 24 separate 3D NCAT phantoms excluding the heart was generated over a respiratory cycle. The beating heart motion was modeled separately with 48 frames per cardiac cycle for each of the 24 respiratory phases. The resultant set of 24  ×  48 3D NCAT phantoms provides a realistic model of a normal human subject at different phases of combined R&C motions. An almost noise-free SPECT projection dataset for each of the 1152 3D NCAT phantoms was generated using Monte Carlo simulation techniques and the radioactivity uptake distribution of 99mTc sestamibi in different organs. By grouping and summing the separate projection datasets, separate or simultaneous R&C gated acquired data with different gating schemes could be simulated. In the initial evaluation, we combined the projection datasets into ungated, 6 respiratory-gates only, 8 cardiac-gates only, and combined 6 respiratory-gates & 8 cardiac-gates projection datasets. Each dataset was reconstructed using 3D OS-EM without and with attenuation correction using the averaged and respiratory-gated attenuation maps, and the resulting reconstructed images were compared. These results were used to demonstrate the effects of R&C motions and the reduction of image artifact due to R&C motions by gating and attenuation corrections. We concluded that the realistic 4D NCAT phantom and Monte Carlo simulated SPECT projection datasets with R&C motions are powerful tools in the study of the effects of R&C motions, as well as in the development of R&C gating schemes and motion correction methods for improved SPECT/CT imaging.
},
	Author = {Lee, T. S. and Tsui, B. M. W.},
	Date-Added = {2015-01-28 07:23:16 +0000},
	Date-Modified = {2015-01-28 07:24:15 +0000},
	Journal = {Physics in Medicine and Biology},
	Number = {4},
	Pages = {1399--1413},
	Title = {The development and initial evaluation of a realistic simulated {SPECT} dataset with simultaneous respiratory and cardiac motion for gated myocardial perfusion {SPECT}},
	Volume = {60},
	Year = {2015}}

@article{VeGrMiFuRo10,
	Abstract = {Fully 4D PET image reconstruction is receiving increasing research interest due to its ability to significantly reduce spatiotemporal noise in dynamic PET imaging. However, thus far in the literature, the important issue of correcting for subject head motion has not been considered. Specifically, as a direct consequence of using temporally extensive basis functions, a single instance of movement propagates to impair the reconstruction of multiple time frames, even if no further movement occurs in those frames. Existing 3D motion compensation strategies have not yet been adapted to 4D reconstruction, and as such the benefits of 4D algorithms have not yet been reaped in a clinical setting where head movement undoubtedly occurs. This work addresses this need, developing a motion compensation method suitable for fully 4D reconstruction methods which exploits an optical tracking system to measure the head motion along with PET superset data to store the motion compensated data. List-mode events are histogrammed as PET superset data according to the measured motion, and a specially devised normalization scheme for motion compensated reconstruction from the superset data is required. This work proceeds to propose the corresponding time-dependent normalization modifications which are required for a major class of fully 4D image reconstruction algorithms (those which use linear combinations of temporal basis functions). Using realistically simulated as well as real high-resolution PET data from the HRRT, we demonstrate both the detrimental impact of subject head motion in fully 4D PET reconstruction and the efficacy of our proposed modifications to 4D algorithms. Benefits are shown both for the individual PET image frames as well as for parametric images of tracer uptake and volume of distribution for (18)F-FDG obtained from Patlak analysis.
},
	Author = {Verhaeghe, J. and Gravel, P. and Mio, R. and Fukasawa, R. and Rosa-Neto, P. and Soucy, J. P. and Thompson, C. J. and Reader, A. J.},
	Date-Added = {2015-01-28 07:05:47 +0000},
	Date-Modified = {2015-01-28 07:21:25 +0000},
	Journal = {Physics in Medicine and Biology},
	Number = {14},
	Pages = {4063--4082},
	Title = {Motion compensation for fully {4D PET} reconstruction using {PET} superset data},
	Volume = {55},
	Year = {2010}}

@article{MaBaPoSa10,
	Abstract = {Sparse coding--that is, modelling data vectors as sparse linear combinations of basis elements--is widely used in machine learning, neuroscience, signal processing, and statistics. This paper focuses on the large-scale matrix factorization problem that consists of learning the basis set in order to adapt it to specific data. Variations of this problem include dictionary learning in signal processing, non-negative matrix factorization and sparse principal component analysis. In this paper, we propose to address these tasks with a new online optimization algorithm, based on stochastic approximations, which scales up gracefully to large data sets with millions of training samples, and extends naturally to various matrix factorization formulations, making it suitable for a wide range of learning problems. A proof of convergence is presented, along with experiments with natural images and genomic data demonstrating that it leads to state-of-the-art performance in terms of speed and optimization for both small and large data sets.},
	Author = {Mairal, J. and Bach, F. and Ponce, J. and Sapiro, G.},
	Date-Added = {2015-01-28 06:55:46 +0000},
	Date-Modified = {2015-01-28 06:57:10 +0000},
	Journal = {Journal of Machine Learning Research},
	Pages = {19--60},
	Title = {Online Learning for Matrix Factorization and Sparse Coding},
	Volume = {11},
	Year = {2010}}

@article{VeVaKhDALe08,
	Abstract = {Tomographic reconstruction from positron emission tomography (PET) data is an ill-posed problem that requires regularization. An attractive approach is to impose an l(1) -regularization constraint, which favors sparse solutions in the wavelet domain. This can be achieved quite efficiently thanks to the iterative algorithm developed by Daubechies et al., 2004. In this paper, we apply this technique and extend it for the reconstruction of dynamic (spatio-temporal) PET data. Moreover, instead of using classical wavelets in the temporal dimension, we introduce exponential-spline wavelets (E-spline wavelets) that are specially tailored to model time activity curves (TACs) in PET. We show that the exponential-spline wavelets naturally arise from the compartmental description of the dynamics of the tracer distribution. We address the issue of the selection of the "optimal" E-spline parameters (poles and zeros) and we investigate their effect on reconstruction quality. We demonstrate the usefulness of spatio-temporal regularization and the superior performance of E-spline wavelets over conventional Battle-LemariE wavelets in a series of experiments: the 1-D fitting of TACs, and the tomographic reconstruction of both simulated and clinical data. We find that the E-spline wavelets outperform the conventional wavelets in terms of the reconstructed signal-to-noise ratio (SNR) and the sparsity of the wavelet coefficients. Based on our simulations, we conclude that replacing the conventional wavelets with E-spline wavelets leads to equal reconstruction quality for a 40% reduction of detected coincidences, meaning an improved image quality for the same number of counts or equivalently a reduced exposure to the patient for the same image quality.
},
	Author = {Verhaeghe, J. and Van de Ville, D. and Khalidov, I. and D'Asseler, Y. and Lemahieu, I. and Unser, M.},
	Date-Added = {2015-01-28 06:48:29 +0000},
	Date-Modified = {2015-01-28 06:49:43 +0000},
	Journal = {IEEE Transactions on Medical Imaging},
	Number = {7},
	Pages = {943--959},
	Title = {Dynamic {PET} reconstruction using wavelet regularization with adapted basis functions},
	Volume = {27},
	Year = {2008}}

@book{CaKu13,
	Abstract = {Over the last 20 years, multiscale methods and wavelets have revolutionized the field of applied mathematics by providing efficient means for encoding isotropic phenomena. Directional multiscale systems, particularly shearlets, are currently having the same dramatic impact on the encoding of multivariate signals, which are usually dominated by anisotropic features. Since its introduction about six years ago, the theory of shearlets has rapidly developed and gained wide recognition as the superior approach for multiscale analysis of multivariate signals, providing a truly unified treatment of both the continuum and the digital setting. By now, this research field has reached full maturity, with deep mathematical results, efficient numerical methods, and a variety of high-impact applications. 

Edited by the topic's two main pioneers, this volume systematically surveys the theory and applications of shearlets. Following a general survey of the subject, carefully selected contributions explore the current state of the field in greater detail. Specific areas covered include:  
- analysis of anisotropic features;
- sparse approximations of multivariate data;
- shearlet smoothness spaces;
- numerical implementations;
- applications to image processing.  

Shearlets is aimed at graduate students and researchers in the areas of applied mathematics, computer science, engineering, and any other field dealing with the development and applications of highly efficient methodologies for processing multivariate data. As the first monograph in the literature to survey shearlets, this volume offers both a unique state-of-the-art resource for scientists dealing with advanced multiscale methods and a supplemental textbook for graduate courses in applied harmonic analysis.},
	Date-Added = {2015-01-27 14:37:45 +0100},
	Date-Modified = {2015-01-27 14:39:46 +0100},
	Editor = {Casazza, P. G. and Kutyniok, G.},
	Publisher = {Springer-Verlag},
	Series = {Applied and Numerical Harmonic Analysis},
	Title = {Finite Frames. {T}heory and Applications},
	Year = {2013}}

@techreport{ProjectPlan1,
	Author = {{\"O}ktem, O},
	Date-Added = {2015-01-26 14:39:15 +0000},
	Date-Modified = {2015-01-26 14:41:24 +0000},
	Institution = {Department of Mathematics, KTH - Royal Institute of Technology},
	Number = {Version 1.13},
	Title = {Low complexity image reconstruction in medical imaging. {P}roject plan for the period 2014-09-01--2014-12-31},
	Type = {Project plan},
	Year = {2014}}

@article{HaSt14,
	Abstract = {     In recent years it has turned out that shearlets have the potential to retrieve directional information so that they became interesting for many applications. Moreover the continuous shearlet transform has the outstanding property to stem from a square integrable group representation. However, to use shearlets and the shearlet transform for reasonable applications one needs fast algorithms to compute a discrete shearlet transform. In this tutorial we present the steps towards an implementation of a fast and finite shearlet transform that is only based on the FFT. Using band-limited shearlets we construct a Parseval frame that provides a simple and straightforward inverse shearlet transform. We provide all proofs and discuss several aspects of our implementation. },
	Author = {H{\"a}user, S. and Steidl, G.},
	Date-Added = {2015-01-23 14:33:21 +0100},
	Date-Modified = {2015-01-23 14:35:39 +0100},
	Journal = {ArXiv},
	Number = {1202.1773v2},
	Title = {Fast Finite Shearlet Transform: a tutorial},
	Volume = {math.NA},
	Year = {2014}}

@article{KuLiRe14,
	Abstract = {Wavelets and their associated transforms are highly efficient when approximating and analyzing one-dimensional signals. However, multivariate signals such as images or videos typically exhibit curvilinear singularities, which wavelets are provably deficient of sparsely approximating and also of analyzing in the sense of, for instance, detecting their direction. Shearlets are a directional representation system extending the wavelet framework, which overcomes those deficiencies. Similar to wavelets, shearlets allow a faithful implementation and fast associated transforms. In this paper, we will introduce a comprehensive carefully documented software package coined ShearLab 3D (www.ShearLab.org) and discuss its algorithmic details. This package provides MATLAB code for a novel faithful algorithmic realization of the 2D and 3D shearlet transform (and their inverses) associated with compactly supported universal shearlet systems incorporating the option of using CUDA. We will present extensive numerical experiments in 2D and 3D concerning denoising, inpainting, and feature extraction, comparing the performance of ShearLab 3D with similar transform-based algorithms such as curvelets, contourlets, or surfacelets. In the spirit of reproducible reseaerch, all scripts are accessible on www.ShearLab.org. },
	Author = {Kutyniok, G. and Lim, W.-Q. and Reisenhofer, R.},
	Date-Added = {2015-01-23 14:29:08 +0100},
	Date-Modified = {2015-01-23 14:33:04 +0100},
	Journal = {ArXiv},
	Number = {1402.5670},
	Title = {{ShearLab 3D}: {F}aithful Digital Shearlet Transforms based on Compactly Supported Shearlets},
	Volume = {math.NA},
	Year = {2014}}

@article{LoZhOsBe10,
	Abstract = {This paper considers two nonlocal regularizations for image recovery, which exploit the spatial interactions in images. We get superior results using preprocessed data as input for the weighted functionals. Applications discussed include image deconvolution and tomographic reconstruction. The numerical results show our method outperforms some previous ones.
},
	Author = {Lou, Y. and Zhang, X. and Osher, S. and Bertozzi, A.},
	Date-Added = {2015-01-23 14:05:43 +0100},
	Date-Modified = {2015-01-23 14:05:43 +0100},
	Journal = {Journal of Scientific Computing},
	Number = {2},
	Pages = {185--197},
	Title = {Image Recovery via Nonlocal Operators},
	Volume = {42},
	Year = {2010}}

@book{WeKi14,
	Abstract = {A comprehensive overview of all types of phantoms used in medical imaging, therapy, nuclear medicine and health physics is provided in this title. For ionizing radiation, dosimetry with respect to issues of material composition, shape, and motion/position effects are all highlighted. For medical imaging, each type of technology will need specific materials and designs, and the physics and indications will be explored for each type. Health physics phantoms are concerned with some of the same issues such as material heterogeneity, but also unique issues such as organ-specific radiation dose from sources distributed in other organs.

Though some of the information enclosed is found in other sources, divided especially along the three categories of imaging, therapy, and health physics, many medical physicists as well as professionals need to bridge these three catagories. Readers will be able to use this book to select the appropriate phantom from a vendor at a clinic, to learn from as a student, to choose materials for custom phantom design, to design dynamic features, and as a reference for a variety of applications.},
	Date-Added = {2015-01-23 12:08:00 +0100},
	Date-Modified = {2015-01-23 12:08:41 +0100},
	Editor = {DeWerd, L. A. and Kissick, M.},
	Publisher = {Springer-Verlag},
	Series = {Biological and Medical Physics, Biomedical Engineering},
	Title = {The Phantoms of Medical and Health Physics: Devices for Research and Development},
	Year = {2014}}

@book{RiGr15,
	Abstract = {The topic of this book -- sparse modeling -- is a particular manifestation of the parsimony principle in the context of modern statistics, machine learning and signal processing. A fundamental problem in those fields is an accurate recovery of an unobserved high-dimensional signal from a relatively small number of measurements, due to measurement costs or other limitations. Image reconstruction, learning model parameters from data, diagnosing system failures or human diseases are just a few examples where this challenging inverse problem arises. In general, high-dimensional, small-sample inference is both underdetermined and computationally intractable, unless the problem has some specific structure, such as, for example, sparsity.

Indeed, quite frequently, the ground-truth solution can be well-approximated by a sparse vector, where only a few variables are truly important, while the remaining ones are zero or nearly-zero; in other words, a small number of most-relevant variables (causes, predictors, etc.) can be often sufficient for explaining a phenomenon of interest. More generally, even if the original problem specification does not yield a sparse solution, one can typically find a mapping to a new a coordinate system, or dictionary, which allows for such sparse representation. Thus, sparse structure appears to be an inherent property of many natural signals -- and without such structure, understanding the world and adapting to it would be considerably more challenging.
In this book, we tried to provide a brief introduction to sparse modeling, including application examples, problem formulations that yield sparse solutions, algorithms for finding such solutions, as well as some recent theoretical results on sparse recovery. The material of this book is based on our tutorial presented several years ago at the ICML-2010 (International Conference on Machine Learning), as well as on a graduate-level course that we taught at the Columbia University in the spring semester of 2011.

We start chapter 1 with motivating examples and a high-level survey of key recent developments in sparse modeling. In chapter 2, we formulate optimization problems that involve commonly used sparsity-enforcing tools such as l0- and l1-norm constraints. Essential theoretical results are presented in chapters 3 and 4, while chapter 5 discusses several well-known algorithms for finding sparse solutions. Then, in chapters 6 and 7, we discuss a variety of sparse recovery problems that extend the basic formulation towards more sophisticated forms of structured sparsity and towards different loss functions, respectively. Chapter 8 discusses a particular class of sparse graphical models such as sparse Gaussian Markov Random Fields, a popular and fast-developing subarea of sparse modeling. Finally, chapter 9 is devoted to dictionary learning and sparse matrix factorizations.},
	Author = {Rish, I. and Grabarnik, G.},
	Date-Added = {2015-01-20 16:26:17 +0000},
	Date-Modified = {2015-01-20 16:27:43 +0000},
	Publisher = {CRC Press},
	Series = {Machine Learning \& Pattern Recognition Series},
	Title = {Sparse Modeling. Theory, Algorithms, and Applications},
	Year = {2015}}

@book{By14,
	Author = {Byrne, C. L.},
	Date-Added = {2015-01-20 15:16:19 +0000},
	Date-Modified = {2015-01-20 15:16:58 +0000},
	Publisher = {CRC Press},
	Series = {Monographs and Research Notes in Mathematics},
	Title = {Iterative Optimization in Inverse Problems},
	Year = {2014}}

@book{ElKu012,
	Abstract = {This book is the first monograph in the literature to provide a comprehensive survey of compressed sensing. The potential reader of this book could be a researcher in the areas of applied mathematics, computer science, and electrical engineering, or a related research area, or a graduate student seeking to learn about CS. The particular design of this volume ensures that it can serve as both a state-of-the-art reference for researchers as well as a textbook for students.

The book contains 12 diverse chapters written by recognized leading experts from all over the world covering a large variety of topics. The book begins with a comprehensive introduction to CS which serves as a background for the remaining chapters, and also sets the notation to be used throughout the book. It does not assume any prior knowledge in the field. The following chapters are then organized into 4 categories: Extended signal models (Chapters 2--4), sensing matrix design (Chapters 5--6), recovery algorithms and performance guarantees (Chapters 7--9), and applications (Chapters 10--12). The chapters are self-contained, covering the most recent research results in the respective topic, and can all be treated independent of the others. A brief summary of each chapter is given next.

Chapter 1 provides a comprehensive introduction to the basics of CS. After a brief historical overview, the chapter begins with a discussion of sparsity and other low- dimensional signal models. The authors then treat the central question of how to accurately recover a high-dimensional signal from a small set of measurements and provide performance guarantees for a variety of sparse recovery algorithms. The chapter concludes with a discussion of some extensions of the sparse recovery framework.

Chapter 2 goes beyond traditional sparse modeling, and addresses collaborative struc- tured sparsity to add stability and prior information to the representation. In structured sparse modeling, instead of considering the dictionary atoms as singletons, the atoms are partitioned in groups, and a few groups are selected at a time for the signal encod- ing. Further structure is then added via collaboration, where multiple signals, which are known to follow the same model, are allowed to collaborate in the coding. The authors discuss applications of these models to image restoration and source separation.

Chapter 3 generalizes CS to reduced-rate sampling of analog signals. It introduces Xampling, a unified framework for low rate sampling and processing of signals lying in a union of subspaces. A hardware-oriented viewpoint is advocated throughout, address- ing practical constraints and exemplifying hardware realizations of sub-Nyquist systems. A variety of analog CS applications are reviewed within the unified Xampling framework including multiband communications with unknown carrier frequencies, ultrasound imaging, and wideband radar.

Chapter 4 considers reduced-rate sampling of finite rate of innovation (FRI) analog signals such as streams of pulses from discrete measurements. Exploiting the fact that only a small number of parameters per unit of time are needed to fully describe FRI signals allows to sample them at rates below Nyquist. The authors provide an overview of the theory and algorithms along with a diverse set of applications in areas such as superresolution, radar and ultrasound.

Chapter 5 considers constructions of random CS matrices with proven performance guarantees. The author provides an overview of basic non-asymptotic methods and con- cepts in random matrix theory. Several tools from geometric functional analysis and probability theory are put together in order to analyze the extreme singular values of random matrices. This then allows deducing results on random matrices used for sensing in CS.

Chapter 6 investigates the advantages of sequential measurement schemes that adap- tively focus sensing using information gathered throughout the measurement process. This is in contrast to most theory and methods for sparse recovery which are based on an assumption of non-adaptive measurements. In particular, the authors show that adaptive sensing can be significantly more powerful when the measurements are contaminated with additive noise.

Chapter 7 introduces a unified high dimensional geometric framework for analyzing the phase transition phenomenon of l1 minimization in sparse recovery. This framework connects studying the phase transitions of l1 minimization with computing the Grass- mann angles in high dimensional convex geometry. The authors further demonstrate the broad applications of this Grassmann angle framework by giving sharp phase transitions for related recovery methods.

Chapter 8 presents an overview of several greedy methods and explores their theoret- ical properties. Greedy algorithms are very fast and easy to implement and often have similar theoretical performance guarantees to convex methods. The authors detail some of the leading greedy approaches for sparse recovery, and consider extensions of these methods to more general signal structures.

Chapter 9 surveys recent work in applying ideas from graphical models and message passing algorithms to solve large scale regularized regression problems. In particular, the focus is on CS reconstruction via l1 penalized least-squares. The author discusses how to derive fast approximate message passing algorithms to solve this problem and shows how the analysis of such algorithms allows to prove exact high-dimensional limit results on the recovery error.

Chapter 10 considers compressed learning, where learning is performed directly in the compressed domain. The authors provide tight bounds demonstrating that the linear kernel SVM's classifier in the measurement domain, with high probability, has true accuracy close to the accuracy of the best linear threshold classifier in the data domain. It is also shown that for a family of well-known CS matrices, compressed learning is provided on the flight. The authors then demonstrate these results in the context of texture analysis.

Chapter 11 surveys methods for data separation by sparse representations. The author considers the use of sparsity in problems in which the data is composed of two or more morphologically distinct constituents. The key idea is to choose a deliberately over- complete representation made of several frames, each one providing a sparse expansion of one of the components to be extracted. The morphological difference between the components is then encoded as incoherence conditions of those frames which allows for separation using CS algorithms.

Chapter 12 applies CS to the classical problem of face recognition. The authors con- sider the problem of recognizing human faces in the presence of real-world nuisances such as occlusion and variabilities in pose and illumination. The main idea behind the proposed approach is to explain any query image using a small number of training images from a single subject category. This core idea is then generalized to account for various physical variabilities encountered in face recognition. The authors demonstrate how the resulting system is capable of accurately recognizing subjects out of a database of several hundred subjects with state-of-the-art accuracy.},
	Date-Added = {2015-01-20 15:11:55 +0000},
	Date-Modified = {2015-01-20 15:13:52 +0000},
	Editor = {Eldar, Y. C. and Kutyniok, G.},
	Publisher = {Cambridge University Press},
	Title = {Compressed Sensing. Theory and Applications},
	Year = {2012}}

@book{Ba14,
	Abstract = {Obtaining sparse solutions is not trivial. Sparsity is defined through the l0 norm--the number of nonzero components--of the variable being optimized. To obtain a sparse solution, this norm must hence be directly minimized or, alternately, imposed as a constraint on the optimization problem. Unfortunately optimization problems involving the l0 norm require determination of the optimal set of components to be assigned nonzero values and are hence combinatorial in nature and are generally computationally intractable. As a result, one must either employ greedy algorithms to obtain a solution or employ proxies that are relaxations of the l0 norm. Both of these approaches have yielded highly effective algorithms for optimization, when the loss function is quadratic or, more generally, convex in nature.

For more generic classes of loss functions, however, the situation is not so clear. Proxies to the l0 norm which can be shown to result in optimally sparse solutions for quadratic or convex loss functions are no longer guaranteed to provide optimal solutions for other loss functions. It is similarly unclear whether greedy algorithms that are effective for well-behaved loss functions will be equally effective in the most general case.

This is the problem space tackled in this monograph. In an outstanding series of results, the author develops and analyzes a greedy framework for sparsity-constrained optimization of a wide class of loss functions, shows how it may be applied to various problems, and finally extends it to handle the case where the solutions are not merely sparse, but restricted to lie in specified subspaces.

GraSP is the proposed greedy framework for sparse optimization of loss functions. Through rigorous analysis, the author demonstrates that it imposes far fewer constraints on the loss function, only requiring it to be convex on sparse subspaces, and converges linearly to the optimal solution. As an illustrative application he applies GraSP to the problem of feature selection through sparse optimization of logistic functions, and demonstrates that it results in significantly better solutions than current methods. One-bit compressive sensing is the problem of reconstructing a signal from a series of one-bit measurements, a challenging but exciting problem. The author demonstrates that GraSP-based solutions can result in greatly improved signal recovery over all other current methods.

Subsequently, the author develops a solution to deal with model-based sparsity: problems where the solutions are not only required to be sparse, but are further restricted to lie on only specific subspaces. Such problems frequently arise, for instance, when additional information is available about the interdependence between the location of nonzero values in the estimated variables.

Finally the author reverses gear and addresses a more philosophical problem--that of identifying the best proxy for gradient-based algorithms for sparsity-constrained least-squares optimization--and arrives at the remarkable result that the optimal proxy is the l0 norm itself.
Together, the contributions of this monograph lay a solid foundation of techniques and results for any aspiring or established researcher wishing to work on the problem of sparse optimization of difficult-to-optimize loss functions. As such, I believe that this monograph is a mandatory inclusion in the library of anybody working on the topic.},
	Author = {Bahmani, S.},
	Date-Added = {2015-01-20 15:09:19 +0000},
	Date-Modified = {2015-01-20 16:33:18 +0000},
	Publisher = {Springer-Verlag},
	Series = {Springer Theses},
	Title = {Algorithms for Sparsity-Constrained Optimization},
	Year = {2014}}

@book{KuLa12,
	Abstract = {Over the last 20 years, multiscale methods and wavelets have revolutionized the field of applied mathematics by providing efficient means for encoding isotropic phenomena. Directional multiscale systems, particularly shearlets, are currently having the same dramatic impact on the encoding of multivariate signals, which are usually dominated by anisotropic features. Since its introduction about six years ago, the theory of shearlets has rapidly developed and gained wide recognition as the superior approach for multiscale analysis of multivariate signals, providing a truly unified treatment of both the continuum and the digital setting. By now, this research field has reached full maturity, with deep mathematical results, efficient numerical methods, and a variety of high-impact applications. 

Edited by the topic's two main pioneers, this volume systematically surveys the theory and applications of shearlets. Following a general survey of the subject, carefully selected contributions explore the current state of the field in greater detail. Specific areas covered include:  
- analysis of anisotropic features;
- sparse approximations of multivariate data;
- shearlet smoothness spaces;
- numerical implementations;
- applications to image processing.  

Shearlets is aimed at graduate students and researchers in the areas of applied mathematics, computer science, engineering, and any other field dealing with the development and applications of highly efficient methodologies for processing multivariate data. As the first monograph in the literature to survey shearlets, this volume offers both a unique state-of-the-art resource for scientists dealing with advanced multiscale methods and a supplemental textbook for graduate courses in applied harmonic analysis.},
	Date-Added = {2015-01-07 10:42:55 +0100},
	Date-Modified = {2015-01-07 10:44:38 +0100},
	Editor = {Kutyniok, G. and Labate, D.},
	Publisher = {Springer-Verlag},
	Series = {Applied and Numerical Harmonic Analysis},
	Title = {Shearlets. Multiscale Analysis for Multivariate Data},
	Year = {2012}}

@article{HaSa12,
	Abstract = {We present a MATLAB package with implementations of several algebraic iterative reconstruction methods for discretizations of inverse problems. These so-called row action methods rely on semi-convergence for achieving the necessary regularization of the problem. Two classes of methods are implemented: Algebraic Reconstruction Techniques (ART) and Simultaneous Iterative Reconstruction Techniques (SIRT). In addition we provide a few simplified test problems from medical and seismic tomography. For each iterative method, a number of strategies are available for choosing the relaxation parameter and the stopping rule. The relaxation parameter can be fixed, or chosen adaptively in each iteration; in the former case we provide a new ``training'' algorithm that finds the optimal parameter for a given test problem. The stopping rules provided are the discrepancy principle, the monotone error rule, and the NCP criterion; for the first two methods ``training'' can be used to find the optimal discrepancy parameter.},
	Author = {Hansen, P. C. and Saxild-Hansen, M.},
	Date-Added = {2015-01-05 11:21:58 +0100},
	Date-Modified = {2015-01-05 11:22:54 +0100},
	Journal = {Journal of Computational and Applied Mathematics},
	Number = {8},
	Pages = {2167--2178},
	Title = {{AIR} Tools - {A MATLAB} Package of Algebraic Iterative Reconstruction Methods},
	Volume = {236},
	Year = {2012}}

@article{JeJoHaJe11,
	Abstract = {We present a practical implementation of an optimal first-order method, due to Nesterov, for large-scale total variation regularization in tomographic reconstruction, image deblurring, etc. The algorithm applies to μ-strongly convex objective functions with L-Lipschitz continuous gradient. In the framework of Nesterov both μ and L are assumed known---an assumption that is seldom satisfied in practice. We propose to incorporate mechanisms to estimate locally sufficient μ and L during the iterations. The mechanisms also allow for the application to non-strongly convex functions. We discuss the convergence rate and iteration complexity of several first-order methods, including the proposed algorithm, and we use a 3D tomography problem to compare the performance of these methods. In numerical simulations we demonstrate the advantage in terms of faster convergence when estimating the strong convexity parameter μ for solving ill-conditioned problems to high accuracy, in comparison with an optimal method for non-strongly convex problems and a first-order method with Barzilai-Borwein step size selection.
},
	Author = {Jensen, T. L. and J{\o}rgensen, J. H. and Hansen, P. C. and Jensen, S. H.},
	Date-Added = {2015-01-05 11:17:34 +0100},
	Date-Modified = {2015-01-05 11:18:28 +0100},
	Journal = {BIT},
	Number = {2},
	Pages = {329--356},
	Title = {Implementation of an Optimal First-Order Method for Strongly Convex Total Variation Regularization},
	Volume = {52},
	Year = {2011}}

@article{DaHaJeJe10,
	Abstract = {This paper describes new algorithms and related software for total variation (TV) image reconstruction, more specifically: denoising, inpainting, and deblurring. The algorithms are based on one of Nesterov's first-order methods, tailored to the image processing applications in such a way that, except for the mandatory regularization parameter, the user needs not specify any parameters in the algorithms. The software is written in C with interface to Matlab (version 7.5 or later), and we demonstrate its performance and use with examples.
},
	Author = {Dahl, J. and Hansen, P. C. and Jensen, S. H. and Jensen, T. L.},
	Date-Added = {2015-01-05 11:15:37 +0100},
	Date-Modified = {2015-01-05 11:16:35 +0100},
	Journal = {Numerical Algorithms},
	Number = {1},
	Pages = {67--92},
	Title = {Algorithms and software for total variation image reconstruction via first-order methods},
	Volume = {53},
	Year = {2010}}

@article{RoSpRa04,
	Abstract = {A multidimensional image navigation and display software was designed for display and interpretation of large sets of multidimensional and multimodality images such as combined PET-CT studies. The software is developed in Objective-C on a Macintosh platform under the MacOS X operating system using the GNUstep development environment. It also benefits from the extremely fast and optimized 3D graphic capabilities of the OpenGL graphic standard widely used for computer games optimized for taking advantage of any hardware graphic accelerator boards available. In the design of the software special attention was given to adapt the user interface to the specific and complex tasks of navigating through large sets of image data. An interactive jog-wheel device widely used in the video and movie industry was implemented to allow users to navigate in the different dimensions of an image set much faster than with a traditional mouse or on-screen cursors and sliders. The program can easily be adapted for very specific tasks that require a limited number of functions, by adding and removing tools from the program's toolbar and avoiding an overwhelming number of unnecessary tools and functions. The processing and image rendering tools of the software are based on the open-source libraries ITK and VTK. This ensures that all new developments in image processing that could emerge from other academic institutions using these libraries can be directly ported to the OsiriX program. OsiriX is provided free of charge under the GNU open-source licensing agreement at http://homepage.mac.com/rossetantoine/osirix.
},
	Author = {Rosset, A. and Spadola, L. and Ratib, O.},
	Date-Added = {2015-01-05 11:11:40 +0100},
	Date-Modified = {2015-01-05 11:12:55 +0100},
	Journal = {Journal of Digital Imaging},
	Number = {3},
	Pages = {205--216},
	Title = {{OsiriX}: an open-source software for navigating in multidimensional {DICOM} images},
	Volume = {17},
	Year = {2004}}

@article{KlDaHe13,
	Abstract = {The problem of reconstruction of slices and volumes from 1D and 2D projections has arisen in a large number of scientific fields (including computerized tomography, electron microscopy, X-ray microscopy, radiology, radio astronomy and holography). Many different methods (algorithms) have been suggested for its solution. In this paper we present a software package, SNARK09, for reconstruction of 2D images from their 1D projections. In the area of image reconstruction, researchers often desire to compare two or more reconstruction techniques and assess their relative merits. SNARK09 provides a uniform framework to implement algorithms and evaluate their performance. It has been designed to treat both parallel and divergent projection geometries and can either create test data (with or without noise) for use by reconstruction algorithms or use data collected by another software or a physical device. A number of frequently-used classical reconstruction algorithms are incorporated. The package provides a means for easy incorporation of new algorithms for their testing, comparison and evaluation. It comes with tools for statistical analysis of the results and ten worked examples.},
	Author = {Klukowska, J. and Davidi, R. and Herman, G. T.},
	Date-Added = {2015-01-05 11:09:35 +0100},
	Date-Modified = {2015-01-05 11:11:13 +0100},
	Journal = {Computer Methods and Programs in Biomedicine},
	Pages = {424--440},
	Title = {{SNARK09} -- A software package for reconstruction of {2D} images from {1D} projections},
	Volume = {110},
	Year = {2013}}

@article{RiViBrLaSaSh14,
	Abstract = {We propose the Reconstruction Toolkit (RTK, http://www.openrtk.org), an open-source toolkit for fast cone-beam CT reconstruction, based on the Insight Toolkit (ITK) and using GPU code extracted from Plastimatch. RTK is developed by an open consortium (see affiliations) under the non-contaminating Apache 2.0 license. The quality of the platform is daily checked with regression tests in partnership with Kitware, the company supporting ITK. Several features are already available: Elekta, Varian and IBA inputs, multi-threaded Feldkamp-David-Kress reconstruction on CPU and GPU, Parker short scan weighting, multi-threaded CPU and GPU forward projectors, etc. Each feature is either accessible through command line tools or C++ classes that can be included in independent software. A MIDAS community has been opened to share CatPhan datasets of several vendors (Elekta, Varian and IBA). RTK will be used in the upcoming cone-beam CT scanner developed by IBA for proton therapy rooms. Many features are under development: new input format support, iterative reconstruction, hybrid Monte Carlo / deterministic CBCT simulation, etc. RTK has been built to freely share tomographic reconstruction developments between researchers and is open for new contributions.
},
	Author = {Rit, S. and Vila Oliva, M. and Brousmiche, S. and Labarbe, R. and Sarrut, D. and Sharp, G. C.},
	Date-Added = {2015-01-05 10:23:28 +0100},
	Date-Modified = {2015-01-05 10:25:25 +0100},
	Journal = {Journal of Physics: Conference Series},
	Number = {1},
	Pages = {012079},
	Title = {The {R}econstruction {T}oolkit {(RTK)}, an open-source cone-beam {CT} reconstruction toolkit based on the {I}nsight {T}oolkit {(ITK)}},
	Volume = {489},
	Year = {2014}}

@article{RuKl11,
	Abstract = {Since scattered radiation in cone-beam volume CT implies severe degradation of CT images by quantification errors, artifacts, and noise increase, scatter suppression is one of the main issues related to image quality in CBCT imaging. The aim of this review is to structurize the variety of scatter suppression methods, to analyze the common structure, and to develop a general framework for scatter correction procedures. In general, scatter suppression combines hardware techniques of scatter rejection and software methods of scatter correction. The authors emphasize that scatter correction procedures consist of the main components scatter estimation (by measurement or mathematical modeling) and scatter compensation (deterministic or statistical methods). The framework comprises most scatter correction approaches and its validity also goes beyond transmission CT. Before the advent of cone-beam CT, a lot of papers on scatter correction approaches in x-ray radiography, mammography, emission tomography, and in Megavolt CT had been published. The opportunity to avail from research in those other fields of medical imaging has not yet been sufficiently exploited. Therefore additional references are included when ever it seems pertinent. Scatter estimation and scatter compensation are typically intertwined in iterative procedures. It makes sense to recognize iterative approaches in the light of the concept of self-consistency. The importance of incorporating scatter compensation approaches into a statistical framework for noise minimization has to be underscored. Signal and noise propagation analysis is presented. A main result is the preservation of differential-signal-to-noise-ratio (dSNR) in CT projection data by ideal scatter correction. The objective of scatter compensation methods is the restoration of quantitative accuracy and a balance between low-contrast restoration and noise reduction. In a synopsis section, the different deterministic and statistical methods are discussed with respect to their properties and applications. The current paper is focused on scatter compensation algorithms. The multitude of scatter estimation models will be dealt with in a separate paper.},
	Author = {R{\"u}hrnschopf, E. P. and Klingenbeck, K.},
	Date-Added = {2015-01-05 09:22:21 +0100},
	Date-Modified = {2015-01-05 09:23:15 +0100},
	Journal = {Medical Physics},
	Number = {7},
	Pages = {4296--4311},
	Title = {A general framework and review of scatter correction methods in x-ray cone-beam computerized tomography. Part 1: {S}catter compensation approaches},
	Volume = {38},
	Year = {2011}}

@book{GiJiDaSc15,
	Abstract = {Respiratory and cardiac motion leads to image degradation in Positron Emission Tomography (PET), which impairs quantification. In this book, the authors present approaches to motion estimation and motion correction in thoracic PET. The approaches for motion estimation are based on dual gating and mass-preserving image registration (VAMPIRE) and mass-preserving optical flow (MPOF). With mass-preservation, image intensity modulations caused by highly non-rigid cardiac motion are accounted for. Within the image registration framework different data terms, different variants of regularization and parametric and non-parametric motion models are examined. Within the optical flow framework, different data terms and further non-quadratic penalization are also discussed. The approaches for motion correction particularly focus on pipelines in dual gated PET. A quantitative evaluation of the proposed approaches is performed on software phantom data with accompanied ground-truth motion information. Further, clinical applicability is shown on patient data. The book concludes with an outlook of recent developments and potential future advances in the field of PET motion correction.
},
	Author = {Gigengack, F. and Jiang, X. and Dawood, M. and Sch{\"a}fers, K. P.},
	Date-Added = {2014-12-29 11:40:14 +0000},
	Date-Modified = {2014-12-29 11:42:08 +0000},
	Publisher = {Springer-Verlag},
	Title = {Motion Correction in Thoracic Positron Emission Tomography},
	Year = {2015}}

@article{NeErPaPeRo04,
	Abstract = {We have reported in our previous studies on the methodology, and feasibility of 4D-PET (Gated PET) acquisition, to reduce respiratory motion artifact in PET imaging of the thorax. In this study, we expand our investigation to address the problem of respiration motion in PET/CT imaging. The respiratory motion of four lung cancer patients were monitored by tracking external markers placed on the thorax. A 4D-CT acquisition was performed using a "step-and-shoot" technique, in which computed tomography (CT) projection data were acquired over a complete respiratory cycle at each couch position. The period of each CT acquisition segment was time stamped with an "x-ray ON" signal, which was recorded by the tracking system. 4D-CT data were then sorted into 10 groups, according to their corresponding phase of the breathing cycle. 4D-PET data were acquired in the gated mode, where each breathing cycle was divided into ten 0.5 s bins. For both CT and PET acquisitions, patients received audio prompting to regularize breathing. The 4D-CT and 4D-PET data were then correlated according to respiratory phase. The effect of 4D acquisition on improving the co-registration of PET and CT images, reducing motion smearing, and consequently increase the quantitation of the SUV, were investigated. Also, quantitation of the tumor motions in PET, and CT, were studied and compared. 4D-PET with matching phase 4D-CTAC showed an improved accuracy in PET-CT image co-registration of up to 41%, compared to measurements from 4D-PET with clinical-CTAC. Gating PET data in correlation with respiratory motion reduced motion-induced smearing, thereby decreasing the observed tumor volume, by as much as 43%. 4D-PET lesions volumes showed a maximum deviation of 19% between clinical CT and phase- matched 4D-CT attenuation corrected PET images. In CT, 4D acquisition resulted in increasing the tumor volume in two patients by up to 79%, and decreasing it in the other two by up to 35%. Consequently, these corrections have yielded an increase in the measured SUV by up to 16% over the clinical measured SUV, and 36% over SUV's measured in 4D-PET with clinical-CT Attenuation Correction (CTAC) SUV's. Quantitation of the maximum tumor motion amplitude, using 4D-PET and 4D-CT, showed up to 30% discrepancy between the two modalities. We have shown that 4D PET/CT is clinically a feasible method, to correct for respiratory motion artifacts in PET/CT imaging of the thorax. 4D PET/CT acquisition can reduce smearing, improve the accuracy in PET-CT co-registration, and increase the measured SUV. This should result in an improved tumor assessment for patients with lung malignancies.
},
	Author = {Nehmeh, S. A. and Erdi, Y. E. and Pan, T. and Pevsner, A. and Rosenzweig, K. E. and Yorke, E. and Mageras, G. S. and Schoder, H. and Vernon, P. and Squire, O. and Mostafavi, H. and Larson, S. M. and Humm, J. L.},
	Date-Added = {2014-12-29 11:25:53 +0000},
	Date-Modified = {2014-12-29 11:30:10 +0000},
	Journal = {Medical Physics},
	Number = {12},
	Pages = {3179--3186},
	Title = {Four-dimensional {(4D) PET/CT} imaging of the thorax},
	Volume = {31},
	Year = {2004}}

@article{RaTaZa09,
	Abstract = {In this article, the authors review novel techniques in the emerging field of spatiotemporal four-dimensional (4D) positron emission tomography (PET) image reconstruction. The conventional approach to dynamic PET imaging, involving independent reconstruction of individual PET frames, can suffer from limited temporal resolution, high noise (especially when higher frame sampling is introduced to better capture fast dynamics), as well as complex reconstructed image noise distributions that can be very difficult and time consuming to model in kinetic parameter estimation tasks. Various approaches that seek to address some or all of these limitations are described, including techniques that utilize (a) iterative temporal smoothing, (b) advanced temporal basis functions, (c) principal components transformation of the dynamic data, (d) wavelet-based techniques, as well as (e) direct kinetic parameter estimation methods. Future opportunities and challenges with regards to the adoption of 4D and higher dimensional image reconstruction techniques are also outlined.
},
	Author = {Rahmim, A. and Tang, J. and Zaidi, H.},
	Date-Added = {2014-12-29 11:24:20 +0000},
	Date-Modified = {2014-12-29 11:25:24 +0000},
	Journal = {Medical Physics},
	Number = {8},
	Pages = {3654--3670},
	Title = {Four-dimensional ({4D}) image reconstruction strategies in dynamic {PET}: beyond conventional independent frame reconstruction},
	Volume = {36},
	Year = {2009}}

@article{SaBaBoFrJa14,
	Abstract = {In this paper, the authors' review the applicability of the open-source GATE Monte Carlo simulation platform based on the GEANT4 toolkit for radiation therapy and dosimetry applications. The many applications of GATE for state-of-the-art radiotherapy simulations are described including external beam radiotherapy, brachytherapy, intraoperative radiotherapy, hadrontherapy, molecular radiotherapy, and in vivo dose monitoring. Investigations that have been performed using GEANT4 only are also mentioned to illustrate the potential of GATE. The very practical feature of GATE making it easy to model both a treatment and an imaging acquisition within the same framework is emphasized. The computational times associated with several applications are provided to illustrate the practical feasibility of the simulations using current computing facilities.
},
	Author = {Sarrut, D. and Bardi{\`e}s, M. and Boussion, N. and Freud, N. and Jan, S. and L{\'e}tang, J. M. and Loudos, G. and Maigne, L. and Marcatili, S. and Mauxion, T. and Papadimitroulas, P. and Perrot, Y. and Pietrzyk, U. and Robert, C. and Schaart, D. and Visvikis, D. and Buvat, I.},
	Date-Added = {2014-12-29 09:47:21 +0000},
	Date-Modified = {2014-12-29 09:49:55 +0000},
	Journal = {Medical Physics},
	Number = {6},
	Pages = {064301},
	Title = {A review of the use and potential of the {GATE} Monte Carlo code for radiation therapy and dosimetry applications},
	Volume = {41},
	Year = {2014}}

@article{JaBeBeCaCa11,
	Abstract = {GATE (Geant4 Application for Emission Tomography) is a Monte Carlo simulation platform developed by the OpenGATE collaboration since 2001 and first publicly released in 2004. Dedicated to the modelling of planar scintigraphy, single photon emission computed tomography (SPECT) and positron emission tomography (PET) acquisitions, this platform is widely used to assist PET and SPECT research. A recent extension of this platform, released by the OpenGATE collaboration as GATE V6, now also enables modelling of x-ray computed tomography and radiation therapy experiments. This paper presents an overview of the main additions and improvements implemented in GATE since the publication of the initial GATE paper (Jan et al 2004 Phys. Med. Biol. 49 4543-61). This includes new models available in GATE to simulate optical and hadronic processes, novelties in modelling tracer, organ or detector motion, new options for speeding up GATE simulations, examples illustrating the use of GATE V6 in radiotherapy applications and CT simulations, and preliminary results regarding the validation of GATE V6 for radiation therapy applications. Upon completion of extensive validation studies, GATE is expected to become a valuable tool for simulations involving both radiotherapy and imaging.
},
	Author = {Jan, S. and Benoit, D. and Becheva, E. and Carlier, T. and Cassol, F. and Descourt, P. and Frisson, T. and Grevillot, L. and Guigues, L. and Maigne, L. and Morel, C. and Perrot, Y. and Rehfeld, N. and Sarrut, D. and Schaart, D. R. and Stute, S. and Pietrzyk, U. and Visvikis, D. and Zahra, N. and Buvat, I.},
	Date-Added = {2014-12-29 09:45:27 +0000},
	Date-Modified = {2014-12-29 09:46:37 +0000},
	Journal = {Physics in Medicine and Biology},
	Pages = {881--901},
	Title = {{GATE V6}: a major enhancement of the {GATE} simulation platform enabling modelling of {CT} and radiotherapy},
	Volume = {56},
	Year = {2011}}

@article{JaSaStStAs04,
	Abstract = {Monte Carlo simulation is an essential tool in emission tomography that can assist in the design of new medical imaging devices, the optimization of acquisition protocols and the development or assessment of image reconstruction algorithms and correction techniques. GATE, the Geant4 Application for Tomographic Emission, encapsulates the Geant4 libraries to achieve a modular, versatile, scripted simulation toolkit adapted to the field of nuclear medicine. In particular, GATE allows the description of time-dependent phenomena such as source or detector movement, and source decay kinetics. This feature makes it possible to simulate time curves under realistic acquisition conditions and to test dynamic reconstruction algorithms. This paper gives a detailed description of the design and development of GATE by the OpenGATE collaboration, whose continuing objective is to improve, document and validate GATE by simulating commercially available imaging systems for PET and SPECT. Large effort is also invested in the ability and the flexibility to model novel detection systems or systems still under design. A public release of GATE licensed under the GNU Lesser General Public License can be downloaded at http:/www-lphe.epfl.ch/GATE/. Two benchmarks developed for PET and SPECT to test the installation of GATE and to serve as a tutorial for the users are presented. Extensive validation of the GATE simulation platform has been started, comparing simulations and measurements on commercially available acquisition systems. References to those results are listed. The future prospects towards the gridification of GATE and its extension to other domains such as dosimetry are also discussed.},
	Author = {Jan, S. and Santin, G. and Strul, D. and Staelens, S. and Assi{\'e}, K. and Autret, D. and Avner, S. and Barbier, R. and Bardies, M. and Bloomfield, P. M. and Brasse, D. and Breton, V. and Bruyndonckx, P. and Buvat, I. and Chatziioannou, A. F. and Choi, Y. and Chung, Y. H. and Comtat, C. and Donnarieix, D. and Ferrer, L. and Glick, S. J. and Groiselle, C. J. and Guez D. and Honore PF. and Kerhoas-Cavata S. and Kirov AS. and Kohli V. and Koole M. and Krieguer M. and van der Laan, D. J. and Lamare, F. and Largeron, G. and Lartizien, C. and Lazaro, D. and Maas, M. C. and Maigne, L. and Mayet, F. and Melot, F. and Merheb, C. and Pennacchio, E. and Perez, J. and Pietrzyk, U. and Rannou, F. R. and Rey, M. and Schaart, D. R. and Schmidtlein, C. R. and Simon, L. and Song, T. Y. and Vieira, J. M. and Visvikis, D. and Van de Walle, R. and Wiers, E. and Morel, C.},
	Date-Added = {2014-12-29 09:35:59 +0000},
	Date-Modified = {2014-12-29 09:39:56 +0000},
	Journal = {Physics in Medicine and Biology},
	Pages = {4543--4561},
	Title = {{GATE}: a simulation toolkit for {PET} and {SPECT}},
	Volume = {49},
	Year = {2004}}

@article{KaKnPeKa06,
	Abstract = {In the beginning of 2004 medical spiral-CT scanners that acquire up to 64 slices simultaneously became available. Most manufacturers use a straightforward acquisition principle, namely an x-ray focus rotating on a circular path and an opposing cylindrical detector whose rotational center coincides with the x-ray focus. The 64-slice scanner available to us, a Somatom Sensation 64 spiral cone-beam CT scanner (Siemens, Medical Solutions, Forchheim, Germany), makes use of a flying focal spot (FFS) that allows for view-by-view deflections of the focal spot in the rotation direction (αFFS) and in the z-direction (zFFS) with the goal of reducing aliasing artifacts. The FFS feature doubles the sampling density in the radial direction (channel direction, αFFS) and in the longitudinal direction (detector row direction or z-direction, zFFS). The cost of increased radial and azimuthal sampling is a two- or four-fold reduction of azimuthal sampling (angular sampling). To compensate for the potential reduction of azimuthal sampling the scanner simply increases the number of detector read-outs (readings) per rotation by a factor two or four. Then, up to four detector readings contribute to what we define as one view or one projection. A significant reduction of in-plane aliasing and of aliasing in the z-direction can be expected. Especially the latter is of importance to spiral CT scans where aliasing is known to produce so-called windmill artifacts. We have derived and analyzed the optimal focal spot deflection values ∂α and ∂z as they would ideally occur in our scanner. Based upon these we show how image reconstruction can be performed in general. A simulation study showing reconstructions of mathematical phantoms further provides evidence that image quality can be significantly improved with the FFS. Aliasing artifacts, that manifest as streaks emerging from high-contrast objects, and windmill artifacts are reduced by almost an order of magnitude with the FFS compared to a simulation without FFS. Patient images acquired with our 64-slice cone-beam CT scanner support these results.

},
	Author = {Kachelriess, M. and Knaup, M. and Penssel, C. and Kalender, W. A.},
	Date-Added = {2014-12-29 08:57:13 +0000},
	Date-Modified = {2014-12-29 08:58:40 +0000},
	Journal = {IEEE Transactions on Nuclear Science},
	Number = {3},
	Pages = {1238--1247},
	Title = {Flying focal spot ({FFS}) in cone-beam {CT}},
	Volume = {53},
	Year = {2006}}

@article{GuDeXiJa14,
	Abstract = {Analysis of tomographic datasets at synchrotron light sources (including X-ray transmission tomography, X-ray fluorescence microscopy and X-ray diffraction tomography) is becoming progressively more challenging due to the increasing data acquisition rates that new technologies in X-ray sources and detectors enable. The next generation of synchrotron facilities that are currently under design or construction throughout the world will provide diffraction-limited X-ray sources and are expected to boost the current data rates by several orders of magnitude, stressing the need for the development and integration of efficient analysis tools. Here an attempt to provide a collaborative framework for the analysis of synchrotron tomographic data that has the potential to unify the effort of different facilities and beamlines performing similar tasks is described in detail. The proposed Python-based framework is open-source, platform- and data-format-independent, has multiprocessing capability and supports procedural programming that many researchers prefer. This collaborative platform could affect all major synchrotron facilities where new effort is now dedicated to developing new tools that can be deployed at the facility for real-time processing, as well as distributed to users for off-site data processing.

},
	Author = {G{\"u}rsoy, D. and De Carlo, F. and Xiao, X. and Jacobsen, C.},
	Date-Added = {2014-12-28 21:34:19 +0000},
	Date-Modified = {2014-12-28 21:36:12 +0000},
	Journal = {Journal of Synchrotron Radiation},
	Number = {5},
	Pages = {1188--1193},
	Title = {{TomoPy}: a framework for the analysis of synchrotron tomographic data},
	Volume = {21},
	Year = {2014}}

@incollection{PeCaLe14,
	Abstract = {We introduce iLang, a language and software framework for probabilistic inference. The iLang framework enables the definition of directed and undirected probabilistic graphical models and the automated synthesis of high performance inference algorithms for imaging applications. The iLang framework is composed of a set of language primitives and of an inference engine based on a message-passing system that integrates cutting-edge computational tools, including proximal algorithms and high performance Hamiltonian Markov Chain Monte Carlo techniques. A set of domain-specific highly optimized GPU-accelerated primitives specializes iLang to the spatial data-structures that arise in imaging applications. We illustrate the framework through a challenging application: spatio-temporal tomographic reconstruction with compressive sensing.
},
	Author = {Pedemonte, S. and Catana, C. and van Leemput, K.},
	Booktitle = {Bayesian and graphical Models for Biomedical Imaging. Revised Selected Papers from the First International Workshop, BAMBI 2014, Cambridge, MA, USA, September 18, 2014},
	Date-Added = {2014-12-28 14:37:15 +0000},
	Date-Modified = {2014-12-28 14:39:37 +0000},
	Editor = {Cardoso, J. M. and Simpson, I. and Arbel, T. and Precup, D. and Ribbens, A.},
	Pages = {61--72},
	Publisher = {Springer Verlag},
	Series = {Lecture Notes in Computer Science},
	Title = {An Inference Language for Imaging},
	Volume = {8677},
	Year = {2014}}

@inproceedings{PeBoErMo10,
	Abstract = {Stochastic methods based on Maximum Likelihood Estimation (MLE) provide accurate tomographic reconstruction for emission imaging. Moreover methods based on MLE allow to include an accurate physical model of the imaging setup in the reconstruction process, thus enabling quantitative reconstruction of radio-tracer activity distribution. It has been shown that inclusion of a spatially dependent PSF that models dependence of the CDR with distance from the detector, improves the quality of reconstruction in terms of noise and bias. The computational complexity associated with stochastic methods has limited adoption of such algorithms for clinical use and inclusion of the PSF further increases the computational cost. This work proposes an accelerated implementation of a reconstruction algorithm specifically designed to take advantage of the architecture of a General Purpose Graphics Processing Unit (GPGPU).
},
	Author = {Pedemonte, S. and Bousse, A. and Erlandsson, K. and Modat, M. and Arridge, S. and Hutton, B. F. and Ourselin, S.},
	Booktitle = {2010 IEEE Nuclear Science Symposium Conference Record (NSS/MIC)},
	Date-Added = {2014-12-28 14:32:32 +0000},
	Date-Modified = {2014-12-28 14:35:12 +0000},
	Pages = {2657--2661},
	Title = {{GPU} accelerated rotation-based emission tomography reconstruction},
	Year = {2010}}

@article{SySuEn11,
	Abstract = {This paper describes a few mild design constraints that permit rapid adaptation of the modelling code for linear wave propagation to imaging/inversion or design optimiza- tion applications, retaining parallelism and other performance enhancements of the underlying simulator. It also describes an abstract software framework preserving the modularity of both optimization and modelling software in building inversion applications and illustrates this possibility via an example framework implemented in C++. Wave inverse problems tend to be afflicted by a variety of features, in- cluding extreme ill-conditioning and nonlinearity, which degrade the performance of optimization formulations. Extended modelling variants of least-squares inversion, motivated by migration velocity analysis, may relieve some of these difficulties. The framework described also accommodates these extensions to standard inversion.},
	Author = {Symes, W. W. and Sun, D. and Enriquez, M.},
	Date-Added = {2014-12-26 12:13:41 +0000},
	Date-Modified = {2014-12-26 12:14:29 +0000},
	Journal = {Geophysical Prospecting},
	Pages = {814---833},
	Title = {From modelling to inversion: designing a well-adapted simulator},
	Volume = {59},
	Year = {2011}}

@article{PaShSy09,
	Abstract = {The Rice Vector Library is a collection of C++ classes expressing core concepts (vector, function,...) of calculus in Hilbert space with minimal implementation dependence, and providing standardized interfaces behind which to hide application-dependent implementation details (data containers, function objects). A variety of coordinate-free algorithms from linear algebra and optimization, in- cluding Krylov subspace methods and various relatives of Newton's method for nonlinear equations and constrained and unconstrained optimization, may be expressed purely in terms of this system of classes. The resulting code may be used without alteration in a wide range of control, design, and parameter estimation applications, in serial and parallel computing environments.},
	Author = {Padula, A. D. and Shannon, D. S. and Symes, W. W.},
	Date-Added = {2014-12-26 12:03:23 +0000},
	Date-Modified = {2014-12-26 12:04:47 +0000},
	Journal = {ACM Transactions on Mathematical Software},
	Number = {2},
	Pages = {8:1--8:36},
	Title = {A software framework for abstract expression of coordinate-free linear algebra and optimization algorithms},
	Volume = {36},
	Year = {2009}}

@techreport{Fe09,
	Abstract = {ASPIRE 3.0 is a collection of ANSI C language programs for performing tomographic image reconstruction and image restoration using statistical methods. This user's guide describes how to compile and use the software.
},
	Annote = {This document is a users guide for the iterative 3D image reconstruction portion of the ASPIRE software suite. This software is available from the author's web site.},
	Author = {Fessler, J. A.},
	Date-Added = {2014-12-26 10:47:14 +0000},
	Date-Modified = {2014-12-26 10:48:16 +0000},
	Institution = {Department of Electrical Engineering and Computer Science, The University of Michigan},
	Number = {293},
	Title = {{ASPIRE 3.0} User's Guide: {A} Sparse Iterative Reconstruction Library},
	Type = {Technical Report},
	Year = {2009}}

@techreport{Fe13,
	Annote = {This document is a users guide for the iterative 3D image reconstruction portion of the ASPIRE software suite. This software is available from the author's web site.},
	Author = {Fessler, J. A.},
	Date-Added = {2014-12-26 10:41:13 +0000},
	Date-Modified = {2014-12-26 10:42:30 +0000},
	Institution = {Department of Electrical Engineering and Computer Science, The University of Michigan},
	Number = {310},
	Title = {Users guide for {ASPIRE} {3D} image reconstruction software},
	Type = {Technical Report},
	Year = {2013}}

@article{SiJoPa12,
	Abstract = {The primal--dual optimization algorithm developed in Chambolle and Pock (CP) (2011 J. Math. Imag. Vis. 40 1--26) is applied to various convex optimization problems of interest in computed tomography (CT) image reconstruction. This algorithm allows for rapid prototyping of optimization problems for the purpose of designing iterative image reconstruction algorithms for CT. The primal--dual algorithm is briefly summarized in this paper, and its potential for prototyping is demonstrated by explicitly deriving CP algorithm instances for many optimization problems relevant to CT. An example application modeling breast CT with low-intensity x-ray illumination is presented.
},
	Author = {Sidky, E. Y. and J{\o}rgensen, J. H. and Pan, X.},
	Date-Added = {2014-12-19 16:39:43 +0100},
	Date-Modified = {2014-12-19 16:41:05 +0100},
	Journal = {Physics in Medicine and Biology},
	Number = {10},
	Pages = {3065--3091},
	Title = {Convex optimization problem prototyping for image reconstruction in computed tomography with the {C}hambolle--{P}ock algorithm},
	Volume = {57},
	Year = {2012}}

@article{BrLoRe14,
	Abstract = {Convergence analysis is carried out for a forward-backward splitting/generalized gradient projection method for the minimization of a special class of non-smooth and genuinely non-convex minimization problems in infinite-dimensional Hilbert spaces. The functionals under consideration are the sum of a smooth, possibly non-convex and non-smooth, necessarily non-convex functional. For separable constraints in the sequence space, we show that the generalized gradient projection method amounts to a discontinuous iterative thresholding procedure, which can easily be implemented. In this case we prove strong subsequential convergence and moreover show that the limit satisfies strengthened necessary conditions for a global minimizer, i.e., it avoids a certain set of non-global minimizers. Eventually, the method is applied to problems arising in the recovery of sparse data, where strong convergence of the whole sequence is shown, and numerical tests are presented.
},
	Author = {Bredies, K. and Lorenz, D. A. and Reiterer, S.},
	Date-Added = {2014-12-19 09:52:37 +0100},
	Date-Modified = {2014-12-19 09:53:58 +0100},
	Journal = {Journal of Optimization Theory and Applications{\S}},
	Title = {Minimization of non-smooth, non-convex functionals by iterative thresholding},
	Year = {2014}}

@incollection{Fo10,
	Abstract = {These lecture notes address the analysis of numerical methods for performing opti- mizations with linear model constraints and additional sparsity conditions to solutions, i.e., we expect solutions which can be represented as sparse vectors with respect to a prescribed basis. In the first part of the manuscript we illustrate the theory of compressed sensing with emphasis on computational aspects. We present the analysis of the homotopy method, the iteratively re-weighted least squares method, and the iterative hard-thresholding. In the second part, start- ing from the analysis of iterative soft-thresholding, we illustrate several numerical methods for addressing sparse optimizations in Hilbert spaces. The third and final part is concerned with numerical techniques, based on domain decomposition methods, for dimensionality reduction in large scale computing. In the notes we are mainly focusing on the analysis of the algorithms, and we report a few illustrative numerical examples.},
	Author = {Fornasier, M.},
	Booktitle = {Theoretical Foundations and Numerical Methods for Sparse Recovery},
	Date-Added = {2014-12-19 09:48:47 +0100},
	Date-Modified = {2014-12-19 09:50:31 +0100},
	Editor = {Fornasier, M.},
	Publisher = {De Gruyter},
	Series = {Radon Series on Computational and Applied Mathematics},
	Title = {Numerical Methods for Sparse Recovery},
	Volume = {9},
	Year = {2010}}

@article{MaRaFe13,
	Abstract = {To reduce blur in noisy images, regularized image restoration methods have been proposed that use nonquadratic regularizers (like l1 regularization or total-variation) that suppress noise while preserving edges in the image. Most of these methods assume a circulant blur (periodic convolution with a blurring kernel) that can lead to wraparound artifacts along the boundaries of the image due to the implied periodicity of the circulant model. Using a noncirculant model could prevent these artifacts at the cost of increased computational complexity. In this paper, we propose to use a circulant blur model combined with a masking operator that prevents wraparound artifacts. The resulting model is noncirculant, so we propose an efficient algorithm using variable splitting and augmented Lagrangian (AL) strategies. Our variable splitting scheme, when combined with the AL framework and alternating minimization, leads to simple linear systems that can be solved noniteratively using fast Fourier transforms (FFTs), eliminating the need for more expensive conjugate gradient-type solvers. The proposed method can also efficiently tackle a variety of convex regularizers, including edge-preserving (e.g., total-variation) and sparsity promoting (e.g., l1-norm) regularizers. Simulation results show fast convergence of the proposed method, along with improved image quality at the boundaries where the circulant model is inaccurate.},
	Author = {Matakos, A. and Ramani, S. and Fessler, J. A.},
	Date-Added = {2014-12-19 09:27:40 +0100},
	Date-Modified = {2014-12-19 09:28:24 +0100},
	Journal = {IEEE Transactions on Image Processing},
	Number = {5},
	Pages = {2019--2029},
	Title = {Accelerated edge-preserving image restoration without boundary artifacts},
	Volume = {22},
	Year = {2013}}

@article{LeRaGi95,
	Abstract = {Tomographic image reconstruction using statistical methods can provide more accurate system modeling, statistical models, and physical constraints than the conventional filtered backprojection (FBP) method. Because of the ill posedness of the reconstruction problem, a roughness penalty is often imposed on the solution to control noise. To avoid smoothing of edges, which are important image attributes, various edge-preserving regularization methods have been proposed. Most of these schemes rely on information from local neighborhoods to determine the presence of edges. In this paper, we propose a cost function that incorporates nonlocal boundary information into the regularization method. We use an alternating minimization algorithm with deterministic annealing to minimize the proposed cost function, jointly estimating region boundaries and object pixel values. We apply variational techniques implemented using level-sets methods to update the boundary estimates; then, using the most recent boundary estimate, we minimize a space-variant quadratic cost function to update the image estimate. For the positron emission tomography transmission reconstruction application, we compare the bias-variance tradeoff of this method with that of a ``conventional'' penalized-likelihood algorithm with local Huber roughness penalty.},
	Author = {Yu, D. F. and Fessler, J. A.},
	Date-Added = {2014-12-19 09:23:05 +0100},
	Date-Modified = {2014-12-19 09:26:06 +0100},
	Journal = {IEEE Transactions on Medical Imaging},
	Number = {2},
	Pages = {159--173},
	Title = {Edge-Preserving Tomographic Reconstruction with Nonlocal Regularization},
	Volume = {21},
	Year = {2002}}

@inproceedings{Fe97,
	Abstract = {We present a new class of algorithms for edge-preserving restoration of piecewise-smooth images measured in non- Gaussian noise under shift-variant blur. The algorithms are based on minimizing a regularized objective function, and are guaranteed to monotonically decrease the objective function. The algorithms are derived by using a combination of two previously unconnected concepts: A. De Pierro's convexity technique for optimization transfer, and P. Huber's iteration for M-estimation. Convergence to the unique global minimum is guaranteed for strictly convex objective functions. The convergence rate is very fast relative to conventional gradient-based iterations. The proposed algorithms are flexibly parallelizable, and easily accommodate non-negativity constraints and arbitrary neighborhood structures. Implementation in Matlab is remarkably simple, requiring no cumbersome line searches or tolerance parameters.},
	Author = {Fessler, J. A.},
	Booktitle = {Image Reconstruction and Restoration II},
	Date-Added = {2014-12-19 09:06:18 +0100},
	Date-Modified = {2014-12-19 09:09:21 +0100},
	Pages = {184--194},
	Series = {Proceedings of SPIE},
	Title = {Grouped coordinate descent algorithms for robust edge-preserving image restoration},
	Volume = {3170},
	Year = {1997}}

@article{MaSaEl08,
	Abstract = {This paper presents a framework for learning multiscale sparse representations of color images and video with overcomplete dictionaries. A single-scale K-SVD algorithm was introduced in [M. Aharon, M. Elad, and A. M. Bruckstein, IEEE Trans. Signal Process., 54 (2006), pp. 4311--4322], formulating sparse dictionary learning for grayscale image representation as an optimization problem, efficiently solved via orthogonal matching pursuit (OMP) and singular value decomposition (SVD). Following this work, we propose a multiscale learned representation, obtained by using an efficient quadtree decomposition of the learned dictionary and overlapping image patches. The proposed framework provides an alternative to predefined dictionaries such as wavelets and is shown to lead to state-of-the-art results in a number of image and video enhancement and restoration applications. This paper describes the proposed framework and accompanies it by numerous examples demonstrating its strength.},
	Author = {Mairal, J. and Sapiro, G. and Elad, M.},
	Date-Added = {2014-12-17 16:55:47 +0100},
	Date-Modified = {2014-12-17 16:56:34 +0100},
	Journal = {SIAM Multiscale Modelling and Simulation},
	Number = {1},
	Pages = {214---241},
	Title = {Learning multiscale sparse representations for image and video restoration},
	Volume = {7},
	Year = {2008}}

@article{PrEl09,
	Abstract = {In this paper, we consider denoising of image sequences that are corrupted by zero-mean additive white Gaussian noise. Relative to single image denoising techniques, denoising of sequences aims to also utilize the temporal dimension. This assists in getting both faster algorithms and better output quality. This paper focuses on utilizing sparse and redundant representations for image sequence denoising, extending the work reported in. In the single image setting, the K-SVD algorithm is used to train a sparsifying dictionary for the corrupted image. This paper generalizes the above algorithm by offering several extensions: i) the atoms used are 3-D; ii) the dictionary is propagated from one frame to the next, reducing the number of required iterations; and iii) averaging is done on patches in both spatial and temporal neighboring locations. These modifications lead to substantial benefits in complexity and denoising performance, compared to simply running the single image algorithm sequentially. The algorithm's performance is experimentally compared to several state-of-the-art algorithms, demonstrating comparable or favorable results.},
	Author = {Protter, M. and Elad, M.},
	Date-Added = {2014-12-17 16:54:01 +0100},
	Date-Modified = {2014-12-17 16:54:50 +0100},
	Journal = {IEEE Transactions on Image Processing},
	Number = {1},
	Pages = {27---36},
	Title = {Image sequence denoising via sparse and redundant representations},
	Volume = {18},
	Year = {2009}}

@inproceedings{ThBoSaHs06,
	Abstract = { Computed Tomography (CT) screening and pediatric imaging, among other applications, demand the development of more efficient reconstruction techniques to diminish radiation dose to the patient. While many methods are proposed to limit or modulate patient exposure to x-ray at scan time, the resulting data is excessively noisy, and generates image artifacts unless properly corrected. Statistical iterative reconstruction (IR) techniques have recently been introduced for reconstruction of low-dose CT data, and rely on the accurate modeling of the distribution of noise in the acquired data. After conversion from detector counts to attenuation measurements, however, noisy data usually deviate from simple Gaussian or Poisson representation, which limits the ability of IR to generate artifact-free images. This paper introduces a recursive filter for IR, which conserves the statistical properties of the measured data while pre-processing attenuation measurements. A basic framework for inclusion of detector electronic noise into the statistical model for IR is also presented. The results are shown to successfully eliminate streaking artifacts in photon-starved situations. },
	Author = {Thibault, J.-B. and Bouman, C. A. and Sauer, K. D. and Hsieh, J.},
	Booktitle = {Computational Imaging IV},
	Date-Added = {2014-12-17 16:51:48 +0100},
	Date-Modified = {2014-12-17 16:53:03 +0100},
	Pages = {60650X},
	Series = {Proceedings of SPIE},
	Title = {A recursive filter for noise reduction in statistical iterative tomographic imaging},
	Volume = {6065},
	Year = {2006}}

@article{SaBo93,
	Abstract = {A method for Bayesian reconstruction which relies on updates of single pixel values, rather than the entire image, at each iteration is presented. The technique is similar to Gauss-Seidel (GS) iteration for the solution of differential equations on finite grids. The computational cost per iteration of the GS approach is found to be approximately equal to that of gradient methods. For continuously valued images, GS is found to have significantly better convergence at modes representing high spatial frequencies. In addition, GS is well suited to segmentation when the image is constrained to be discretely valued. It is shown that Bayesian segmentation using GS iteration produces useful estimates at much lower signal-to-noise ratios than required for continuously valued reconstruction. The convergence properties of gradient ascent and GS for reconstruction from integral projections are analyzed, and simulations of both maximum-likelihood and maximum a posteriori cases are included
},
	Author = {Sauer, K. and Bouman, C.},
	Date-Added = {2014-12-17 16:50:21 +0100},
	Date-Modified = {2014-12-17 16:51:11 +0100},
	Journal = {IEEE Transactions in Signal Processing},
	Number = {2},
	Pages = {534---548},
	Title = {A local update strategy for iterative reconstruction from projections},
	Volume = {41},
	Year = {1993}}

@article{LaCa84,
	Abstract = {Two proposed likelihood models for emission and transmission image reconstruction accurately incorporate the Poisson nature of photon counting noise and a number of other relevant physical features. As in most algebraic schemes, the region to be reconstructed is divided into small pixels. For each pixel a concentration or attenuation coefficient must be estimated. In the maximum likelihood approach these parameters are estimated by maximizing the likelihood (probability of the observations). EM algorithms are iterative techniques for finding maximum likelihood estimates. In this paper we discuss the general principles behind all EM algorithms and derive in detail the specific algorithms for emission and transmission tomography. The virtues of the EM algorithms include (a) accurate incorporation of a good physical model, (b) automatic inclusion of non-negativity constraints on all parameters, (c) an excellent measure of the quality of a reconstruction, and (d) global convergence to a single vector of parameter estimates. We discuss the specification of necessary physical features such as source and detector geometries. Actual reconstructions are deferred to a later time.},
	Author = {Lange, K. and Carson, R.},
	Date-Added = {2014-12-17 16:48:47 +0100},
	Date-Modified = {2014-12-17 16:49:55 +0100},
	Journal = {Journal of Computer Assisted Tomography},
	Number = {2},
	Pages = {306---316},
	Title = {{EM} reconstruction algorithms for emission and transmission tomography},
	Volume = {8},
	Year = {1984}}

@article{RoMa77,
	Abstract = {The stochastic nature of the projections used in transmission image reconstruction has received little attention to date. This paper utilizes the joint probability density function of the projections to derive the reconstruction scheme which is optimum in the maximum likelihood sense. Two regimes are examined: that where there is significant probability of a zero count projection, and that where the zero count event may be safely ignored. The former regime leads to a complicated algorithm whose performance is data dependent. The latter regime leads to a simpler algorithm. Its performance, in terms of its bias and variance, has been calculated. It is shown that, for an average number of counts detected in excess of approximately 100 per projection, the image is essentially unbiased, and for counts in excess of approximately 2500 per projection, the image approximately attains the minimum variance of any reconstruction scheme using the same measurements.
},
	Author = {Rockmore, A. J. and Macovski, A.},
	Date-Added = {2014-12-17 16:47:27 +0100},
	Date-Modified = {2014-12-17 16:48:28 +0100},
	Journal = {IEEE Transactions in Nuclear Science},
	Number = {3},
	Pages = {1929---1935},
	Title = {A maximum likelihood approach to transmission image reconstruction from projections},
	Volume = {24},
	Year = {1977}}

@article{YaFe98,
	Abstract = {Positron emission tomography (PET) measurements are usually precorrected for accidental coincidence events by real-time subtraction of the delayed-window coincidences. Randoms subtraction compensates on average for accidental coincidences but destroys the Poisson statistics. We propose and analyze two new approximations to the exact log-likelihood of the precorrected measurements, one based on a 'shifted Poisson' model, the other based on saddle-point approximations to the measurement of probability mass function (PMF). The methods apply to both emission and transmission tomography; however, in this paper we focus on transmission tomography. We compare the new models to conventional data-weighted least-squares (WLS) and conventional maximum-likelihood methods [based on the ordinary Poisson (OP) model] using simulations and analytic approximations. The results demonstrate that the proposed methods avoid the systematic bias of the WLS method, and lead to significantly lower variance than the conventional OP method. The saddle-point method provides a more accurate approximation to the exact log-likelihood than the WLS, OP and shifted Poisson alternatives. However, the simpler shifted Poisson method yielded comparable bias-variance performance to the saddle-point method in the simulations. The new methods offer improved image reconstruction in PET through more realistic statistical modeling, yet with negligible increase in computation time over the conventional OP method.},
	Author = {Yavuz, M. and Fessler, J. A.},
	Date-Added = {2014-12-17 16:45:34 +0100},
	Date-Modified = {2014-12-17 16:46:34 +0100},
	Journal = {Medical Image Analysis},
	Number = {4},
	Pages = {369---378},
	Title = {Statistical image reconstruction methods for randoms-precorrected {PET} scans},
	Volume = {2},
	Year = {1998}}

@article{SnHeLaFaWh95,
	Abstract = {Data acquired with a CCD camera are modeled as an additive Poisson--Gaussian mixture, with the Poisson component representing cumulative counts of object-dependent photoelectrons, object-independent photoelectrons, bias electrons, and thermoelectrons and the Gaussian component representing readout noise. Two methods are examined for compensating for readout noise.

One method relies on approximating the Gaussian readout noise by a Poisson noise and then using a modified Richardson--Lucy algorithm to effect the compensation. This method has been used for restoring images acquired with CCD's in the original Wide-Field Planetary Camera aboard the Hubble Space Telescope. The second method directly uses the expectation-maximization algorithm derived for the Poisson--Gaussian mixture data. This requires the determination of the conditional-mean estimate of the Poisson component of the mixture, which is accomplished by the evaluation of a nonlinear function of the data. The second method requires more computation than the first but is more accurate mathematically and yields modest improvements in the quality of the restorations, particularly for fainter objects. As a specific example, we compare the two methods in restorations of images representative of those acquired with that camera; they contain excess blurring that is due to spherical aberration and a rms readout noise level of 13 electrons.},
	Author = {Snyder, D. L. and Helstrom, C. W. and Lanterman, A. D. and Faisal, M. and White, R. L.},
	Date-Added = {2014-12-17 16:42:49 +0100},
	Date-Modified = {2014-12-17 16:44:17 +0100},
	Journal = {Journal of the Optical Society of America A},
	Number = {12},
	Pages = {272--283},
	Title = {Compensation for readout noise in {CCD} images},
	Volume = {12},
	Year = {1995}}

@article{SnHaWh93,
	Abstract = { A model for data acquired with the use of a charge-coupled-device camera is given and is then used for developing a new iterative method for restoring intensities of objects observed with such a camera. The model includes the effects of point spread, photoconversion noise, readout noise, nonuniform flat-field response, nonuniform spectral response, and extraneous charge carriers resulting from bias, dark current, and both internal and external background radiation. An iterative algorithm is identified that produces a sequence of estimates converging toward a constrained maximum-likelihood estimate of the intensity distribution of an imaged object. An example is given for restoring images from data acquired with the use of the Hubble Space Telescope.},
	Author = {Snyder, D. L. and Hammoud, A. M. and White, R. L.},
	Date-Added = {2014-12-17 16:40:50 +0100},
	Date-Modified = {2014-12-17 16:42:10 +0100},
	Journal = {Journal of the Optical Society of America A},
	Number = {5},
	Pages = {1014---1023},
	Title = {Image recovery from data acquired with a charge-coupled-device camera},
	Volume = {10},
	Year = {1993}}

@article{LaWhWi07,
	Abstract = {Statistical image reconstruction (SR) algorithms have the potential to significantly reduce x-ray CT image artefacts because they use a more accurate model than conventional filtered backprojection and can incorporate effects such as noise, incomplete data and nonlinear detector response. Most SR algorithms assume that the CT detectors are photon-counting devices and generate Poisson-distributed signals. However, actual CT detectors integrate energy from the x-ray beam and exhibit compound Poisson-distributed signal statistics. This study presents the first assessment of the impact on image quality of the resultant mismatch between the detector and signal statistics models assumed by the sinogram data model and the reconstruction algorithm. A 2D CT projection simulator was created to generate synthetic polyenergetic transmission data assuming (i) photon-counting with simple Poisson-distributed signals and (ii) energy-weighted detection with compound Poisson-distributed signals. An alternating minimization (AM) algorithm was used to reconstruct images from the data models (i) and (ii) for a typical abdominal scan protocol with incident particle fluence levels ranging from 105 to 1.6 × 106 photons/detector. The images reconstructed from data models (i) and (ii) were compared by visual inspection and image-quality figures of merit. The reconstructed image quality degraded significantly when the means were mismatched from the assumed model. However, if the signal means are appropriately modified, images from data models (i) and (ii) did not differ significantly even when SNR is very low. While data-mean mismatches characteristic of the difference between particle-fluence and energy-fluence transmission can cause significant streaking and cupping artefacts, the mismatch between the actual and assumed CT detector signal statistics did not significantly degrade image quality once systematic data means mismatches were corrected.
},
	Author = {Lasio, G. M. and Whiting, B. R. and Williamson, J. F.},
	Date-Added = {2014-12-17 16:36:28 +0100},
	Date-Modified = {2014-12-17 16:37:34 +0100},
	Journal = {Physics in Medicine and Biology},
	Number = {8},
	Pages = {2247---2266},
	Title = {Statistical reconstruction for x-ray computed tomography using energy-integrating detectors},
	Volume = {52},
	Year = {2007}}

@inproceedings{ElFe03,
	Abstract = {We report a novel approach for statistical image reconstruction in X-ray CT. Statistical image reconstruction depends on maximizing a likelihood derived from a statistical model for the measurements. Traditionally, the measurements are assumed to be statistically Poisson, but more recent work has argued that CT measurements actually follow a compound Poisson distribution due to the polyenergetic nature of the X-ray source. Unlike the Poisson distribution, compound Poisson statistics have a complicated likelihood that impedes direct use of statistical reconstruction. Using a generalization of the saddle-point integration method, we derive an approximate likelihood for use with iterative algorithms. In its most realistic form, the approximate likelihood we derive accounts for polyenergetic X-rays and poisson light statistics in the detector scintillator, and can be extended to account for electronic additive noise. The approximate likelihood is closer to the exact likelihood than is the conventional Poisson likelihood, and carries the promise of more accurate reconstruction, especially in low X-ray dose situations.},
	Author = {Elbakri, A. I. and Fessler, J. A.},
	Booktitle = {Medical Imaging 2003: Image Processing},
	Date-Added = {2014-12-17 16:34:20 +0100},
	Date-Modified = {2014-12-17 16:36:15 +0100},
	Pages = {1839--1850},
	Series = {Proceedings of SPIE},
	Title = {Efficient and accurate likelihood for iterative image reconstruction in X-ray computed tomography},
	Volume = {5032},
	Year = {2003}}

@inproceedings{Wh02,
	Abstract = {Probability distribution functions (pdfs) of x-ray computed tomography (CT) signals form the basis for statistical reconstruction algorithms and for noise-simulation experiments. The conventional model for pdf's assumes a quanta-counting process obeying a discrete Poisson distribution. In reality, CT scanners employ energy-integrating sensors detecting a polyenergetic X-ray beam, with data quantized for digital reconstructions. A model was developed for the CT signal consisting of an energy spectrum of x-ray quanta (individually obeying Poisson statistics), contributing an incremental signal, proportional to their energy, to an analog-to-digital converter. Using a moment generating function approach, the pdf is shown to be a Compound Poisson process that is functionally dependent on the x-ray energy spectrum, flux level, and quantization step size. Pdfs were computed numerically by a Fourier transform of the characteristic function and compared to experimental pdfs collected from phantom scans. For exposures encountered in normal clinical usage, the pdf is similar to the conventional Gaussian approximation, with rescaling for quantization and polyenergetic spectra. For low intensities, the deviation from the conventional photon-counting (Poisson) model is significant and may have implications for statistical reconstruction algorithms.},
	Author = {Whiting, B. R.},
	Booktitle = {Medical Imaging 2002: Physics of Medical Imaging},
	Date-Added = {2014-12-17 16:31:41 +0100},
	Date-Modified = {2014-12-17 16:33:58 +0100},
	Pages = {53--60},
	Series = {Proceedings of SPIE},
	Title = {Signal statistics in x-ray computed tomography},
	Volume = {4682},
	Year = {2002}}

@inproceedings{MaBaPoSa09,
	Abstract = {Sparse coding---that is, modelling data vectors as sparse linear combinations of basis elements---is widely used in machine learning, neuroscience, signal processing, and statistics. This paper focuses on learning the basis set, also called dictionary, to adapt it to specific data, an approach that has recently proven to be very effective for signal reconstruction and classification in the audio and image processing domains. This paper proposes a new online optimization algorithm for dictionary learning, based on stochastic approximations, which scales up gracefully to large datasets with millions of training samples. A proof of convergence is presented, along with experiments with natural images demonstrating that it leads to faster performance and better dictionaries than classical batch algorithms for both small and large datasets.
},
	Author = {Mairal, J. and Bach, F. and Ponce, J. and Sapiro, G.},
	Booktitle = {Proceedings of the International Conference on Machine Learning (ICML)},
	Date-Added = {2014-12-17 16:11:34 +0100},
	Date-Modified = {2014-12-17 16:13:36 +0100},
	Pages = {689--696},
	Title = {Online dictionary learning for sparse coding},
	Year = {2009}}

@article{ElAh06,
	Abstract = {We address the image denoising problem, where zero-mean white and homogeneous Gaussian additive noise is to be removed from a given image. The approach taken is based on sparse and redundant representations over trained dictionaries. Using the K-SVD algorithm, we obtain a dictionary that describes the image content effectively. Two training options are considered: using the corrupted image itself, or training on a corpus of high-quality image database. Since the K-SVD is limited in handling small image patches, we extend its deployment to arbitrary image sizes by defining a global image prior that forces sparsity over patches in every location in the image. We show how such Bayesian treatment leads to a simple and effective denoising algorithm. This leads to a state-of-the-art denoising performance, equivalent and sometimes surpassing recently published leading alternative denoising methods
},
	Author = {Elad, M. and Aharon, M.},
	Date-Added = {2014-12-17 16:03:48 +0100},
	Date-Modified = {2014-12-17 16:04:43 +0100},
	Journal = {IEEE Transactions on Image Processing},
	Number = {12},
	Pages = {3736---3745},
	Title = {Image denoising via sparse and redundant representations over learned dictionaries},
	Volume = {54},
	Year = {2006}}

@inproceedings{AwDi12,
	Abstract = {Image reconstruction using compressed sensing relies on sparse representations of signals in some dictionary. Current state-of-the-art dictionary-learning methods are designed for spatial images and fail to systematically generalize to dynamic imaging scenarios where the spatiotemporal data, and thereby the spatiotemporal dictionary atoms, exhibit joint coherence in space and time leading to low rank. This paper proposes a novel method for learning low-rank spatiotemporal dictionaries. While leading compressed-sensing reconstruction methods employ either l1 analysis or synthesis approaches using mathematical frames (e.g. overcomplete wavelets), approaches using dictionary learning (very recent) ignore the frame-based l1-sparsity constraints. This paper proposes a novel method combining frame-based l1 analysis with spatiotemporal-dictionary based sparsity (related to l1 synthesis). The results demonstrate improved reconstructions, on simulated and clinical highly-undersampled dynamic images, using the combined approach.
},
	Author = {Awate, S.P. and DiBella, E.V.R.},
	Booktitle = {2012 9th IEEE International Symposium on Biomedical Imaging (ISBI)},
	Date-Added = {2014-12-17 15:57:25 +0100},
	Date-Modified = {2014-12-17 15:58:21 +0100},
	Pages = {318--321},
	Title = {Spatiotemporal dictionary learning for undersampled dynamic {MRI} reconstruction via joint frame-based and dictionary-based sparsity},
	Year = {2012}}

@article{LiCiCaMuBuMyMiSo08,
	Abstract = {This paper reviews recent advances in 4D medical imaging (4DMI) and 4D radiation therapy (4DRT), which study, characterize, and minimize patient motion during the processes of imaging and radiotherapy. Patient motion is inevitably present in these processes, producing artifacts and uncertainties in target (lesion) identification, delineation, and localization. 4DMI includes time-resolved volumetric CT, MRI, PET, PET/CT, SPECT, and US imaging. To enhance the performance of these volumetric imaging techniques, parallel multi-detector array has been employed for acquiring image projections and the volumetric image reconstruction has been advanced from the 2D to the 3D tomography paradigm. The time information required for motion characterization in 4D imaging can be obtained either prospectively or retrospectively using respiratory gating or motion tracking techniques. The former acquires snapshot projections for reconstructing a motion-free image. The latter acquires image projections continuously with an associated timestamp indicating respiratory phases using external surrogates and sorts these projections into bins that represent different respiratory phases prior to reconstructing the cyclical series of 3D images. These methodologies generally work for all imaging modalities with variations in detailed implementation. In 4D CT imaging, both multi-slice CT (MSCT) and cone-beam CT (CBCT) are applicable in 4D imaging. In 4D MR imaging, parallel imaging with multi-coil-detectors has made 4D volumetric MRI possible. In 4D PET and SPECT, rigid and non-rigid motions can be corrected with aid of rigid and deformable registration, respectively, without suffering from low statistics due to signal binning. In 4D PET/CT and SPECT/CT, a single set of 4D images can be utilized for motion-free image creation, intrinsic registration, and attenuation correction. In 4D US, volumetric ultrasonography can be employed to monitor fetal heart beating with relatively high temporal resolution. 4DRT aims to track and compensate for target motion during radiation treatment, minimizing normal tissue injury, especially critical structures adjacent to the target, and/or maximizing radiation dose to the target. 4DRT requires 4DMI, 4D radiation treatment planning (4D RTP), and 4D radiation treatment delivery (4D RTD). Many concepts in 4DRT are borrowed, adapted and extended from existing image-guided radiation therapy (IGRT) and adaptive radiation therapy (ART). The advantage of 4DRT is its promise of sparing additional normal tissue by synchronizing the radiation beam with the moving target in real-time. 4DRT can be implemented differently depending upon how the time information is incorporated and utilized. In an ideal situation, the motion adaptive approach guided by 4D imaging should be applied to both RTP and RTD. However, until new automatic planning and motion feedback tools are developed for 4DRT, clinical implementation of ideal 4DRT will meet with limited success. However, simplified forms of 4DRT have been implemented with minor modifications of existing planning and delivery systems. The most common approach is the use of gating techniques in both imaging and treatment, so that the planned and treated target localizations are identical. In 4D planning, the use of a single planning CT image, which is representative of the statistical respiratory mean, seems preferable. In 4D delivery, on-site CBCT imaging or 3D US localization imaging for patient setup and internal fiducial markers for target motion tracking can significantly reduce the uncertainty in treatment delivery, providing improved normal tissue sparing. Most of the work on 4DRT can be regarded as a proof-of-principle and 4DRT is still in its early stage of development.},
	Author = {Li, G. and Citrin, D. and Camphausen, K. and Mueller, B. and Burman, C. and Mychalczak, B. and Miller, R. W. and Song, Y.},
	Date-Added = {2014-12-17 15:29:20 +0100},
	Date-Modified = {2014-12-17 15:29:20 +0100},
	Journal = {Technology in Cancer Research \& Treatment},
	Number = {1},
	Pages = {67--81},
	Title = {Advances in {4D} medical imaging and {4D} radiation therapy},
	Volume = {7},
	Year = {2008}}

@article{Le08,
	Abstract = {Positron emission tomography (PET) is a tool for metabolic imaging that has been utilized since the earliest days of nuclear medicine. A key component of such imaging systems is the detector modules---an area of research and development with a long, rich history. Development of detectors for PET has often seen the migration of technologies, originally developed for high energy physics experiments, into prototype PET detectors. Of the many areas explored, some detector designs go on to be incorporated into prototype scanner systems and a few of these may go on to be seen in commercial scanners. There has been a steady, often very diverse development of prototype detectors, and the pace has accelerated with the increased use of PET in clinical studies (currently driven by PET/CT scanners) and the rapid proliferation of pre-clinical PET scanners for academic and commercial research applications. Most of these efforts are focused on scintillator-based detectors, although various alternatives continue to be considered. For example, wire chambers have been investigated many times over the years and more recently various solid-state devices have appeared in PET detector designs for very high spatial resolution applications. But even with scintillators, there have been a wide variety of designs and solutions investigated as developers search for solutions that offer very high spatial resolution, fast timing, high sensitivity and are yet cost effective. In this review, we will explore some of the recent developments in the quest for better PET detector technology.
},
	Author = {Lewellen, T. K.},
	Date-Added = {2014-12-17 15:29:13 +0100},
	Date-Modified = {2014-12-17 15:29:13 +0100},
	Journal = {Physics in Medicine and Biology},
	Number = {17},
	Pages = {R287--R317},
	Title = {Recent developments in {PET} detector technology},
	Volume = {53},
	Year = {2008}}

@article{HoHa09,
	Abstract = {The l1 minimization problem has been studied extensively in the past few years. Recently, there has been a growing interest in its application for inverse problems. Most studies have concentrated in devising ways for sparse representation of a solution using a given prototype dictionary. Very few studies have addressed the more challenging problem of optimal dictionary construction, and even these were primarily devoted to the simplistic sparse coding application. In this paper, sensitivity analysis of the inverse solution with respect to the dictionary is presented. This analysis reveals some of the salient features and intrinsic difficulties which are associated with the dictionary design problem. Equipped with these insights, we propose an optimization strategy that alleviates these hurdles while utilizing the derived sensitivity relations for the design of a locally optimal dictionary. Our optimality criterion is based on local minimization of the Bayesian risk, given a set of training models. We present a mathematical formulation and an algorithmic framework to achieve this goal. The proposed framework offers the design of dictionaries for inverse problems that incorporate non-trivial, non-injective observation operators, where the data and the recovered parameters may reside in different spaces. We test our algorithm and show that it yields improved dictionaries for a diverse set of inverse problems in geophysics and medical imaging.},
	Author = {Horesh, L. and Haber, E.},
	Date-Added = {2014-12-17 15:29:08 +0100},
	Date-Modified = {2014-12-17 15:29:08 +0100},
	Journal = {Inverse Problems},
	Pages = {095009 (20pp)},
	Title = {Sensitivity computation of the $\ell_1$ minimization problem and its application to dictionary design of ill-posed problems},
	Volume = {25},
	Year = {2009}}

@article{StElThRoDeShWh05,
	Abstract = {The use of multifocal-plane, time-lapse recordings of living specimens has allowed investigators to visualize dynamic events both within ensembles of cells and individual cells. Recordings of such four- dimensional(4D) data from digital optical sectioning microscopy produce very large data sets. We describe a wavelet-based data compression algorithm that capitalizes on the inherent redunancies within multidimen- sional data to achieve higher compression levels than can be obtained from single images. The algorithm will permit remote users to roam through large 4D data sets using communication channels of modest bandwidth at high speed. This will allow animation to be used as a powerful aid to visualizing dynamic changes in three-dimensional structures.},
	Author = {Stefansson, N. H and Eliceiri, K. W. and Thomas, C. F. and Ron, A. and DeVore, R. and Sharpley, R. and White, J. G.},
	Date-Added = {2014-12-17 15:29:00 +0100},
	Date-Modified = {2014-12-17 15:29:00 +0100},
	Journal = {Microscopy and Microanalysis},
	Number = {9---17},
	Title = {Wavelet Compression of Three-Dimensional Time-Lapse Biological Image Data},
	Volume = {11},
	Year = {2005}}

@conference{ZhVaWaWa08,
	Abstract = {A novel data structure is proposed for magnitude-ordering 4D wavelet coefficients of wavelet-based multiview video coding. This data structure consists of temporal-view 2D zerotree data structures followed by spatial 2D zerotree data structures. Based on this 4D data structure, an algorithm is developed for the coding of 4D wavelet coefficients. Experiment results confirm that the proposed coding algorithm outperforms conventional algorithms that do not use the 4D zerotree data structure.
},
	Author = {Zhang, L. and Vazquez, C. and Wa, J. T. and Wang, D.},
	Booktitle = {2008 IEEE International Conference on Multimedia and Expo, June 23 2008-April 26 2008 in Hannover},
	Date-Added = {2014-12-17 15:28:54 +0100},
	Date-Modified = {2014-12-17 15:28:54 +0100},
	Pages = {737--740},
	Title = {Zerotree data structure for {4D} wavelet coefficient coding},
	Year = {2008}}

@conference{GaFeTrKa06,
	Abstract = {In this paper, a novel framework for scalable multi-view video coding is described. A well known wavelet based scalable coding scheme for single-view video sequences has been adopted and extended to match the specific needs of scalable multi-view video coding. Motion compensated temporal filtering (MCTF) is applied to each video sequence of each camera. The use of a wavelet lifting structure guarantees perfect invertibility of this step, and as a consequence of its open-loop architecture, SNR and temporal scalability are attained. Correlations between the temporal subbands of adjacent cameras are reduced by a novel disparity compensated view filtering (DCVF), method which is also lifting based and open-loop to enable view scalability. Spatial scalability and entropy coding are achieved by the JPEG2000 spatial wavelet transform and EBCOT coding, respectively. Rate allocation along the temporal-view-filtered subbands is done by means of an RD-optimal algorithm. Experimental results show the high scaling capability in terms of SNR, temporal and view scalability
},
	Author = {Garbas, J.-U. and Fecker, U. and Troger, T. and Kaup, A.},
	Booktitle = {2006 IEEE 8th Workshop on Multimedia Signal Processing, 3--6 October 2006 in Victoria, BC},
	Date-Added = {2014-12-17 15:28:46 +0100},
	Date-Modified = {2014-12-17 15:28:46 +0100},
	Pages = {54--58},
	Title = {{4D} Scalable Multi-View Video Coding Using Disparity Compensated View Filtering and Motion Compensated Temporal Filtering},
	Year = {2006}}

@inproceedings{ZeJaUnHu01,
	Abstract = {High resolution multidimensional image data yield huge datasets. For compression and analysis, 2D approaches are often used, neglecting the information coherence in higher dimensions, which can be exploited for improved compression. We designed a wavelet compression algorithm suited for data of arbitrary dimensions, and assessed its ability for compression of 4D medical images. Basically, separable wavelet transforms are done in each dimension, followed by quantization and standard coding. Results were compared with conventional 2D wavelet. We found that in 4D heart images, this algorithm allowed high compression ratios, preserving diagnostically important image features. For similar image quality, compression ratios using the 3D/4D approaches were typically much higher (2-4 times per added dimension) than with the 2D approach. For low-resolution images created with the requirement to keep predefined key diagnostic information (contractile function of the heart), compression ratios up to 2000 could be achieved. Thus, higher-dimensional wavelet compression is feasible, and by exploitation of data coherence in higher image dimensions allows much higher compression than comparable 2D approaches. The proven applicability of this approach to multidimensional medical imaging has important implications especially for the fields of image storage and transmission and, specifically, for the emerging field of telemedicine.},
	Author = {Zeng, L. and Jansen, C. and Unser, M. A. and Hunziker, P.},
	Booktitle = {SPIE Proceedings. Wavelets: Applications in Signal and Image Processing IX, 427 (December 5, 2001)},
	Date-Added = {2014-12-17 15:28:41 +0100},
	Date-Modified = {2014-12-17 15:28:41 +0100},
	Editor = {Laine, A. F. and Unser, M. A. and Aldroubi, A.},
	Pages = {427--433},
	Title = {Extension of wavelet compression algorithms to {3D} and {4D} image data: exploitation of data coherence in higher dimensions allows very high compression ratios},
	Volume = {4478},
	Year = {2001}}

@inproceedings{VaGoVaVaPi12,
	Abstract = {In computerized tomography, it is important to reduce the image noise without increasing the acquisition dose. Extensive research has been done into total variation minimization for image denoising and sparse-view reconstruction. However, TV minimization methods show superior denoising performance for simple images (with little texture), but result in texture information loss when applied to more complex images. Since in medical imaging, we are often confronted with textured images, it might not be beneficial to use TV. Our objective is to find a regularization term outperforming TV for sparse-view reconstruction and image denoising in general. A recent efficient solver was developed for convex problems, based on a split-Bregman approach, able to incorporate regularization terms different from TV. In this work, a proof-of-concept study demonstrates the usage of the discrete shearlet transform as a sparsifying transform within this solver for CT reconstructions. In particular, the regularization term is the 1-norm of the shearlet coefficients. We compared our newly developed shearlet approach to traditional TV on both sparse-view and on low-count simulated and measured preclinical data. Shearlet-based regularization does not outperform TV-based regularization for all datasets. Reconstructed images exhibit small aliasing artifacts in sparse-view reconstruction problems, but show no staircasing effect. This results in a slightly higher resolution than with TV-based regularization.},
	Author = {Vandeghinste, B. and Goossens, B. and Van Holen, R. and Vanhove, C. and Pizurica, A. and Vandenberghe, S. and Staelens, S.},
	Booktitle = {Medical Imaging 2012: Physics of Medical Imaging, 83133I (February 23, 2012)},
	Date-Added = {2014-12-17 15:28:29 +0100},
	Date-Modified = {2014-12-17 15:28:29 +0100},
	Organization = {SPIE},
	Pages = {83133I-83133I-7},
	Series = {Proceedings of SPIE},
	Title = {Iterative {CT} reconstruction using shearlet-based regularization},
	Volume = {8313},
	Year = {2012}}

@conference{SeThKhMo12,
	Abstract = {Compressive sensing (CS) technique addresses the issue of compressing the sparse signal with a rate below Nyquist rate of sampling. For medical images there are always issues of acquisition time and compression, the compressive sensing is found to be a better technique that works in a manner that it first acquires samples less than signal dimensionality and reconstructs the same signal. In this paper Wavelet transform is applied along with compressive sensing on CT images. Three various measurements (for three compression ratio values) have been taken and calculated PSNR, CoC, and RMSE. As measurements are increased PSNR, CoC and visual quality increases and RMSE decreases. The main observation is that only 60% measurements can reproduce image with PSNR of more than 25 dB and with CoC more than 0.99.
},
	Author = {Sevak, M. M. and Thakkar, F. N. and Kher, R. K. and Modi, C. K.},
	Booktitle = {2012 International Conference on Communication Systems and Network Technologies (CSNT), 11-13 May 2012},
	Date-Added = {2014-12-17 15:28:21 +0100},
	Date-Modified = {2014-12-17 15:28:21 +0100},
	Pages = {138--142},
	Title = {{CT} Image Compression Using Compressive Sensing and Wavelet Transform},
	Year = {2012}}

@article{ShElZi13,
	Abstract = {We propose a direct nonlinear reconstruction algorithm for Computed Tomography (CT), designed to handle low-dose measurements. It involves the Filtered Back-Projection and adaptive non-linear filtering in both the projection and the image domains. The filter is an extension of the learned shrinkage method by Hel-Or and Shaked to the case of indirect observations. The shrinkage functions are learned using a training set of reference CT images. The optimization is performed with respect to an error functional in the image domain, that combines the Mean Square Error with a gradient-based penalty, promoting image sharpness. Our numerical simulations indicate that the proposed algorithm can manage well with noisy measurements, allowing a dose-reduction by a factor of 4, while reducing noise and streak artifacts in the FBP reconstruction, comparable to the performance of a statistically-based iterative algorithm.},
	Author = {Shtok, J. and Elad, M. and Zibulevsky, M.},
	Date-Added = {2014-12-17 15:28:12 +0100},
	Date-Modified = {2015-01-29 10:06:11 +0100},
	Journal = {International Journal of Biomedical Imaging},
	Number = {609274},
	Pages = {20pp},
	Title = {Learned Shrinkage Approach For Low-Dose Reconstruction in Computed Tomography},
	Volume = {2013},
	Year = {2013}}

@article{XuYuMoZhHs12,
	Abstract = {Although diagnostic medical imaging provides enormous benefits in the early detection and accuracy diagnosis of various diseases, there are growing concerns on the potential side effect of radiation induced genetic, cancerous and other diseases. How to reduce radiation dose while maintaining the diagnostic performance is a major challenge in the computed tomography (CT) field. Inspired by the compressive sensing theory, the sparse constraint in terms of total variation (TV) minimization has already led to promising results for low-dose CT reconstruction. Compared to the discrete gradient transform used in the TV method, dictionary learning is proven to be an effective way for sparse representation. On the other hand, it is important to consider the statistical property of projection data in the low-dose CT case. Recently, we have developed a dictionary learning based approach for low-dose X-ray CT. In this paper, we present this method in detail and evaluate it in experiments. In our method, the sparse constraint in terms of a redundant dictionary is incorporated into an objective function in a statistical iterative reconstruction framework. The dictionary can be either predetermined before an image reconstruction task or adaptively defined during the reconstruction process. An alternating minimization scheme is developed to minimize the objective function. Our approach is evaluated with low-dose X-ray projections collected in animal and human CT studies, and the improvement associated with dictionary learning is quantified relative to filtered backprojection and TV-based reconstructions. The results show that the proposed approach might produce better images with lower noise and more detailed structural features in our selected cases. However, there is no proof that this is true for all kinds of structures.
},
	Author = {Xu, Q. and Yu, H. and Mou, X. and Zhan, D. and Hsieh, J. and Wang, G.},
	Date-Added = {2014-12-17 15:27:51 +0100},
	Date-Modified = {2014-12-17 15:27:51 +0100},
	Journal = {IEEE Transactions on Medical Imaging},
	Number = {9},
	Pages = {1682--1697},
	Title = {Low-Dose X-ray {CT} Reconstruction via Dictionary Learning},
	Volume = {31},
	Year = {2012}}

@article{Lo11,
	Abstract = {A strategy for the derivation of fast, accurate and stable algorithms for combining the reconstruction and the feature extraction step for solving linear ill-posed problems in just one method is presented. The precomputation of special reconstruction kernels with optimized parameters for the combination of the two tasks allows for fast implementations and better results than separate realizations. The concept of order optimality is generalized to the solution of feature reconstruction and to Banach spaces in order to find criteria for the selection of suitable mollifiers. Results from real data in different tomographic modalities and scanning geometries are presented with the direct calculation of derivatives, as in Canny edge detectors, and the Laplacian of the solution used in many segmentation algorithms. The method works also when the searched-for solution is not smooth or when the data are very noisy. This shows the versatility of the approach.

},
	Author = {Louis, A. K.},
	Date-Added = {2014-12-17 15:27:10 +0100},
	Date-Modified = {2014-12-17 15:27:10 +0100},
	Journal = {Inverse Problems},
	Pages = {065010 (21pp)},
	Title = {Feature Reconstruction in Inverse Problems},
	Volume = {27},
	Year = {2011}}

@article{TaNeCh09,
	Abstract = {Of all available reconstruction methods, statistical iterative reconstruction algorithms appear particularly promising since they enable accurate physical noise modeling. The newly developed compressive sampling/compressed sensing (CS) algorithm has shown the potential to accurately reconstruct images from highly undersampled data. The CS algorithm can be implemented in the statistical reconstruction framework as well. In this study, we compared the performance of two standard statistical reconstruction algorithms (penalized weighted least squares and q-GGMRF) to the CS algorithm. In assessing the image quality using these iterative reconstructions, it is critical to utilize realistic background anatomy as the reconstruction results are object dependent. A cadaver head was scanned on a Varian Trilogy system at different dose levels. Several figures of merit including the relative root mean square error and a quality factor which accounts for the noise performance and the spatial resolution were introduced to objectively evaluate reconstruction performance. A comparison is presented between the three algorithms for a constant undersampling factor comparing different algorithms at several dose levels. To facilitate this comparison, the original CS method was formulated in the framework of the statistical image reconstruction algorithms. Important conclusions of the measurements from our studies are that (1) for realistic neuro-anatomy, over 100 projections are required to avoid streak artifacts in the reconstructed images even with CS reconstruction, (2) regardless of the algorithm employed, it is beneficial to distribute the total dose to more views as long as each view remains quantum noise limited and (3) the total variation-based CS method is not appropriate for very low dose levels because while it can mitigate streaking artifacts, the images exhibit patchy behavior, which is potentially harmful for medical diagnosis.},
	Author = {Tang, J. and Nett, B. E. and Chen, G. H.},
	Date-Added = {2014-12-17 13:36:34 +0100},
	Date-Modified = {2014-12-17 13:36:34 +0100},
	Journal = {Physics in Medicine and Biology},
	Number = {19},
	Pages = {5781--5804},
	Title = {Performance comparison between total variation {(TV)}-based compressed sensing and statistical iterative reconstruction algorithms},
	Volume = {54},
	Year = {2009}}

@book{Sc11,
	Date-Added = {2014-12-17 13:36:08 +0100},
	Date-Modified = {2014-12-17 13:36:08 +0100},
	Editor = {Scherzer, O.},
	Publisher = {Springer Verlag},
	Title = {Handbook of Mathematical Methods in Imaging},
	Year = {2011}}

@incollection{BuOs13,
	Abstract = {Total variation methods and similar approaches based on regularizations with l1-type norms (and seminorms) have become a very popular tool in image processing and inverse problems due to peculiar features that cannot be realized with smooth regularizations. In particular total variation techniques had particular success due to their ability to realize cartoon-type reconstructions with sharp edges. Due to an explosion of new developments in this field within the last decade it is a difficult task to keep an overview of the major results in analysis, the computational schemes, and the application fields. With these lectures we attempt to provide such an overview, of course biased by our major lines of research.

We are focusing on the basic analysis of total variation methods and the extension of the original ROF-denoising model due various application fields. Furthermore we provide a brief discussion of state of-the art computational methods and give an outlook to applications in different disciplines.},
	Author = {Burger, M. and Osher, S.},
	Booktitle = {Level Set and {PDE} Based Reconstruction Methods in Imaging},
	Date-Added = {2014-12-17 13:22:38 +0100},
	Date-Modified = {2014-12-17 13:22:38 +0100},
	Editor = {Burger, M. and Osher, S.},
	Note = {In press},
	Publisher = {Springer Verlag},
	Series = {Lecture Notes in Mathematics},
	Title = {A guide to the {TV} zoo},
	Volume = {2090},
	Year = {2013}}

@book{CrFi13,
	Abstract = {This book provides an accessible introduction to the theory of variable Lebesgue spaces. These spaces generalize the classical Lebesgue spaces by replacing the constant exponent p with a variable exponent p(x). They were introduced in the early 1930s but have become the focus of renewed interest since the early 1990s because of their connection with the calculus of variations and partial differential equations with nonstandard growth conditions, and for their applications to problems in physics and image processing.

The book begins with the development of the basic function space properties. It avoids a more abstract, functional analysis approach, instead emphasizing an hands-on approach that makes clear the similarities and differences between the variable and classical Lebesgue spaces. The subsequent chapters are devoted to harmonic analysis on variable Lebesgue spaces. The theory of the Hardy-Littlewood maximal operator is completely developed, and the connections between variable Lebesgue spaces and the weighted norm inequalities are introduced. The other important operators in harmonic analysis - singular integrals, Riesz potentials, and approximate identities - are treated using a powerful generalization of the Rubio de Francia theory of extrapolation from the theory of weighted norm inequalities. The final chapter applies the results from previous chapters to prove basic results about variable Sobolev spaces.},
	Address = {Basel},
	Author = {Cruz-Uribe, D. V. and Fiorenza, A.},
	Date-Added = {2014-12-17 13:20:01 +0100},
	Date-Modified = {2014-12-17 13:20:01 +0100},
	Publisher = {Springer Verlag},
	Series = {Applied and Numerical Harmonic Analysis},
	Title = {Variable {L}ebesgue Spaces. Foundations and Harmonic Analysis},
	Year = {2013}}

@inproceedings{MaStHe13,
	Abstract = {Dose reduction in X-ray Computed Tomography (CT) is of high practical relevance. Compressed Sensing allows for efficient under-sampling while still achieving an acceptable image quality. Especially Total Variation (TV) regularization obtains accurate, robust and stable results. However, it often suffers from the loss of fine structures and stair-casing artifacts. In order to overcome these limitations, we propose a generalization of TV by higher order derivatives. We demonstrate in this paper that both stair-casing and the loss of small structures in TV-based iterative tomographic reconstructions can be overcome.},
	Author = {Maurice, D. and Stsepankou, D. and Hesser , J.},
	Booktitle = {Fully Three-Dimensional Image Reconstruction in Radiology and Nuclear Medicine (Fully3D 2013), Lake Tahoe, CA, USA},
	Date-Added = {2014-12-17 13:18:33 +0100},
	Date-Modified = {2014-12-17 13:19:27 +0100},
	Pages = {134--137},
	Title = {{CT} Reconstruction from Few-Views by Higher Order Adaptive Weighted Total Variation Minimization},
	Year = {2013}}

@inproceedings{RiBeWuMaFaMa13,
	Abstract = {Iterative reconstruction methods with regularization become more and more popular. In the literature, amazing results are reported that are able to reconstruct images from very few views and from trajectories that do not acquire complete data sets such as would be required for analytical reconstruction methods.

A large disadvantage of iterative methods is their high computational demand. Bruder et al. have shown that if the reconstruction is accurate enough, the regularization can be performed in the reconstructed image only which allows for much faster application of the regularization term.

In this paper, we present a heuristic compensation weight that corrects for the loss of mass in a filtered back-projection type reconstruction given a limited angle problem. Although the reconstruction contains artifacts, we show that the application of a bilateral filter in the reconstruction domain is able to recover almost the same signal as a TV-regularized iterative reconstruction. The reconstruction error is reduced from 0.130 to 0.057 which is the same as for the iterative case.},
	Author = {Riess,C. and Berger, M. and Wu, H. and Manhart, M. and Fahrig, R. and Maier, A.},
	Booktitle = {Fully Three-Dimensional Image Reconstruction in Radiology and Nuclear Medicine (Fully3D 2013), Lake Tahoe, CA, USA},
	Date-Added = {2014-12-17 13:12:31 +0100},
	Date-Modified = {2014-12-17 13:16:31 +0100},
	Pages = {341--344},
	Title = {{TV} or not {TV}? {T}hat is the Question},
	Year = {2013}}

@article{BeBrBuMu13,
	Abstract = {In this work we analyze and compare two recent variational models for image denoising and improve their reconstructions by applying a Bregman iteration strategy. One of the standard techniques in image denoising, the ROF-model (cf. Rudin et al. in Physica D 60:259--268, 1992), is well known for recovering sharp edges of a signal or image, but also for producing staircase-like artifacts. In order to overcome these model-dependent deficiencies, total variation modifications that incorporate higher-order derivatives have been proposed (cf. Chambolle and Lions in Numer. Math. 76:167--188, 1997; Bredies et al. in SIAM J. Imaging Sci. 3(3):492--526, 2010). These models reduce staircasing for reasonable parameter choices. However, the combination of derivatives of different order leads to other undesired side effects, which we shall also highlight in several examples. The goal of this paper is to analyze capabilities and limitations of the different models and to improve their reconstructions in quality by introducing Bregman iterations. Besides general modeling and analysis we discuss efficient numerical realizations of Bregman iterations and modified versions thereof. },
	Author = {Benning, M. and Brune, C. and Burger, M. and M{\"u}ller, J.},
	Date-Added = {2014-12-17 13:06:36 +0100},
	Date-Modified = {2014-12-17 13:06:36 +0100},
	Journal = {Journal of Scientific Computing},
	Number = {2--3},
	Pages = {269--310},
	Title = {Higher-Order TV Methods --- Enhancement via {B}regman Iteration},
	Volume = {54},
	Year = {2013}}

@article{LaSaSi09,
	Abstract = {Bayesian solution of an inverse problem for indirect measurement M = AU + e is considered, where U is a function on a domain of R^d. Here A is a smoothing linear operator and e is Gaussian white noise. The data is a realization m_k of the random variable M_k = P_k A U+P_k e, where P_k is a linear, finite dimensional operator related to measurement device. To allow computerized inversion, the unknown is discretized as U_n=T_nU , where T_n is a finite dimensional projection, leading to the computational measurement model M_{kn}=P_k A U_n + P_k e. Bayes formula gives then the posterior distribution
  pi_{kn}(u_n | m_{kn}) ~ Product_n (u_n) exp(-1/2 | m_{kn} - P_kA u_n |_2^2
in R^d, and the mean u_{kn} : = integral u_n pi_{kn}(u_n | m_k) du_n is considered as the reconstruction of U. We discuss a systematic way of choosing prior distributions for all n >n_0>0 by achieving them as projections of a distribution in a infinite-dimensional limit case. Such choice of prior distributions is discretization-invariant in the sense that prior distributions represent the same a priori information for all n and that the mean u_{kn} converges to a limit estimate as k,n tend to infinity. Gaussian smoothness priors and wavelet-based Besov space priors are shown to be discretization invariant. In particular, Bayesian inversion in dimension two with B^1_11 prior is related to penalizing the l^1 norm of the wavelet coefficients of U.

},
	Author = {Lassas, M. and Saksman, E. and Siltanen, S.},
	Date-Added = {2014-12-17 13:05:59 +0100},
	Date-Modified = {2014-12-17 13:05:59 +0100},
	Journal = {Inverse Problems and Imaging},
	Number = {1},
	Pages = {87--122},
	Title = {Discretization-invariant Bayesian inversion and Besov space priors},
	Volume = {3},
	Year = {2009}}

@inproceedings{ShElZi11,
	Abstract = {We propose a sinogram restoration method which consists of a patch-wise non-linear processing, based on a sparsity prior in terms of a learned dictionary. An off-line learning process uses a statistical model of the sinogram noise and minimizes an error measure in the image domain over the training set. The error measure is designed to preserve low-contrast edges for visibility of soft tissues. Our numerical study shows that the algorithm improves on the performance of the standard Filtered Back-Projection algorithm and effectively allows to halve the radiation dose for the same image quality.
},
	Author = {Shtok, J. and Elad, M. and Zibulevsky, M.},
	Booktitle = {2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Prague, Czech Republic, 22-27 May, 2011},
	Date-Added = {2014-12-08 15:12:53 +0100},
	Date-Modified = {2014-12-08 15:15:15 +0100},
	Pages = {569--572},
	Title = {Sparsity-Based Sinogram for Low-Dose Computed Tomography},
	Year = {2011}}

@article{RuBrEl10,
	Abstract = {Sparse and redundant representation modeling of data assumes an ability to describe signals as linear combinations of a few atoms from a pre-specified dictionary. As such, the choice of the dictionary that sparsifies the signals is crucial for the success of this model. In general, the choice of a proper dictionary can be done using one of two ways: i) building a sparsifying dictionary based on a mathematical model of the data, or ii) learning a dictionary to perform best on a training set. In this paper we describe the evolution of these two paradigms. As manifestations of the first approach, we cover topics such as wavelets, wavelet packets, contourlets, and curvelets, all aiming to exploit 1-D and 2-D mathematical models for constructing effective dictionaries for signals and images. Dictionary learning takes a different route, attaching the dictionary to a set of examples it is supposed to serve. From the seminal work of Field and Olshausen, through the MOD, the K-SVD, the Generalized PCA and others, this paper surveys the various options such training has to offer, up to the most recent contributions and structures.
},
	Author = {Rubinstein, R. and Bruckstein, A. M. and Elad, M.},
	Date-Added = {2014-12-08 14:39:43 +0100},
	Date-Modified = {2014-12-08 14:39:43 +0100},
	Journal = {Proceedings of the IEEE},
	Number = {6},
	Pages = {1045--1057},
	Title = {Dictionaries for Sparse Representation Modeling},
	Volume = {98},
	Year = {2010}}

@book{El10,
	Abstract = {The field of sparse and redundant representation modeling has gone through a major revolution in the past two decades. This started with a series of algorithms for approximating the sparsest solutions of linear systems of equations, later to be followed by surprising theoretical results that guarantee these algorithms' performance. With these contributions in place, major barriers in making this model practical and applicable were removed, and sparsity and redundancy became central, leading to state-of-the-art results in various disciplines. One of the main beneficiaries of this progress is the field of image processing, where this model has been shown to lead to unprecedented performance in various applications.

This book provides a comprehensive view of the topic of sparse and redundant representation modeling, and its use in signal and image processing. It offers a systematic and ordered exposure to the theoretical foundations of this data model, the numerical aspects of the involved algorithms, and the signal and image processing applications that benefit from these advancements. The book is well-written, presenting clearly the flow of the ideas that brought this field of research to its current achievements. It avoids a succession of theorems and proofs by providing an informal description of the analysis goals and building this way the path to the proofs. The applications described help the reader to better understand advanced and up-to-date concepts in signal and image processing.
Written as a text-book for a graduate course for engineering students, this book can also be used as an easy entry point for readers interested in stepping into this field, and for others already active in this area that are interested in expanding their understanding and knowledge.

The book is accompanied by a Matlab software package that reproduces most of the results demonstrated in the book. A link to the free software is available on springer.com.},
	Author = {Elad, M.},
	Date-Added = {2014-12-08 14:38:52 +0100},
	Date-Modified = {2014-12-08 14:40:46 +0100},
	Publisher = {Springer-Verlag},
	Title = {Sparse and Redundant Representations: From Theory to Applications in Signal and Image Processing},
	Year = {2010}}

@article{SeTs09,
	Abstract = {Recent work in the development of computerized phantoms has focused on the creation of ideal ldquohybridrdquo models that seek to combine the realism of a patient-based voxelized phantom with the flexibility of a mathematical or stylized phantom. We have been leading the development of such computerized phantoms for use in medical imaging research. This paper will summarize our developments dating from the original four-dimensional (4-D) mathematical cardiac-torso (MCAT) phantom, a stylized model based on geometric primitives, to the current 4-D extended cardiac-torso (XCAT) and mouse whole-body (MOBY) phantoms, hybrid models of the human and laboratory mouse based on state-of-the-art computer graphics techniques. This paper illustrates the evolution of computerized phantoms toward more accurate models of anatomy and physiology. This evolution was catalyzed through the introduction of nonuniform rational b-spline (NURBS) and subdivision (SD) surfaces, tools widely used in computer graphics, as modeling primitives to define a more ideal hybrid phantom. With NURBS and SD surfaces as a basis, we progressed from a simple geometrically based model of the male torso (MCAT) containing only a handful of structures to detailed, whole-body models of the male and female (XCAT) anatomies (at different ages from newborn to adult), each containing more than 9000 structures. The techniques we applied for modeling the human body were similarly used in the creation of the 4-D MOBY phantom, a whole-body model for the mouse designed for small animal imaging research. From our work, we have found the NURBS and SD surface modeling techniques to be an efficient and flexible way to describe the anatomy and physiology for realistic phantoms. Based on imaging data, the surfaces can accurately model the complex organs and structures in the body, providing a level of realism comparable to that of a voxelized phantom. In addition, they are very flexible. Like stylized models, they can easily b- e manipulated to model anatomical variations and patient motion. With the vast improvement in realism, the phantoms developed in our lab can be combined with accurate models of the imaging process (SPECT, PET, CT, magnetic resonance imaging, and ultrasound) to generate simulated imaging data close to that from actual human or animal subjects. As such, they can provide vital tools to generate predictive imaging data from many different subjects under various scanning parameters from which to quantitatively evaluate and improve imaging devices and techniques. From the MCAT to XCAT, we will demonstrate how NURBS and SD surface modeling have resulted in a major evolutionary advance in the development of computerized phantoms for imaging research.},
	Author = {Segars, P. W. and Tsui, B. M. W.},
	Date-Added = {2014-11-26 11:40:29 +0100},
	Date-Modified = {2014-11-26 11:40:29 +0100},
	Journal = {Proceedings of the IEEE},
	Number = {12},
	Pages = {1954--1968},
	Title = {{MCAT} to {XCAT}: {T}he Evolution of {4-D} Computerized Phantoms for Imaging Research},
	Volume = {97},
	Year = {2009}}

@article{GiRuBuJiSc12,
	Abstract = {Respiratory and cardiac motion leads to image degradation in positron emission tomography (PET) studies of the human heart. In this paper we present a novel approach to motion correction based on dual gating and mass-preserving hyperelastic image registration. Thereby, we account for intensity modulations caused by the highly nonrigid cardiac motion. This leads to accurate and realistic motion estimates which are quantitatively validated on software phantom data and carried over to clinically relevant data using a hardware phantom. For patient data, the proposed method is first evaluated in a high statistic (20 min scans) dual gating study of 21 patients. It is shown that the proposed approach properly corrects PET images for dual-cardiac as well as respiratory-motion. In a second study the list mode data of the same patients is cropped to a scan time reasonable for clinical practice (3 min). This low statistic study not only shows the clinical applicability of our method but also demonstrates its robustness against noise obtained by hyperelastic regularization.},
	Author = {Gigengack, F. and Ruthotto, L. and Burger, M. and Jiang, X. and Sch{\"a}fers, K. P.},
	Date-Added = {2014-11-26 11:40:20 +0100},
	Date-Modified = {2015-02-06 15:51:35 +0000},
	Journal = {IEEE Transactions on Medical Imaging},
	Number = {3},
	Pages = {698--712},
	Title = {Motion correction in dual gated cardiac {PET} using mass preserving image registration},
	Volume = {31},
	Year = {2012}}

@article{Sa06,
	Abstract = {Ths paper examines several applications of deformable registration algorithms in the field of image-guided radiotherapy. The first part focuses on the description of input and output of deformable registration algorithms, with a brief review of conventional and most current methods. The typical applications of deformable registration are then reviewed on the basis of four practical examples. The first two sets of examples deal with the fusion of images obtained from the same patient (inter-fraction registration), with time intervals of several days between each image. The other two examples deal with the fusion of images obtained in immediate sequence (intra-fraction registration); in this case, the focus is the displacement during image acquisition or patient treatment (mainly due to respiratory movement), with time intervals in the order of magnitude of tenths of seconds. Finally, the registration of images of different patients (inter-patient registration) is also discussed. In conclusion, deformable registration has become a fundamental tool for image analysis in radiotherapy. Although extensive validation of the numerous existing methods is required before extending its clinical use, deformable registration is expected to become a standard methodology in the treatment planning systems in the near future.},
	Author = {Sarrut, D.},
	Date-Added = {2014-11-26 11:40:13 +0100},
	Date-Modified = {2014-11-26 11:40:13 +0100},
	Journal = {Zeitschrift f{\"u}r Medizinische Physik},
	Number = {4},
	Pages = {285--297},
	Title = {Deformable registration for image-guided radiation therapy},
	Volume = {16},
	Year = {2006}}

@article{GaFaEs11,
	Abstract = {Myocardial perfusion imaging (MPI) using nuclear cardiology techniques has been widely applied in clinical practice because of its well-documented value in the diagnosis and prognosis of coronary artery disease. Industry has developed innovative designs for dedicated cardiac SPECT cameras that constrain the entire detector area to imaging just the heart. New software that recovers image resolution and limits image noise has also been implemented. These SPECT innovations are resulting in shortened study times or reduced radiation doses to patients, promoting easier scheduling, higher patient satisfaction, and, importantly, higher image quality. This article describes these cardiocentric SPECT software and hardware innovations, which provide a strong foundation for the continued success of myocardial perfusion SPECT.},
	Author = {Garcia, E. V. and Faber, T. L. and Esteves, F. P.},
	Date-Added = {2014-11-26 11:40:06 +0100},
	Date-Modified = {2014-11-26 11:40:06 +0100},
	Journal = {Journal of Nuclear Medicine},
	Number = {2},
	Pages = {210--217},
	Title = {Cardiac dedicated ultrafast {SPECT} cameras: new designs and clinical implications.},
	Volume = {52},
	Year = {2011}}

@article{BoSt04,
	Abstract = {We study the inversion of weighted Radon transforms in two dimensions where the weight function has a special form. It was an important breakthrough when R.G. Novikov recently gave an explicit formula for the inverse of this transform (attenuated Radon transform). Here we prove similar results for a somewhat larger class of weight fiunctions using completely different and quite elementary methods.
},
	Author = {Boman, J. and Str{\"o}mberg, J.-O.},
	Date-Added = {2014-11-26 11:39:58 +0100},
	Date-Modified = {2014-11-26 11:39:58 +0100},
	Journal = {The Journal of Geometric Analysis},
	Number = {2},
	Pages = {185--198},
	Title = {Novikov's inversion formula for the attenuated {R}adon transform},
	Volume = {14},
	Year = {2004}}

@article{PeFu11,
	Abstract = {The development of radiation detectors capable of delivering spatial information about gamma-ray interactions was one of the key enabling technologies for nuclear medicine imaging and, eventually, single-photon emission computed tomography (SPECT). The continuous sodium iodide scintillator crystal coupled to an array of photomultiplier tubes, almost universally referred to as the Anger Camera after its inventor, has long been the dominant SPECT detector system. Nevertheless, many alternative materials and configurations have been investigated over the years. Technological advances as well as the emerging importance of specialized applications, such as cardiac and preclinical imaging, have spurred innovation such that alternatives to the Anger Camera are now part of commercial imaging systems. Increased computing power has made it practical to apply advanced signal processing and estimation schemes to make better use of the information contained in the detector signals. In this review we discuss the key performance properties of SPECT detectors and survey developments in both scintillator and semiconductor detectors and their readouts with an eye toward some of the practical issues at least in part responsible for the continuing prevalence of the Anger Camera in the clinic.

},
	Author = {Peterson, T. E. and Furenlid, L. R.},
	Date-Added = {2014-11-25 10:51:19 +0000},
	Date-Modified = {2014-11-25 10:52:53 +0000},
	Journal = {Physics in Medicine and Biology},
	Number = {17},
	Pages = {R145--R182},
	Title = {{SPECT} detectors: the {A}nger Camera and beyond},
	Volume = {56},
	Year = {2011}}

@article{NuDeFeZbBe13,
	Abstract = {There is an increasing interest in iterative reconstruction (IR) as a key tool to improve quality and increase applicability of x-ray CT imaging. IR has the ability to significantly reduce patient dose; it provides the flexibility to reconstruct images from arbitrary x-ray system geometries and allows one to include detailed models of photon transport and detection physics to accurately correct for a wide variety of image degrading effects. This paper reviews discretization issues and modelling of finite spatial resolution, Compton scatter in the scanned object, data noise and the energy spectrum. The widespread implementation of IR with a highly accurate model-based correction, however, still requires significant effort. In addition, new hardware will provide new opportunities and challenges to improve CT with new modelling.

},
	Author = {Nuyts, J. and De Man, B. and Fessler, J. A. and Zbijewski, W. and Beekman, F. J.},
	Date-Added = {2014-11-25 10:48:28 +0000},
	Date-Modified = {2014-11-25 10:51:10 +0000},
	Journal = {Physics in Medicine and Biology},
	Number = {12},
	Pages = {R63--R96},
	Title = {Modelling the physics in the iterative reconstruction for transmission computed tomography},
	Volume = {58},
	Year = {2013}}

@article{ChFeDe13,
	Abstract = {Compensating for the collimator-detector response (CDR) in SPECT is important for accurate quantification. The CDR consists of both a geometric response and a septal penetration and collimator scatter response. The geometric response can be modeled analytically and is often used for modeling the whole CDR if the geometric response dominates. However, for radionuclides that emit medium or high-energy photons such as I-131, the septal penetration and collimator scatter response is significant and its modeling in the CDR correction is important for accurate quantification. There are two main methods for modeling the depth-dependent CDR so as to include both the geometric response and the septal penetration and collimator scatter response. One is to fit a Gaussian plus exponential function that is rotationally invariant to the measured point source response at several source-detector distances. However, a rotationally-invariant exponential function cannot represent the star-shaped septal penetration tails in detail. Another is to perform Monte-Carlo (MC) simulations to generate the depth-dependent point spread functions (PSFs) for all necessary distances. However, MC simulations, which require careful modeling of the SPECT detector components, can be challenging and accurate results may not be available for all of the different SPECT scanners in clinics. In this paper, we propose an alternative approach to CDR modeling. We use a Gaussian function plus a 2-D B-spline PSF template and fit the model to measurements of an I-131 point source at several distances. The proposed PSF-template-based approach is nearly non-parametric, captures the characteristics of the septal penetration tails, and minimizes the difference between the fitted and measured CDR at the distances of interest. The new model is applied to I-131 SPECT reconstructions of experimental phantom measurements, a patient study, and a MC patient simulation study employing the XCAT phantom. The proposed model yields up to a 16.5 and 10.8% higher recovery coefficient compared to the results with the conventional Gaussian model and the Gaussian plus exponential model, respectively.
},
	Author = {Chun, S. Y. and Fessler, J. A. and Dewaraja, Y. K.},
	Date-Added = {2014-11-25 10:38:46 +0000},
	Date-Modified = {2014-11-25 10:40:13 +0000},
	Journal = {IEEE Transactions on Medical Imaging},
	Number = {2},
	Pages = {295--305},
	Title = {Correction for collimator-detector response in {SPECT} using point spread function template},
	Volume = {32},
	Year = {2013}}

@article{BeEiViBoSl93,
	Abstract = {An analytical expression for the point spread function (PSF) and the line spread function (LSF) of a parallel hole gamma camera is presented, for homogeneous media and for photons having mainly Compton interactions in the object. The PSF of scattered photons is described by convolving a zeroth-order modified Bessel function of the second kind with the unscattered PSF, which is approximated by a Gaussian. The complete PSF (scatter plus nonscatter) depends on the source distance (z) and depth (d) of the source in the object. For convex-shaped emitting objects, the dependence of the PSF on the object contour can be incorporated by a simple correction. Thus, a complete mathematical model of the imaging of an activity distribution in a homogeneous medium is obtained. The model has been evaluated for 99mTechnetium line sources by using a LEAP collimator at various energy windows. It is shown that the model is valid to a high accuracy at energy windows=15%, for a large range of values of z and d.

},
	Author = {F. J. Beekman and E. G. J. Eijkman and M. A. Viergever and G. F. Borm and E. T. P. Slijpen},
	Date-Added = {2014-11-25 10:32:19 +0000},
	Date-Modified = {2014-11-25 10:33:35 +0000},
	Journal = {IEEE Transactions in Nuclear Science},
	Month = {February},
	Number = {1},
	Pages = {31--39},
	Title = {Object shape dependent {PSF} model for {SPECT} imaging},
	Volume = {40},
	Year = {1993}}

@article{SePiCaRoLe00,
	Abstract = {One limitation in a practical implementation of statistical iterative image reconstruction is to compute a transition matrix accurately modeling the relationship between projection and image spaces. Detector response function (DRF) in positron emission tomography (PET) is broad and spatially-variant, leading to large transition matrices taking too much space to store. In this work, the authors investigate the effect of simpler DRF models on image quality in maximum likelihood expectation maximization reconstruction. The authors studied 6 cases of modeling projection/image relationship: tube/pixel geometric overlap with tubes centered on detector face; same as previous with tubes centered on DRF maximum; two different fixed-width Gaussian functions centered on DRF maximum weighing tube/pixel overlap; same as previous with a Gaussian of the same spectral resolution as DRF; analytic DRF based on linear attenuation of γ-rays in detector arrays weighing tube/pixel overlap. The authors found that DRF oversimplification may affect visual image quality and image quantification dramatically, including artefact generation. They showed that analytic DRF yielded images of excellent quality for a small animal PET system with long, narrow detectors and generated a transition matrix for 2-D reconstruction that could be easily fitted into the memory of current stand-alone computers

},
	Author = {Selivanov, V. V. and Picard, Y. and Cadorette, J. and Rodrigue, S. and Lecomte, R.},
	Date-Added = {2014-11-25 10:28:44 +0000},
	Date-Modified = {2014-11-25 10:37:58 +0000},
	Journal = {IEEE Transactions in Nuclear Science},
	Number = {3},
	Pages = {1168--1175},
	Title = {Detector response models for statistical iterative image reconstruction in high resolution {PET}},
	Volume = {47},
	Year = {2000}}

@incollection{BaJo11,
	Author = {Bal, G. and Jollivet, A.},
	Booktitle = {Tomography and Inverse Transport Theory},
	Date-Added = {2014-11-25 10:21:19 +0000},
	Date-Modified = {2014-11-25 10:24:50 +0000},
	Editor = {Guillaume, B. and Finch, D. and Kuchment, P. and Schotland, J. and Stefanov, P. and Uhlmann, G.},
	Pages = {13--28},
	Publisher = {American Mathematcial Society},
	Series = {Contemporary Mathematics},
	Title = {Combined source and attenuation reconstructions in {SPECT}},
	Volume = {559},
	Year = {2011}}

@article{GoNo02,
	Abstract = {The image reconstruction process in emission computed tomography (ECT) is an inverse problem for the photon transport equation. For monochromatic emission sources it is closely related to the inversion of the attenuated Radon transform, a nonlinear ill posed inverse problem. Due to its practical importance for medical diagnostics, this problem has been addressed various times. Here we present the theoretical setting of ECT and discuss some new numerical strategies based on regularization techniques. We include experiments to compare some of the numerical approaches.
},
	Author = {Gourion, D. and Noll, D.},
	Date-Added = {2014-11-25 10:18:48 +0000},
	Date-Modified = {2014-11-25 10:19:52 +0000},
	Journal = {Inverse Problems},
	Pages = {1435---1460},
	Title = {The inverse problem of emission tomography},
	Volume = {18},
	Year = {2002}}

@article{HuBuBe11,
	Abstract = {Detection of scattered gamma quanta degrades image contrast and quantitative accuracy of single-photon emission computed tomography (SPECT) imaging. This paper reviews methods to characterize and model scatter in SPECT and correct for its image degrading effects, both for clinical and small animal SPECT. Traditionally scatter correction methods were limited in accuracy, noise properties and/or generality and were not very widely applied. For small animal SPECT, these approximate methods of correction are often sufficient since the fraction of detected scattered photons is small. This contrasts with patient imaging where better accuracy can lead to significant improvement of image quality. As a result, over the last two decades, several new and improved scatter correction methods have been developed, although often at the cost of increased complexity and computation time. In concert with (i) the increasing number of energy windows on modern SPECT systems and (ii) excellent attenuation maps provided in SPECT/CT, some of these methods give new opportunities to remove degrading effects of scatter in both standard and complex situations and therefore are a gateway to highly quantitative single- and multi-tracer molecular imaging with improved noise properties. Widespread implementation of such scatter correction methods, however, still requires significant effort.},
	Author = {Hutton, B. F. and Buvat, I. and Beekman, F. J.},
	Date-Added = {2014-11-25 10:17:13 +0000},
	Date-Modified = {2014-11-25 10:17:59 +0000},
	Journal = {Physics in Medicine and Biology},
	Pages = {R85--R112},
	Title = {Review and current status of {SPECT} scatter correction},
	Volume = {56},
	Year = {2011}}

@article{QiLe06,
	Abstract = {In emission tomography statistically based iterative methods can improve image quality relative to analytic image reconstruction through more accurate physical and statistical modelling of high-energy photon production and detection processes. Continued exponential improvements in computing power, coupled with the development of fast algorithms, have made routine use of iterative techniques practical, resulting in their increasing popularity in both clinical and research environments. Here we review recent progress in developing statistically based iterative techniques for emission computed tomography. We describe the different formulations of the emission image reconstruction problem and their properties. We then describe the numerical algorithms that are used for optimizing these functions and illustrate their behaviour using small scale simulations.
},
	Author = {Qi, J. and Leahy, R. M.},
	Date-Added = {2014-11-25 10:15:20 +0000},
	Date-Modified = {2014-11-25 10:16:04 +0000},
	Journal = {Physics in Medicine and Biology},
	Pages = {R541--R578},
	Title = {Iterative reconstruction techniques in emission computed tomography},
	Volume = {51},
	Year = {2006}}

@article{GuReSiMaBu10,
	Abstract = {The very nature of nuclear medicine, the visual representation of injected radiopharmaceuticals, implies imaging of dynamic processes such as the uptake and wash-out of radiotracers from body organs. For years, nuclear medicine has been touted as the modality of choice for evaluating function in health and disease. This evaluation is greatly enhanced using single photon emission computed tomography (SPECT), which permits three-dimensional (3D) visualization of tracer distributions in the body. However, to fully realize the potential of the technique requires the imaging of in vivo dynamic processes of flow and metabolism. Tissue motion and deformation must also be addressed. Absolute quantification of these dynamic processes in the body has the potential to improve diagnosis. This paper presents a review of advancements toward the realization of the potential of dynamic SPECT imaging and a brief history of the development of the instrumentation. A major portion of the paper is devoted to the review of special data processing methods that have been developed for extracting kinetics from dynamic cardiac SPECT data acquired using rotating detector heads that move as radiopharmaceuticals exchange between biological compartments. Recent developments in multi-resolution spatiotemporal methods enable one to estimate kinetic parameters of compartment models of dynamic processes using data acquired from a single camera head with slow gantry rotation. The estimation of kinetic parameters directly from projection measurements improves bias and variance over the conventional method of first reconstructing 3D dynamic images, generating time-activity curves from selected regions of interest and then estimating the kinetic parameters from the generated time--activity curves. Although the potential applications of SPECT for imaging dynamic processes have not been fully realized in the clinic, it is hoped that this review illuminates the potential of SPECT for dynamic imaging, especially in light of new developments that enable measurement of dynamic processes directly from projection measurements.},
	Author = {Gullberg, G. T. and Reutter, B. W. and Sitek, A. and Maltz, J. S. and Budinger, T. F.},
	Date-Added = {2014-11-25 10:12:11 +0000},
	Date-Modified = {2014-11-25 10:13:52 +0000},
	Journal = {Physics in Medicine and Biology},
	Pages = {R111--R191},
	Title = {Dynamic single photon emission computed tomography -- basic principles and cardiac applications},
	Volume = {55},
	Year = {2010}}

@article{ErBuPrThHu12,
	Abstract = {Accurate quantification in PET and SPECT requires correction for a number of physical factors, such as photon attenuation, Compton scattering and random coincidences (in PET). Another factor affecting quantification is the limited spatial resolution. While considerable effort has gone into development of routine correction techniques for the former factors, less attention has been paid to the latter. Spatial resolution-related effects, referred to as `partial volume effects' (PVEs), depend not only on the characteristics of the imaging system but also on the object and activity distribution. Spatial and/or temporal variations in PVE can often be confounding factors. Partial volume correction (PVC) could in theory be achieved by some kind of inverse filtering technique, reversing the effect of the system PSF. However, these methods are limited, and usually lead to noise-amplification or image artefacts. Some form of regularization is therefore needed, and this can be achieved using information from co-registered anatomical images, such as CT or MRI. The purpose of this paper is to enhance understanding of PVEs and to review possible approaches for PVC. We also present a review of clinical applications of PVC within the fields of neurology, cardiology and oncology, including specific examples.},
	Author = {Erlandsson, K. and Buvat, I. and Pretorius, H. P. and Thomas, B. A. and Hutton, B. F.},
	Date-Added = {2014-11-25 10:09:53 +0000},
	Date-Modified = {2014-11-25 10:11:27 +0000},
	Journal = {Physics in Medicine and Biology},
	Pages = {R119---R159},
	Title = {A review of partial volume correction techniques for emission tomography and their applications in neurology, cardiology and oncology},
	Volume = {57},
	Year = {2012}}

@phdthesis{Ko10,
	Abstract = {Positron Emission Tomography (PET) plays an important role in medical imaging. In contrast to techniques like Computerized Tomography (CT) providing information about the anatomy of the patients body, PET can be used to study the metabolism of the patient. Often both, functional and morphological, devices are combined in a single system in order to benefit from both techniques.

Although PET is already used in daily routine, improvements in the underlying mathematical model are still possible. In general, the X-ray transform is used the analytical model for PET. The main drawback of this model in connection with PET is the fact that it is only suitable for particles that travel along straight rays. Scattered particles, which are a major problem in quantitative PET, are not considered by this model.

In order to use the straight line model the scatter fraction is usually estimated based on physical considerations and subtracted from the measured data. After this correction the X-ray transform can be assumed as the correct model for PET. Obviously, this data correction leads to reduced statistics and strictly speaking the model is still somehow wrong for PET.

In this work a new model is presented that includes, in addition to the particles travelling along straight rays, all possible measurements resulting from scattered events. It is shown in particular that this mathematical model, which is based on the Boltzmann equation, fits exactly to the physical processes behind PET. A detailed analysis using a Neumann series approach leading to Sobolev space estimates is performed in order to show that the new model can be seen as a perturbation of the X-ray transform. Additionally, this approach allows to recover the unknown activity distribution using the inversion formulas for the X-ray transform.

The scatter estimate of the new model can be used as input for standard scatter correction methods. Next the the classical approaches it is possible to modify existing reconstruction algorithms by projecting along volumes resulting from the scatter distribution. This reconstruction technique contains a precise but time-consuming preprocessing step where all projection volumes for all possible lines of responses are precalculated. An evaluation of this approach on simulated 2D data proofed its superiority in case of low statistics.},
	Address = {Westf{\"a}lischen Wilhelms-Universit{\"a}t M{\"u}nster},
	Author = {K{\"o}sters, T.},
	Date-Added = {2014-11-25 10:05:06 +0000},
	Date-Modified = {2015-01-05 09:39:53 +0100},
	School = {Fachbereich Mathematik und Informatik der Mathematisch-Naturwissenschaftlichen Fakult{\"a}t},
	Title = {Derivation and Analysis of Scatter Correction Algorithms for Quantitative Positron Emission Tomography},
	Type = {{PhD} thesis},
	Year = {2010}}

@article{LuQiSt14,
	Abstract = {Motivated by recent theoretical results obtained by the third author for the identification problem arising in single-photon emission computerized tomography (SPECT), we propose an adjoint state method for recovering both the source and the attenuation in the attenuated X-ray transform. Our starting point is the transport-equation characterization of the attenuated X-ray transform, and we apply efficient fast sweeping methods to solve static transport equations and adjoint state equations. Numerous examples are presented to demonstrate various features of the identification problem, such as uniqueness and nonuniqueness, stability and instability, and recovery of the wave front set.},
	Author = {Luo, S. and Qian, J. and Stefanov, P.},
	Date-Added = {2014-11-25 09:55:23 +0000},
	Date-Modified = {2014-11-25 09:56:59 +0000},
	Journal = {SIAM Journal on Imaging Sciences},
	Number = {2},
	Pages = {696--715},
	Title = {Adjoint State Method for the Identification Problem in {SPECT}: {R}ecovery of Both the Source and the Attenuation in the Attenuated {X}-Ray Transform},
	Volume = {7},
	Year = {2014}}

@article{KaOuHuDoKa14,
	Abstract = {There has been a rapid expansion of multi-modal imaging techniques in tomography. In biomedical imaging, patients are now regularly imaged using both single photon emission computed tomography (SPECT) and x-ray computed tomography (CT), or using both positron emission tomography and magnetic resonance imaging (MRI). In non-destructive testing of materials both neutron CT (NCT) and x-ray CT are widely applied to investigate the inner structure of material or track the dynamics of physical processes. The potential benefits from combining modalities has led to increased interest in iterative reconstruction algorithms that can utilize the data from more than one imaging mode simultaneously. We present a new regularization term in iterative reconstruction that enables information from one imaging modality to be used as a structural prior to improve resolution of the second modality. The regularization term is based on a modified anisotropic tensor diffusion filter, that has shape-adapted smoothing properties. By considering the underlying orientations of normal and tangential vector fields for two co-registered images, the diffusion flux is rotated and scaled adaptively to image features. The images can have different greyscale values and different spatial resolutions. The proposed approach is particularly good at isolating oriented features in images which are important for medical and materials science applications. By enhancing the edges it enables both easy identification and volume fraction measurements aiding segmentation algorithms used for quantification. The approach is tested on a standard denoising and deblurring image recovery problem, and then applied to 2D and 3D reconstruction problems; thereby highlighting the capabilities of the algorithm. Using synthetic data from SPECT co-registered with MRI, and real NCT data co-registered with x-ray CT, we show how the method can be used across a range of imaging modalities.

},
	Author = {Kazantsev, D. and Ourselin, S. and Hutton, B. F. and Dobson, K. J. and Kaestner, A. P. and Lionheart, W. R. B. and Withers, P. J. and Lee, P. D. and Arridge, S. R.},
	Date-Added = {2014-11-25 09:52:24 +0000},
	Date-Modified = {2014-11-25 09:53:59 +0000},
	Journal = {Inverse Problems},
	Number = {6},
	Pages = {065004},
	Title = {A novel technique to incorporate structural prior information into multi-modal tomographic reconstruction},
	Volume = {30},
	Year = {2014}}

@book{Sc14,
	Abstract = {This book presents new integrated registration and segmentation techniques for pulmonary image analysis. He develops two approaches in which segmentations of specific structures are employed to integrate anatomical and physiological knowledge into the registration algorithm. On the one hand, the sliding motion at the lung boundaries is modeled directly in the registration approach. This specific characteristic of lung physiology plays an important role, because the discontinuous nature of the sliding lung motion cannot be described accurately using common non-linear registration methods. On the other hand, a segmentation of the pulmonary lobes is integrated into the registration method to align the interlobular fissures. Lung lobe registration is challenging due to the extremely low contrast in CT images, which results in unsatisfying results with conventional approaches. The presented approach enables an alignment of the lung lobes by considering the morphological knowledge about the lung provided by integrated segmentations. With the combined segmentation and registration approach he shows that an explicit consideration of morphological and physiological a priori knowledge about the lung can considerably improve registration accuracy and plausibility.

This book provides the reader with a deep insight into the field of image registration and segmentation including a thorough introduction to the mathematical background. Moreover, the problem-specific adaption of registration approaches is demonstrated. Many medical examples illustrate the methods and give the reader an impression of the application of the methods in practice. This book is highly recommended for all readers interested in the registration and segmentation of medical image data.},
	Author = {Schmidt-Richberg, A.},
	Date-Added = {2014-11-25 09:48:02 +0000},
	Date-Modified = {2014-11-25 09:49:50 +0000},
	Publisher = {Springer-Verlag},
	Series = {Aktuelle Forschung Medizintechnik},
	Title = {Registration Methods for Pulmonary Image Analysis. Integration of Morphological and Physiological Knowledge},
	Year = {2014}}

@article{PaBaSi11,
	Abstract = {Iterative reconstruction algorithms are becoming increasingly important in electron tomography of biological samples. These algorithms, however, impose major computational demands. Parallelization must be employed to maintain acceptable running times. Graphics Processing Units (GPUs) have been demonstrated to be highly cost-effective for carrying out these computations with a high degree of parallelism. In a recent paper by Xu et al. (2010), a GPU implementation strategy was presented that obtains a speedup of an order of magnitude over a previously proposed GPU-based electron tomography implementation. In this technical note, we demonstrate that by making alternative design decisions in the GPU implementation, an additional speedup can be obtained, again of an order of magnitude. By carefully considering memory access locality when dividing the workload among blocks of threads, the GPU's cache is used more efficiently, making more effective use of the available memory bandwidth.},
	Author = {Palenstijn, W. J. and Batenburg, K. J. and Sijbers, J.},
	Date-Added = {2014-11-25 09:42:51 +0000},
	Date-Modified = {2014-11-25 09:42:51 +0000},
	Journal = {Journal of Structural Biology},
	Number = {2},
	Pages = {250--253},
	Title = {Performance improvements for iterative electron tomography reconstruction using graphics processing units ({GPU}s)},
	Volume = {176},
	Year = {2011}}

@conference{KoScWu11,
	Abstract = {We present a flexible image reconstruction framework for emission tomography data called EMRECON. The software includes multiple expectation maximization based reconstruction algorithms as well as support for several scanner geometries. In order to implement novel reconstruction techniques (e.g. TV-based regularization or combined reconstruction and motion correction) or scanner models, full access to every stage of the reconstruction pipeline is vital. EMrecon is fully open and well-documented, thus permits testing without the need to care about data formats or standard reconstruction and data correction algorithms. Due to the GATE-like syntax new scanner geometries, including an exact definition of each single crystal, can be added easily. The parallel (multi-core) C implementation was successfully tested on several Linux distributions. This makes EMRECON a useful tool for the development of new reconstruction algorithms and also serves as a platform for testing different scanner geometries.
},
	Author = {Kosters, T. and Sch{\"a}fers, K. P. and W{\"u}bbeling, F.},
	Booktitle = {2011 IEEE Nuclear Science Symposium and Medical Imaging Conference (NSS/MIC)},
	Date-Added = {2014-11-25 09:36:24 +0000},
	Date-Modified = {2015-02-06 15:51:50 +0000},
	Pages = {4365--4368},
	Publisher = {IEEE},
	Title = {{EMRECON}: {A}n expectation maximization based image reconstruction framework for emission tomography data},
	Year = {2011}}

@inproceedings{RiLa14,
	Abstract = {In this work we introduce a novel generalization of the total-variation semi-norm for vector-valued images and demonstrate its potential usefulness for reconstructing sparse-view, spectral CT data. The proposed vectorial TV (VTV) encourages a common edge structure among spectral channels. We show that it leads to a simple optimization problem that can be efficiently minimized using the primal-dual algorithm proposed by Chambolle et al [1] and present the results of a simulation study. We find that the coupling between channels results in better apparent image quality and improved overall reconstruction accuracy when compared with independent, channel-by-channel reconstruction.},
	Author = {Ridge, D. S. and La Rivi{\`e}re, P. J.},
	Booktitle = {The third international conference on image formation in {X}-ray computed tomography, June 22-25, 2014 Fort Douglas/Olympic Village, Salt Lake City, Utah, USA},
	Date-Added = {2014-11-25 09:31:00 +0000},
	Date-Modified = {2014-11-25 09:36:05 +0000},
	Editor = {Noo, F.},
	Organization = {University of Utah},
	Pages = {9--12},
	Title = {A generalized vectorial total-variation for spectral {CT} reconstruction},
	Year = {2014}}

@article{EhThPiAtOu14,
	Abstract = {Recent advances in technology have enabled the combination of positron emission tomography (PET) with magnetic resonance imaging (MRI). These PET-MRI scanners simultaneously acquire functional PET and anatomical or functional MRI data. As function and anatomy are not independent of one another the images to be reconstructed are likely to have shared structures. We aim to exploit this inherent structural similarity by reconstructing from both modalities in a joint reconstruction framework. The structural similarity between two modalities can be modelled in two different ways: edges are more likely to be at similar positions and/or to have similar orientations. We analyse the diffusion process generated by minimizing priors that encapsulate these different models. It turns out that the class of parallel level set priors always corresponds to anisotropic diffusion which is sometimes forward and sometimes backward diffusion. We perform numerical experiments where we jointly reconstruct from blurred Radon data with Poisson noise (PET) and under-sampled Fourier data with Gaussian noise (MRI). Our results show that both modalities benefit from each other in areas of shared edge information. The joint reconstructions have less artefacts and sharper edges compared to separate reconstructions and the l2-error can be reduced in all of the considered cases of under-sampling.
},
	Author = {Ehrhardt, M. and Thielemans, K. and Pizarro, L. and Atkinson, D. and Ourselin, S. and Hutton, B. and Arridge, S.},
	Date-Added = {2014-11-25 09:27:40 +0000},
	Date-Modified = {2014-11-25 09:31:22 +0000},
	Journal = {Inverse Problems},
	Note = {Accepted for publication},
	Title = {Joint reconstruction of {PET-MRI} by exploiting structural similarity},
	Year = {2014}}

@article{FuFaTsLiAg13,
	Abstract = {PURPOSE: The Software for Tomographic Image Reconstruction (STIR, http://stir.sourceforge.net) package is an open source object-oriented library implemented in C++. Although its modular design is suitable for reconstructing data from several modalities, it currently only supports Positron Emission Tomography (PET) data. In this work, the authors present results for Single Photon Emission Computed Tomography (SPECT) imaging.
METHODS: This was achieved by the complete integration of a 3D SPECT system matrix modeling library into STIR.
RESULTS: The authors demonstrate the flexibility of the combined software by reconstructing simulated and acquired projections from three different scanners with different iterative algorithms of STIR.
CONCLUSIONS: The extension of the open source STIR project with advanced SPECT modeling will enable the research community to study the performance of several algorithms on SPECT data, and potentially implement new algorithms by expanding the existing framework.
},
	Author = {Fuster, B. M. and Falcon, C. and Tsoumpas, C. and Livieratos, L. and Aguiar, P. and Cot, A. and Ros, D. and Thielemans, K.},
	Date-Added = {2014-11-25 09:23:52 +0000},
	Date-Modified = {2014-11-25 09:25:59 +0000},
	Journal = {Medical Physics},
	Number = {9},
	Pages = {092502},
	Title = {Integration of advanced {3D SPECT} modeling into the open-source {STIR} framework},
	Volume = {40},
	Year = {2013}}

@article{ThTsMuBeAg12,
	Abstract = {We present a new version of STIR (Software for Tomographic Image Reconstruction), an open source object-oriented library implemented in C++ for 3D positron emission tomography reconstruction. This library has been designed such that it can be used for many algorithms and scanner geometries, while being portable to various computing platforms. This second release enhances its flexibility and modular design and includes additional features such as Compton scatter simulation, an additional iterative reconstruction algorithm and parametric image reconstruction (both indirect and direct). We discuss the new features in this release and present example results. STIR can be downloaded from http://stir.sourceforge.net.
},
	Author = {Thielemans, K. and Tsoumpas, C. and Mustafovic, S. and Beisel, T. and Aguiar, P. and Dikaios, N. and Jacobson, M. W.},
	Date-Added = {2014-11-25 09:20:25 +0000},
	Date-Modified = {2014-11-25 09:22:54 +0000},
	Journal = {Physics in Medicine and Biology},
	Number = {4},
	Pages = {867--883},
	Title = {{STIR} - software for tomographic image reconstruction release 2},
	Volume = {57},
	Year = {2012}}
